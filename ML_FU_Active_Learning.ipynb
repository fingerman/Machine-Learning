{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "osH6Y6GIhc9y"
   },
   "source": [
    "<center> <h1>  Active Learning </h1> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A5EdAJoah9Ww"
   },
   "source": [
    "# **Code**\n",
    "\n",
    "## **Active Learning - MNIST**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vz5gysf-hQM1"
   },
   "outputs": [],
   "source": [
    "import os, time, json, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy import stats\n",
    "from pylab import rcParams\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances_argmin_min, accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEL-7CbPeoiT"
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "trainset_size = 60000\n",
    "max_queried = 500\n",
    "\n",
    "\n",
    "def download():\n",
    " mnist = fetch_openml('mnist_784')\n",
    " X = mnist.data\n",
    " y = mnist.target.astype('float64')  # fetch_openml() returns targets as strings\n",
    " print ('MNIST:', X.shape, y.shape)\n",
    " return (X, y)\n",
    "\n",
    "def split(train_size):\n",
    " X_train_full = X[:train_size]\n",
    " y_train_full = y[:train_size]\n",
    " X_test = X[train_size:]\n",
    " y_test = y[train_size:]\n",
    " return (X_train_full, y_train_full, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ddc0A335iqQ0"
   },
   "outputs": [],
   "source": [
    "# 'BaseModel' is a base model for the class architecture, you can implement new models and use them interchangeably\n",
    "\n",
    "class BaseModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_predict(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SvmModel(BaseModel):\n",
    "\n",
    "    model_type = 'Support Vector Machine with linear Kernel'\n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training svm...')\n",
    "        self.classifier = SVC(C=1, kernel='linear', probability=True,\n",
    "                              class_weight=c_weight)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
    "                self.test_y_predicted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RfModel(BaseModel):\n",
    "\n",
    "    model_type = 'Random Forest'\n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training random forest...')\n",
    "        self.classifier = RandomForestClassifier(n_estimators=500, class_weight=c_weight)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
    "\n",
    "\n",
    "class NBModel(BaseModel):\n",
    "\n",
    "    model_type = 'Multinomial Naive Bayes'\n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training Naive Bayes  ...')\n",
    "        self.classifier = MultinomialNB()\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0w5ozf-djDTZ"
   },
   "outputs": [],
   "source": [
    "# 'TrainModel' class accepts one of the previously in defined learning algorithms,\n",
    "#  trains using the training set and gets performance measurements from the test set.\n",
    "\n",
    "class TrainModel:\n",
    "\n",
    "    def __init__(self, model_object):        \n",
    "        self.accuracies = []\n",
    "        self.model_object = model_object()        \n",
    "\n",
    "    def print_model_type(self):\n",
    "        print (self.model_object.model_type)\n",
    "\n",
    "    # we train normally and get probabilities for the validation set. i.e., we use the probabilities to select the most uncertain samples\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('Train set:', X_train.shape, 'y:', y_train.shape)\n",
    "        print ('Val   set:', X_val.shape)\n",
    "        print ('Test  set:', X_test.shape)\n",
    "        t0 = time.time()\n",
    "        (X_train, X_val, X_test, self.val_y_predicted,\n",
    "         self.test_y_predicted) = \\\n",
    "            self.model_object.fit_predict(X_train, y_train, X_val, X_test, c_weight)\n",
    "        self.run_time = time.time() - t0\n",
    "        return (X_train, X_val, X_test)  # we return them in case we use PCA, with all the other algorithms, this is not needed.\n",
    "\n",
    "    # we want accuracy only for the test set\n",
    "\n",
    "    def get_test_accuracy(self, i, y_test):\n",
    "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
    "        self.accuracies.append(classif_rate)               \n",
    "        print('--------------------------------')\n",
    "        print('Iteration:',i)\n",
    "        print('--------------------------------')\n",
    "        print('y-test set:',y_test.shape)\n",
    "        print(\"Accuracy rate for classification rate of: %f \" % (classif_rate))    \n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
    "       # print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
    "        print(\"F1 Score : %f\" % metrics.f1_score(y_test, self.test_y_predicted, average='macro'))\n",
    "        print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GL_ajX0NjNJX"
   },
   "outputs": [],
   "source": [
    "# 'BaseSelectionFunction' is a base class for various sample selection methods.\n",
    "#  current implementations include random-selection,  margin sampling-selection \n",
    "\n",
    "class BaseSelectionFunction(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def select(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomSelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        random_state = check_random_state(0)\n",
    "        selection = np.random.choice(probas_val.shape[0], initial_labeled_samples, replace=False)\n",
    "        return selection\n",
    "\n",
    "      \n",
    "class MarginSamplingSelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        rev = np.sort(probas_val, axis=1)[:, ::-1]\n",
    "        values = rev[:, 0] - rev[:, 1]\n",
    "        selection = np.argsort(values)[:initial_labeled_samples]\n",
    "        return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "giby34TNlhoN"
   },
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \n",
    "    def normalize(self, X_train, X_val, X_test):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val   = self.scaler.transform(X_val)\n",
    "        X_test  = self.scaler.transform(X_test)\n",
    "        return (X_train, X_val, X_test) \n",
    "    \n",
    "    def inverse(self, X_train, X_val, X_test):\n",
    "        X_train = self.scaler.inverse_transform(X_train)\n",
    "        X_val   = self.scaler.inverse_transform(X_val)\n",
    "        X_test  = self.scaler.inverse_transform(X_test)\n",
    "        return (X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDDYBnOEliMU"
   },
   "outputs": [],
   "source": [
    "def get_k_random_samples(initial_labeled_samples, X_train_full,\n",
    "                         y_train_full):\n",
    "    random_state = check_random_state(0)\n",
    "    permutation = np.random.choice(trainset_size,\n",
    "                                   initial_labeled_samples,\n",
    "                                   replace=False)\n",
    "    print ()\n",
    "    print ('initial random chosen samples', permutation.shape),\n",
    "#            permutation)\n",
    "    X_train = X_train_full[permutation]\n",
    "    y_train = y_train_full[permutation]\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    bin_count = np.bincount(y_train.astype('int64'))\n",
    "    unique = np.unique(y_train.astype('int64'))\n",
    "    print (\n",
    "        'initial train set:',\n",
    "        X_train.shape,\n",
    "        y_train.shape,\n",
    "        'unique(labels):',\n",
    "        bin_count,\n",
    "        unique,\n",
    "        )\n",
    "    return (permutation, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6r1bAFAlzN8"
   },
   "outputs": [],
   "source": [
    "# - select 'k' random samples, \n",
    "# - train a model, \n",
    "# - select the most informative samples, \n",
    "# - remove from the validation set, \n",
    "# - query their labels \n",
    "# - and retrain using those samples until reaching the stop criteria.\n",
    "\n",
    "\n",
    "class TheAlgorithm(object):\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    def __init__(self, initial_labeled_samples, model_object, selection_function):\n",
    "        self.initial_labeled_samples = initial_labeled_samples\n",
    "        self.model_object = model_object\n",
    "        self.sample_selection_function = selection_function\n",
    "\n",
    "    def run(self, X_train_full, y_train_full, X_test, y_test):\n",
    "\n",
    "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
    "\n",
    "        (permutation, X_train, y_train) = \\\n",
    "            get_k_random_samples(self.initial_labeled_samples,\n",
    "                                 X_train_full, y_train_full)\n",
    "        self.queried = self.initial_labeled_samples\n",
    "        self.samplecount = [self.initial_labeled_samples]\n",
    "\n",
    "\n",
    "        # assign the val set the rest of the 'unlabelled' training data\n",
    "\n",
    "        X_val = np.array([])\n",
    "        y_val = np.array([])\n",
    "        X_val = np.copy(X_train_full)\n",
    "        X_val = np.delete(X_val, permutation, axis=0)\n",
    "        y_val = np.copy(y_train_full)\n",
    "        y_val = np.delete(y_val, permutation, axis=0)\n",
    "        print ('val set:', X_val.shape, y_val.shape, permutation.shape)\n",
    "        print ()\n",
    "\n",
    "        # normalize data\n",
    "\n",
    "        normalizer = Normalize()\n",
    "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)   \n",
    "        \n",
    "        self.clf_model = TrainModel(self.model_object)\n",
    "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
    "        active_iteration = 1\n",
    "        self.clf_model.get_test_accuracy(1, y_test)\n",
    "\n",
    "\n",
    "        while self.queried < max_queried:\n",
    "\n",
    "            active_iteration += 1\n",
    "\n",
    "            # get validation probabilities\n",
    "\n",
    "            probas_val = \\\n",
    "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
    "            print ('val predicted:',\n",
    "                   self.clf_model.val_y_predicted.shape,\n",
    "                   self.clf_model.val_y_predicted)\n",
    "            print ('probabilities:', probas_val.shape, '\\n',\n",
    "                   np.argmax(probas_val, axis=1))\n",
    "\n",
    "            # select samples using a selection function\n",
    "\n",
    "            uncertain_samples = \\\n",
    "                self.sample_selection_function.select(probas_val, self.initial_labeled_samples)\n",
    "\n",
    "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
    " \n",
    "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
    "\n",
    "            # get the uncertain samples from the validation set\n",
    "\n",
    "            print ('trainset before', X_train.shape, y_train.shape)\n",
    "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
    "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
    "            print ('trainset after', X_train.shape, y_train.shape)\n",
    "            self.samplecount.append(X_train.shape[0])\n",
    "\n",
    "            bin_count = np.bincount(y_train.astype('int64'))\n",
    "            unique = np.unique(y_train.astype('int64'))\n",
    "            print (\n",
    "                'updated train set:',\n",
    "                X_train.shape,\n",
    "                y_train.shape,\n",
    "                'unique(labels):',\n",
    "                bin_count,\n",
    "                unique,\n",
    "                )\n",
    "\n",
    "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
    "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
    "            print ('val set:', X_val.shape, y_val.shape)\n",
    "            print ()\n",
    "\n",
    "            # normalize again after creating the 'new' train/test sets\n",
    "            normalizer = Normalize()\n",
    "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
    "\n",
    "            self.queried += self.initial_labeled_samples\n",
    "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
    "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
    "\n",
    "        print ('final active learning accuracies',\n",
    "               self.clf_model.accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "syEkrssCmiGQ",
    "outputId": "e2dfe92e-d60c-427e-9de4-a4c32109a2d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST: (70000, 784) (70000,)\n",
      "train: (60000, 784) (60000,)\n",
      "test : (10000, 784) (10000,)\n",
      "unique classes 10\n",
      "stopping at: 500\n",
      "Count = 1, using model = NBModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
      "\n",
      "initial random chosen samples (250,)\n",
      "initial train set: (250, 784) (250,) unique(labels): [26 26 22 24 31 26 19 31 19 26] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,) (250,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.470000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.95      0.90       980\n",
      "         1.0       0.69      0.82      0.74      1135\n",
      "         2.0       0.88      0.73      0.80      1032\n",
      "         3.0       0.79      0.82      0.80      1010\n",
      "         4.0       0.80      0.71      0.75       982\n",
      "         5.0       0.69      0.63      0.66       892\n",
      "         6.0       0.87      0.78      0.82       958\n",
      "         7.0       0.94      0.68      0.79      1028\n",
      "         8.0       0.56      0.61      0.58       974\n",
      "         9.0       0.62      0.80      0.70      1009\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.77      0.75      0.76     10000\n",
      "weighted avg       0.77      0.75      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.755409\n",
      "--------------------------------\n",
      "val predicted: (59750,) [5. 0. 4. ... 5. 1. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [5 0 4 ... 5 1 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [55 58 50 49 53 52 44 51 46 42] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 81.580000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.95      0.91       980\n",
      "         1.0       0.82      0.93      0.87      1135\n",
      "         2.0       0.89      0.77      0.83      1032\n",
      "         3.0       0.86      0.78      0.82      1010\n",
      "         4.0       0.85      0.80      0.83       982\n",
      "         5.0       0.80      0.67      0.73       892\n",
      "         6.0       0.88      0.88      0.88       958\n",
      "         7.0       0.94      0.76      0.84      1028\n",
      "         8.0       0.60      0.75      0.67       974\n",
      "         9.0       0.73      0.85      0.78      1009\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.81      0.81     10000\n",
      "weighted avg       0.83      0.82      0.82     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.814909\n",
      "--------------------------------\n",
      "final active learning accuracies [75.47, 81.58]\n",
      "saved Active-learning-experiment-1.pkl /content ['.config', 'Active-learning-experiment-1.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 2, using model = NBModel, selection_function = RandomSelection, k = 100, iteration = 0.\n",
      "\n",
      "initial random chosen samples (100,)\n",
      "initial train set: (100, 784) (100,) unique(labels): [10  9 14  8 10 14  8 10  6 11] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,) (100,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 60.100000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.89      0.91       980\n",
      "         1.0       0.56      0.86      0.68      1135\n",
      "         2.0       0.75      0.73      0.74      1032\n",
      "         3.0       0.79      0.45      0.57      1010\n",
      "         4.0       0.60      0.63      0.61       982\n",
      "         5.0       0.32      0.57      0.41       892\n",
      "         6.0       0.86      0.51      0.64       958\n",
      "         7.0       0.79      0.61      0.69      1028\n",
      "         8.0       0.17      0.13      0.15       974\n",
      "         9.0       0.57      0.58      0.57      1009\n",
      "\n",
      "    accuracy                           0.60     10000\n",
      "   macro avg       0.63      0.60      0.60     10000\n",
      "weighted avg       0.64      0.60      0.60     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.597334\n",
      "--------------------------------\n",
      "val predicted: (59900,) [1. 0. 8. ... 8. 6. 1.]\n",
      "probabilities: (59900, 10) \n",
      " [1 0 8 ... 8 6 1]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [28 19 25 14 17 23 17 19 14 24] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 70.310000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.93      0.90       980\n",
      "         1.0       0.57      0.89      0.70      1135\n",
      "         2.0       0.86      0.77      0.82      1032\n",
      "         3.0       0.80      0.64      0.71      1010\n",
      "         4.0       0.71      0.54      0.61       982\n",
      "         5.0       0.53      0.54      0.54       892\n",
      "         6.0       0.88      0.65      0.75       958\n",
      "         7.0       0.87      0.64      0.74      1028\n",
      "         8.0       0.72      0.60      0.65       974\n",
      "         9.0       0.52      0.77      0.62      1009\n",
      "\n",
      "    accuracy                           0.70     10000\n",
      "   macro avg       0.73      0.70      0.70     10000\n",
      "weighted avg       0.73      0.70      0.71     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.703511\n",
      "--------------------------------\n",
      "val predicted: (59800,) [3. 0. 1. ... 5. 6. 1.]\n",
      "probabilities: (59800, 10) \n",
      " [3 0 1 ... 5 6 1]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [40 28 35 25 23 35 28 28 26 32] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 76.400000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91       980\n",
      "         1.0       0.68      0.90      0.78      1135\n",
      "         2.0       0.89      0.79      0.84      1032\n",
      "         3.0       0.87      0.78      0.82      1010\n",
      "         4.0       0.71      0.55      0.62       982\n",
      "         5.0       0.78      0.65      0.71       892\n",
      "         6.0       0.89      0.73      0.80       958\n",
      "         7.0       0.90      0.69      0.78      1028\n",
      "         8.0       0.63      0.78      0.70       974\n",
      "         9.0       0.59      0.81      0.68      1009\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.78      0.76      0.76     10000\n",
      "weighted avg       0.78      0.76      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.763896\n",
      "--------------------------------\n",
      "val predicted: (59700,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [54 40 43 40 30 45 37 37 34 40] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 78.110000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.95      0.92       980\n",
      "         1.0       0.80      0.91      0.85      1135\n",
      "         2.0       0.91      0.79      0.84      1032\n",
      "         3.0       0.82      0.81      0.82      1010\n",
      "         4.0       0.73      0.62      0.67       982\n",
      "         5.0       0.82      0.64      0.72       892\n",
      "         6.0       0.87      0.82      0.84       958\n",
      "         7.0       0.83      0.71      0.77      1028\n",
      "         8.0       0.63      0.78      0.70       974\n",
      "         9.0       0.60      0.76      0.67      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.79      0.78      0.78     10000\n",
      "weighted avg       0.79      0.78      0.78     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.779977\n",
      "--------------------------------\n",
      "val predicted: (59600,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [57 59 50 52 42 52 47 56 40 45] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 79.950000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91       980\n",
      "         1.0       0.85      0.91      0.88      1135\n",
      "         2.0       0.91      0.80      0.85      1032\n",
      "         3.0       0.86      0.82      0.84      1010\n",
      "         4.0       0.74      0.69      0.72       982\n",
      "         5.0       0.81      0.64      0.71       892\n",
      "         6.0       0.87      0.83      0.85       958\n",
      "         7.0       0.85      0.78      0.81      1028\n",
      "         8.0       0.64      0.79      0.71       974\n",
      "         9.0       0.65      0.77      0.70      1009\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.81      0.80      0.80     10000\n",
      "weighted avg       0.81      0.80      0.80     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.798195\n",
      "--------------------------------\n",
      "final active learning accuracies [60.099999999999994, 70.30999999999999, 76.4, 78.11, 79.95]\n",
      "saved Active-learning-experiment-2.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-1.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 3, using model = NBModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
      "\n",
      "initial random chosen samples (25,)\n",
      "initial train set: (25, 784) (25,) unique(labels): [5 1 2 1 4 1 4 1 3 3] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59975, 784) (59975,) (25,)\n",
      "\n",
      "Train set: (25, 784) y: (25,)\n",
      "Val   set: (59975, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 32.790000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.63      0.58       980\n",
      "         1.0       0.02      0.05      0.03      1135\n",
      "         2.0       0.86      0.23      0.36      1032\n",
      "         3.0       0.42      0.11      0.17      1010\n",
      "         4.0       0.52      0.41      0.46       982\n",
      "         5.0       0.34      0.07      0.11       892\n",
      "         6.0       0.52      0.54      0.53       958\n",
      "         7.0       0.58      0.33      0.42      1028\n",
      "         8.0       0.29      0.57      0.39       974\n",
      "         9.0       0.41      0.37      0.39      1009\n",
      "\n",
      "    accuracy                           0.33     10000\n",
      "   macro avg       0.45      0.33      0.34     10000\n",
      "weighted avg       0.45      0.33      0.34     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.343919\n",
      "--------------------------------\n",
      "val predicted: (59975,) [3. 0. 1. ... 3. 1. 1.]\n",
      "probabilities: (59975, 10) \n",
      " [3 0 1 ... 3 1 1]\n",
      "trainset before (25, 784) (25,)\n",
      "trainset after (50, 784) (50,)\n",
      "updated train set: (50, 784) (50,) unique(labels): [9 2 5 3 7 3 5 6 6 4] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59950, 784) (59950,)\n",
      "\n",
      "Train set: (50, 784) y: (50,)\n",
      "Val   set: (59950, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 38.790000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.72      0.62       980\n",
      "         1.0       0.02      0.05      0.03      1135\n",
      "         2.0       0.65      0.46      0.54      1032\n",
      "         3.0       0.74      0.14      0.24      1010\n",
      "         4.0       0.69      0.65      0.67       982\n",
      "         5.0       0.34      0.06      0.10       892\n",
      "         6.0       0.86      0.51      0.64       958\n",
      "         7.0       0.45      0.55      0.50      1028\n",
      "         8.0       0.30      0.73      0.43       974\n",
      "         9.0       0.26      0.04      0.07      1009\n",
      "\n",
      "    accuracy                           0.39     10000\n",
      "   macro avg       0.49      0.39      0.39     10000\n",
      "weighted avg       0.48      0.39      0.38     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.385186\n",
      "--------------------------------\n",
      "val predicted: (59950,) [1. 0. 1. ... 1. 1. 8.]\n",
      "probabilities: (59950, 10) \n",
      " [1 0 1 ... 1 1 8]\n",
      "trainset before (50, 784) (50,)\n",
      "trainset after (75, 784) (75,)\n",
      "updated train set: (75, 784) (75,) unique(labels): [10  4  7 10 10  5  8  8  8  5] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59925, 784) (59925,)\n",
      "\n",
      "Train set: (75, 784) y: (75,)\n",
      "Val   set: (59925, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 48.910000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.90      0.83       980\n",
      "         1.0       0.11      0.19      0.14      1135\n",
      "         2.0       0.88      0.52      0.65      1032\n",
      "         3.0       0.59      0.74      0.65      1010\n",
      "         4.0       0.73      0.66      0.69       982\n",
      "         5.0       0.48      0.12      0.19       892\n",
      "         6.0       0.82      0.52      0.64       958\n",
      "         7.0       0.55      0.55      0.55      1028\n",
      "         8.0       0.31      0.65      0.42       974\n",
      "         9.0       0.34      0.07      0.11      1009\n",
      "\n",
      "    accuracy                           0.49     10000\n",
      "   macro avg       0.56      0.49      0.49     10000\n",
      "weighted avg       0.55      0.49      0.48     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.487250\n",
      "--------------------------------\n",
      "val predicted: (59925,) [1. 0. 1. ... 1. 1. 8.]\n",
      "probabilities: (59925, 10) \n",
      " [1 0 1 ... 1 1 8]\n",
      "trainset before (75, 784) (75,)\n",
      "trainset after (100, 784) (100,)\n",
      "updated train set: (100, 784) (100,) unique(labels): [14 12 10 11 12  5  9 11  8  8] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 64.210000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.90      0.83       980\n",
      "         1.0       0.87      0.87      0.87      1135\n",
      "         2.0       0.86      0.66      0.74      1032\n",
      "         3.0       0.61      0.77      0.68      1010\n",
      "         4.0       0.65      0.77      0.71       982\n",
      "         5.0       0.17      0.17      0.17       892\n",
      "         6.0       0.84      0.57      0.68       958\n",
      "         7.0       0.68      0.70      0.69      1028\n",
      "         8.0       0.58      0.60      0.59       974\n",
      "         9.0       0.39      0.34      0.36      1009\n",
      "\n",
      "    accuracy                           0.64     10000\n",
      "   macro avg       0.64      0.63      0.63     10000\n",
      "weighted avg       0.65      0.64      0.64     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.631817\n",
      "--------------------------------\n",
      "val predicted: (59900,) [5. 0. 5. ... 8. 5. 8.]\n",
      "probabilities: (59900, 10) \n",
      " [5 0 5 ... 8 5 8]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (125, 784) (125,)\n",
      "updated train set: (125, 784) (125,) unique(labels): [17 14 13 13 17  7 13 11 11  9] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59875, 784) (59875,)\n",
      "\n",
      "Train set: (125, 784) y: (125,)\n",
      "Val   set: (59875, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 64.910000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.91      0.87       980\n",
      "         1.0       0.88      0.82      0.85      1135\n",
      "         2.0       0.90      0.65      0.75      1032\n",
      "         3.0       0.66      0.74      0.70      1010\n",
      "         4.0       0.63      0.78      0.70       982\n",
      "         5.0       0.19      0.16      0.17       892\n",
      "         6.0       0.83      0.60      0.70       958\n",
      "         7.0       0.84      0.67      0.74      1028\n",
      "         8.0       0.47      0.71      0.56       974\n",
      "         9.0       0.38      0.39      0.38      1009\n",
      "\n",
      "    accuracy                           0.65     10000\n",
      "   macro avg       0.66      0.64      0.64     10000\n",
      "weighted avg       0.67      0.65      0.65     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.642237\n",
      "--------------------------------\n",
      "val predicted: (59875,) [5. 0. 5. ... 8. 5. 8.]\n",
      "probabilities: (59875, 10) \n",
      " [5 0 5 ... 8 5 8]\n",
      "trainset before (125, 784) (125,)\n",
      "trainset after (150, 784) (150,)\n",
      "updated train set: (150, 784) (150,) unique(labels): [20 17 13 14 20 13 15 13 14 11] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59850, 784) (59850,)\n",
      "\n",
      "Train set: (150, 784) y: (150,)\n",
      "Val   set: (59850, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 6\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 68.720000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.89      0.88       980\n",
      "         1.0       0.85      0.80      0.83      1135\n",
      "         2.0       0.93      0.65      0.76      1032\n",
      "         3.0       0.73      0.70      0.72      1010\n",
      "         4.0       0.66      0.77      0.71       982\n",
      "         5.0       0.73      0.30      0.42       892\n",
      "         6.0       0.85      0.72      0.78       958\n",
      "         7.0       0.87      0.68      0.76      1028\n",
      "         8.0       0.47      0.78      0.59       974\n",
      "         9.0       0.37      0.53      0.43      1009\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.73      0.68      0.69     10000\n",
      "weighted avg       0.73      0.69      0.69     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.687534\n",
      "--------------------------------\n",
      "val predicted: (59850,) [3. 0. 9. ... 5. 9. 8.]\n",
      "probabilities: (59850, 10) \n",
      " [3 0 9 ... 5 9 8]\n",
      "trainset before (150, 784) (150,)\n",
      "trainset after (175, 784) (175,)\n",
      "updated train set: (175, 784) (175,) unique(labels): [22 18 17 16 23 14 16 18 18 13] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59825, 784) (59825,)\n",
      "\n",
      "Train set: (175, 784) y: (175,)\n",
      "Val   set: (59825, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 7\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 68.230000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.88      0.88       980\n",
      "         1.0       0.63      0.80      0.71      1135\n",
      "         2.0       0.90      0.63      0.74      1032\n",
      "         3.0       0.72      0.73      0.73      1010\n",
      "         4.0       0.71      0.71      0.71       982\n",
      "         5.0       0.67      0.26      0.37       892\n",
      "         6.0       0.84      0.73      0.78       958\n",
      "         7.0       0.79      0.69      0.74      1028\n",
      "         8.0       0.50      0.81      0.62       974\n",
      "         9.0       0.45      0.53      0.48      1009\n",
      "\n",
      "    accuracy                           0.68     10000\n",
      "   macro avg       0.71      0.68      0.68     10000\n",
      "weighted avg       0.71      0.68      0.68     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.675988\n",
      "--------------------------------\n",
      "val predicted: (59825,) [3. 0. 1. ... 5. 1. 8.]\n",
      "probabilities: (59825, 10) \n",
      " [3 0 1 ... 5 1 8]\n",
      "trainset before (175, 784) (175,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [26 22 19 19 24 18 18 18 21 15] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 8\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 71.070000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.87      0.88       980\n",
      "         1.0       0.81      0.87      0.84      1135\n",
      "         2.0       0.91      0.65      0.76      1032\n",
      "         3.0       0.71      0.73      0.72      1010\n",
      "         4.0       0.72      0.66      0.69       982\n",
      "         5.0       0.78      0.34      0.47       892\n",
      "         6.0       0.85      0.79      0.82       958\n",
      "         7.0       0.85      0.68      0.76      1028\n",
      "         8.0       0.53      0.82      0.64       974\n",
      "         9.0       0.41      0.63      0.50      1009\n",
      "\n",
      "    accuracy                           0.71     10000\n",
      "   macro avg       0.75      0.71      0.71     10000\n",
      "weighted avg       0.75      0.71      0.71     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.709789\n",
      "--------------------------------\n",
      "val predicted: (59800,) [3. 0. 9. ... 5. 6. 8.]\n",
      "probabilities: (59800, 10) \n",
      " [3 0 9 ... 5 6 8]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (225, 784) (225,)\n",
      "updated train set: (225, 784) (225,) unique(labels): [28 24 21 22 25 21 20 22 24 18] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59775, 784) (59775,)\n",
      "\n",
      "Train set: (225, 784) y: (225,)\n",
      "Val   set: (59775, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 9\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 72.530000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.87      0.89       980\n",
      "         1.0       0.67      0.86      0.76      1135\n",
      "         2.0       0.92      0.63      0.74      1032\n",
      "         3.0       0.71      0.75      0.73      1010\n",
      "         4.0       0.78      0.64      0.70       982\n",
      "         5.0       0.80      0.41      0.55       892\n",
      "         6.0       0.84      0.83      0.84       958\n",
      "         7.0       0.88      0.68      0.77      1028\n",
      "         8.0       0.54      0.82      0.65       974\n",
      "         9.0       0.54      0.71      0.61      1009\n",
      "\n",
      "    accuracy                           0.73     10000\n",
      "   macro avg       0.76      0.72      0.72     10000\n",
      "weighted avg       0.76      0.73      0.73     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.723838\n",
      "--------------------------------\n",
      "val predicted: (59775,) [3. 0. 9. ... 5. 6. 8.]\n",
      "probabilities: (59775, 10) \n",
      " [3 0 9 ... 5 6 8]\n",
      "trainset before (225, 784) (225,)\n",
      "trainset after (250, 784) (250,)\n",
      "updated train set: (250, 784) (250,) unique(labels): [29 27 25 23 28 26 23 26 25 18] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 10\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 73.810000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.87      0.89       980\n",
      "         1.0       0.84      0.88      0.86      1135\n",
      "         2.0       0.91      0.68      0.78      1032\n",
      "         3.0       0.73      0.76      0.74      1010\n",
      "         4.0       0.74      0.66      0.70       982\n",
      "         5.0       0.77      0.49      0.60       892\n",
      "         6.0       0.83      0.79      0.81       958\n",
      "         7.0       0.86      0.69      0.76      1028\n",
      "         8.0       0.61      0.81      0.69       974\n",
      "         9.0       0.46      0.71      0.56      1009\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.76      0.73      0.74     10000\n",
      "weighted avg       0.77      0.74      0.74     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.739565\n",
      "--------------------------------\n",
      "val predicted: (59750,) [5. 0. 9. ... 5. 6. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [5 0 9 ... 5 6 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (275, 784) (275,)\n",
      "updated train set: (275, 784) (275,) unique(labels): [29 27 27 25 31 30 27 30 29 20] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59725, 784) (59725,)\n",
      "\n",
      "Train set: (275, 784) y: (275,)\n",
      "Val   set: (59725, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 11\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 74.540000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.86      0.88       980\n",
      "         1.0       0.78      0.87      0.82      1135\n",
      "         2.0       0.92      0.69      0.79      1032\n",
      "         3.0       0.74      0.76      0.75      1010\n",
      "         4.0       0.74      0.66      0.70       982\n",
      "         5.0       0.79      0.56      0.66       892\n",
      "         6.0       0.83      0.83      0.83       958\n",
      "         7.0       0.86      0.69      0.77      1028\n",
      "         8.0       0.59      0.82      0.68       974\n",
      "         9.0       0.52      0.69      0.59      1009\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.77      0.74      0.75     10000\n",
      "weighted avg       0.77      0.75      0.75     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.746635\n",
      "--------------------------------\n",
      "val predicted: (59725,) [1. 0. 9. ... 5. 6. 8.]\n",
      "probabilities: (59725, 10) \n",
      " [1 0 9 ... 5 6 8]\n",
      "trainset before (275, 784) (275,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [31 28 28 29 34 31 29 33 34 23] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 12\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.140000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.85      0.88       980\n",
      "         1.0       0.74      0.87      0.80      1135\n",
      "         2.0       0.91      0.70      0.79      1032\n",
      "         3.0       0.75      0.77      0.76      1010\n",
      "         4.0       0.74      0.68      0.71       982\n",
      "         5.0       0.85      0.56      0.67       892\n",
      "         6.0       0.82      0.83      0.82       958\n",
      "         7.0       0.86      0.72      0.78      1028\n",
      "         8.0       0.57      0.84      0.68       974\n",
      "         9.0       0.57      0.68      0.62      1009\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.77      0.75      0.75     10000\n",
      "weighted avg       0.77      0.75      0.75     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.752259\n",
      "--------------------------------\n",
      "val predicted: (59700,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (325, 784) (325,)\n",
      "updated train set: (325, 784) (325,) unique(labels): [33 29 29 34 37 33 29 35 38 28] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59675, 784) (59675,)\n",
      "\n",
      "Train set: (325, 784) y: (325,)\n",
      "Val   set: (59675, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 13\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.260000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.86      0.88       980\n",
      "         1.0       0.74      0.85      0.79      1135\n",
      "         2.0       0.92      0.70      0.80      1032\n",
      "         3.0       0.73      0.81      0.77      1010\n",
      "         4.0       0.77      0.64      0.70       982\n",
      "         5.0       0.85      0.51      0.64       892\n",
      "         6.0       0.83      0.83      0.83       958\n",
      "         7.0       0.89      0.74      0.81      1028\n",
      "         8.0       0.54      0.83      0.66       974\n",
      "         9.0       0.61      0.73      0.66      1009\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.78      0.75      0.75     10000\n",
      "weighted avg       0.78      0.75      0.75     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.752579\n",
      "--------------------------------\n",
      "val predicted: (59675,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59675, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (325, 784) (325,)\n",
      "trainset after (350, 784) (350,)\n",
      "updated train set: (350, 784) (350,) unique(labels): [36 33 30 37 39 34 30 39 43 29] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59650, 784) (59650,)\n",
      "\n",
      "Train set: (350, 784) y: (350,)\n",
      "Val   set: (59650, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 14\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.930000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.88      0.89       980\n",
      "         1.0       0.76      0.87      0.81      1135\n",
      "         2.0       0.91      0.70      0.79      1032\n",
      "         3.0       0.73      0.80      0.76      1010\n",
      "         4.0       0.78      0.67      0.72       982\n",
      "         5.0       0.88      0.50      0.64       892\n",
      "         6.0       0.83      0.84      0.84       958\n",
      "         7.0       0.89      0.74      0.81      1028\n",
      "         8.0       0.53      0.83      0.65       974\n",
      "         9.0       0.63      0.74      0.68      1009\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.79      0.76      0.76     10000\n",
      "weighted avg       0.79      0.76      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.758907\n",
      "--------------------------------\n",
      "val predicted: (59650,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59650, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (350, 784) (350,)\n",
      "trainset after (375, 784) (375,)\n",
      "updated train set: (375, 784) (375,) unique(labels): [37 36 36 39 40 35 31 41 47 33] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59625, 784) (59625,)\n",
      "\n",
      "Train set: (375, 784) y: (375,)\n",
      "Val   set: (59625, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 15\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 76.150000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.87      0.89       980\n",
      "         1.0       0.76      0.88      0.81      1135\n",
      "         2.0       0.92      0.73      0.81      1032\n",
      "         3.0       0.75      0.81      0.78      1010\n",
      "         4.0       0.81      0.66      0.72       982\n",
      "         5.0       0.90      0.48      0.63       892\n",
      "         6.0       0.83      0.84      0.84       958\n",
      "         7.0       0.89      0.72      0.80      1028\n",
      "         8.0       0.54      0.83      0.66       974\n",
      "         9.0       0.61      0.76      0.68      1009\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.79      0.76      0.76     10000\n",
      "weighted avg       0.79      0.76      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.760451\n",
      "--------------------------------\n",
      "val predicted: (59625,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59625, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (375, 784) (375,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [41 39 39 43 41 39 32 43 48 35] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 16\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 77.090000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.89      0.89       980\n",
      "         1.0       0.78      0.89      0.83      1135\n",
      "         2.0       0.91      0.73      0.81      1032\n",
      "         3.0       0.73      0.81      0.77      1010\n",
      "         4.0       0.81      0.66      0.73       982\n",
      "         5.0       0.88      0.52      0.66       892\n",
      "         6.0       0.84      0.85      0.85       958\n",
      "         7.0       0.90      0.73      0.80      1028\n",
      "         8.0       0.58      0.81      0.68       974\n",
      "         9.0       0.61      0.76      0.68      1009\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.79      0.77      0.77     10000\n",
      "weighted avg       0.79      0.77      0.77     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.769484\n",
      "--------------------------------\n",
      "val predicted: (59600,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (425, 784) (425,)\n",
      "updated train set: (425, 784) (425,) unique(labels): [45 44 40 46 42 40 35 48 50 35] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59575, 784) (59575,)\n",
      "\n",
      "Train set: (425, 784) y: (425,)\n",
      "Val   set: (59575, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 17\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 77.670000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.90      0.89       980\n",
      "         1.0       0.81      0.91      0.86      1135\n",
      "         2.0       0.91      0.74      0.82      1032\n",
      "         3.0       0.74      0.83      0.78      1010\n",
      "         4.0       0.81      0.65      0.72       982\n",
      "         5.0       0.90      0.52      0.66       892\n",
      "         6.0       0.84      0.85      0.84       958\n",
      "         7.0       0.91      0.75      0.82      1028\n",
      "         8.0       0.59      0.81      0.68       974\n",
      "         9.0       0.60      0.76      0.67      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.80      0.77      0.77     10000\n",
      "weighted avg       0.80      0.78      0.78     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.774827\n",
      "--------------------------------\n",
      "val predicted: (59575,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59575, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (425, 784) (425,)\n",
      "trainset after (450, 784) (450,)\n",
      "updated train set: (450, 784) (450,) unique(labels): [46 47 40 48 43 45 39 51 54 37] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59550, 784) (59550,)\n",
      "\n",
      "Train set: (450, 784) y: (450,)\n",
      "Val   set: (59550, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 18\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 78.060000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.90      0.89       980\n",
      "         1.0       0.81      0.91      0.86      1135\n",
      "         2.0       0.91      0.74      0.82      1032\n",
      "         3.0       0.76      0.82      0.79      1010\n",
      "         4.0       0.81      0.65      0.72       982\n",
      "         5.0       0.89      0.58      0.70       892\n",
      "         6.0       0.84      0.83      0.84       958\n",
      "         7.0       0.91      0.75      0.82      1028\n",
      "         8.0       0.60      0.81      0.69       974\n",
      "         9.0       0.60      0.77      0.68      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.80      0.78      0.78     10000\n",
      "weighted avg       0.80      0.78      0.78     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.779946\n",
      "--------------------------------\n",
      "val predicted: (59550,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59550, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (450, 784) (450,)\n",
      "trainset after (475, 784) (475,)\n",
      "updated train set: (475, 784) (475,) unique(labels): [48 52 41 50 47 45 41 53 59 39] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59525, 784) (59525,)\n",
      "\n",
      "Train set: (475, 784) y: (475,)\n",
      "Val   set: (59525, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 19\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 78.080000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.91      0.90       980\n",
      "         1.0       0.85      0.91      0.88      1135\n",
      "         2.0       0.91      0.73      0.81      1032\n",
      "         3.0       0.75      0.82      0.78      1010\n",
      "         4.0       0.81      0.65      0.72       982\n",
      "         5.0       0.90      0.56      0.69       892\n",
      "         6.0       0.84      0.85      0.85       958\n",
      "         7.0       0.91      0.76      0.83      1028\n",
      "         8.0       0.59      0.81      0.68       974\n",
      "         9.0       0.59      0.78      0.67      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.80      0.78      0.78     10000\n",
      "weighted avg       0.80      0.78      0.78     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.779990\n",
      "--------------------------------\n",
      "val predicted: (59525,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59525, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (475, 784) (475,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [49 54 42 52 51 48 44 57 62 41] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 20\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 78.350000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.91      0.90       980\n",
      "         1.0       0.84      0.91      0.88      1135\n",
      "         2.0       0.91      0.73      0.81      1032\n",
      "         3.0       0.76      0.82      0.79      1010\n",
      "         4.0       0.81      0.66      0.72       982\n",
      "         5.0       0.90      0.58      0.70       892\n",
      "         6.0       0.84      0.86      0.85       958\n",
      "         7.0       0.91      0.75      0.82      1028\n",
      "         8.0       0.58      0.80      0.67       974\n",
      "         9.0       0.61      0.77      0.68      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.80      0.78      0.78     10000\n",
      "weighted avg       0.80      0.78      0.79     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.782989\n",
      "--------------------------------\n",
      "final active learning accuracies [32.79, 38.79, 48.91, 64.21, 64.91, 68.72, 68.23, 71.07, 72.53, 73.81, 74.53999999999999, 75.14, 75.26, 75.92999999999999, 76.14999999999999, 77.09, 77.66999999999999, 78.06, 78.08, 78.35]\n",
      "saved Active-learning-experiment-3.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 4, using model = NBModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
      "\n",
      "initial random chosen samples (250,)\n",
      "initial train set: (250, 784) (250,) unique(labels): [15 32 26 25 25 32 28 30 24 13] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,) (250,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 73.740000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.81      0.86       980\n",
      "         1.0       0.88      0.94      0.91      1135\n",
      "         2.0       0.85      0.73      0.79      1032\n",
      "         3.0       0.83      0.77      0.80      1010\n",
      "         4.0       0.72      0.81      0.76       982\n",
      "         5.0       0.75      0.60      0.67       892\n",
      "         6.0       0.82      0.79      0.80       958\n",
      "         7.0       0.66      0.79      0.72      1028\n",
      "         8.0       0.63      0.80      0.71       974\n",
      "         9.0       0.33      0.30      0.31      1009\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.74      0.73      0.73     10000\n",
      "weighted avg       0.74      0.74      0.74     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.733436\n",
      "--------------------------------\n",
      "val predicted: (59750,) [3. 0. 4. ... 5. 9. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [3 0 4 ... 5 9 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [34 39 42 43 56 60 34 48 62 82] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 80.160000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.96      0.94       980\n",
      "         1.0       0.71      0.96      0.82      1135\n",
      "         2.0       0.92      0.76      0.83      1032\n",
      "         3.0       0.90      0.74      0.81      1010\n",
      "         4.0       0.86      0.65      0.74       982\n",
      "         5.0       0.79      0.63      0.70       892\n",
      "         6.0       0.86      0.83      0.85       958\n",
      "         7.0       0.91      0.76      0.83      1028\n",
      "         8.0       0.69      0.82      0.75       974\n",
      "         9.0       0.64      0.87      0.74      1009\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.82      0.80      0.80     10000\n",
      "weighted avg       0.82      0.80      0.80     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.800265\n",
      "--------------------------------\n",
      "final active learning accuracies [73.74000000000001, 80.16]\n",
      "saved Active-learning-experiment-4.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 5, using model = NBModel, selection_function = MarginSamplingSelection, k = 100, iteration = 0.\n",
      "\n",
      "initial random chosen samples (100,)\n",
      "initial train set: (100, 784) (100,) unique(labels): [ 6 16 10 10  8  7 12 11 12  8] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,) (100,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 61.730000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.75      0.79       980\n",
      "         1.0       0.88      0.83      0.86      1135\n",
      "         2.0       0.91      0.59      0.72      1032\n",
      "         3.0       0.66      0.63      0.65      1010\n",
      "         4.0       0.29      0.44      0.35       982\n",
      "         5.0       0.77      0.28      0.41       892\n",
      "         6.0       0.69      0.76      0.72       958\n",
      "         7.0       0.58      0.59      0.59      1028\n",
      "         8.0       0.49      0.78      0.60       974\n",
      "         9.0       0.48      0.46      0.47      1009\n",
      "\n",
      "    accuracy                           0.62     10000\n",
      "   macro avg       0.66      0.61      0.62     10000\n",
      "weighted avg       0.66      0.62      0.62     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.616126\n",
      "--------------------------------\n",
      "val predicted: (59900,) [5. 0. 4. ... 5. 6. 5.]\n",
      "probabilities: (59900, 10) \n",
      " [5 0 4 ... 5 6 5]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [11 23 22 20 34 19 13 14 18 26] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 70.220000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.88      0.87       980\n",
      "         1.0       0.86      0.93      0.89      1135\n",
      "         2.0       0.80      0.59      0.68      1032\n",
      "         3.0       0.66      0.70      0.68      1010\n",
      "         4.0       0.77      0.69      0.73       982\n",
      "         5.0       0.75      0.46      0.57       892\n",
      "         6.0       0.47      0.79      0.59       958\n",
      "         7.0       0.81      0.56      0.66      1028\n",
      "         8.0       0.68      0.65      0.66       974\n",
      "         9.0       0.59      0.74      0.66      1009\n",
      "\n",
      "    accuracy                           0.70     10000\n",
      "   macro avg       0.72      0.70      0.70     10000\n",
      "weighted avg       0.73      0.70      0.70     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.698270\n",
      "--------------------------------\n",
      "val predicted: (59800,) [5. 0. 6. ... 5. 6. 8.]\n",
      "probabilities: (59800, 10) \n",
      " [5 0 6 ... 5 6 8]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [15 29 27 26 42 37 21 26 42 35] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.780000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.92      0.75       980\n",
      "         1.0       0.74      0.95      0.83      1135\n",
      "         2.0       0.94      0.63      0.76      1032\n",
      "         3.0       0.87      0.70      0.77      1010\n",
      "         4.0       0.90      0.64      0.75       982\n",
      "         5.0       0.81      0.60      0.69       892\n",
      "         6.0       0.84      0.85      0.85       958\n",
      "         7.0       0.89      0.63      0.74      1028\n",
      "         8.0       0.61      0.82      0.70       974\n",
      "         9.0       0.65      0.79      0.71      1009\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.79      0.76      0.76     10000\n",
      "weighted avg       0.79      0.76      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.755677\n",
      "--------------------------------\n",
      "val predicted: (59700,) [5. 0. 1. ... 5. 6. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [5 0 1 ... 5 6 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [18 33 41 35 57 55 26 34 54 47] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 78.900000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.93      0.83       980\n",
      "         1.0       0.72      0.96      0.82      1135\n",
      "         2.0       0.92      0.72      0.81      1032\n",
      "         3.0       0.93      0.68      0.78      1010\n",
      "         4.0       0.88      0.75      0.81       982\n",
      "         5.0       0.78      0.69      0.74       892\n",
      "         6.0       0.84      0.85      0.84       958\n",
      "         7.0       0.93      0.66      0.77      1028\n",
      "         8.0       0.69      0.79      0.74       974\n",
      "         9.0       0.67      0.84      0.74      1009\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.81      0.79      0.79     10000\n",
      "weighted avg       0.81      0.79      0.79     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.787881\n",
      "--------------------------------\n",
      "val predicted: (59600,) [5. 0. 0. ... 5. 6. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 0 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [22 36 54 50 75 66 30 48 63 56] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 80.900000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.95      0.90       980\n",
      "         1.0       0.69      0.96      0.81      1135\n",
      "         2.0       0.88      0.75      0.81      1032\n",
      "         3.0       0.91      0.75      0.82      1010\n",
      "         4.0       0.86      0.80      0.83       982\n",
      "         5.0       0.83      0.70      0.76       892\n",
      "         6.0       0.88      0.86      0.87       958\n",
      "         7.0       0.89      0.73      0.81      1028\n",
      "         8.0       0.71      0.79      0.75       974\n",
      "         9.0       0.71      0.78      0.74      1009\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.82      0.81      0.81     10000\n",
      "weighted avg       0.82      0.81      0.81     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.808948\n",
      "--------------------------------\n",
      "final active learning accuracies [61.73, 70.22, 75.78, 78.9, 80.9]\n",
      "saved Active-learning-experiment-5.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'sample_data']\n",
      "{\n",
      "  \"NBModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          61.73,\n",
      "          70.22,\n",
      "          75.78,\n",
      "          78.9,\n",
      "          80.9\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          73.74000000000001,\n",
      "          80.16\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          60.099999999999994,\n",
      "          70.30999999999999,\n",
      "          76.4,\n",
      "          78.11,\n",
      "          79.95\n",
      "        ]\n",
      "      ],\n",
      "      \"25\": [\n",
      "        [\n",
      "          32.79,\n",
      "          38.79,\n",
      "          48.91,\n",
      "          64.21,\n",
      "          64.91,\n",
      "          68.72,\n",
      "          68.23,\n",
      "          71.07,\n",
      "          72.53,\n",
      "          73.81,\n",
      "          74.53999999999999,\n",
      "          75.14,\n",
      "          75.26,\n",
      "          75.92999999999999,\n",
      "          76.14999999999999,\n",
      "          77.09,\n",
      "          77.66999999999999,\n",
      "          78.06,\n",
      "          78.08,\n",
      "          78.35\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          75.47,\n",
      "          81.58\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 6, using model = NBModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
      "\n",
      "initial random chosen samples (25,)\n",
      "initial train set: (25, 784) (25,) unique(labels): [3 2 2 3 2 1 3 1 3 5] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59975, 784) (59975,) (25,)\n",
      "\n",
      "Train set: (25, 784) y: (25,)\n",
      "Val   set: (59975, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 39.280000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.64      0.66       980\n",
      "         1.0       0.38      0.75      0.51      1135\n",
      "         2.0       0.92      0.19      0.32      1032\n",
      "         3.0       0.51      0.39      0.44      1010\n",
      "         4.0       0.55      0.19      0.29       982\n",
      "         5.0       0.15      0.45      0.23       892\n",
      "         6.0       0.72      0.50      0.59       958\n",
      "         7.0       0.52      0.40      0.45      1028\n",
      "         8.0       0.23      0.20      0.21       974\n",
      "         9.0       0.30      0.20      0.24      1009\n",
      "\n",
      "    accuracy                           0.39     10000\n",
      "   macro avg       0.50      0.39      0.39     10000\n",
      "weighted avg       0.50      0.39      0.40     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.393360\n",
      "--------------------------------\n",
      "val predicted: (59975,) [3. 0. 5. ... 5. 5. 8.]\n",
      "probabilities: (59975, 10) \n",
      " [3 0 5 ... 5 5 8]\n",
      "trainset before (25, 784) (25,)\n",
      "trainset after (50, 784) (50,)\n",
      "updated train set: (50, 784) (50,) unique(labels): [4 4 4 4 6 2 8 3 6 9] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59950, 784) (59950,)\n",
      "\n",
      "Train set: (50, 784) y: (50,)\n",
      "Val   set: (59950, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 54.820000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.84      0.84       980\n",
      "         1.0       0.88      0.80      0.84      1135\n",
      "         2.0       0.92      0.22      0.36      1032\n",
      "         3.0       0.91      0.39      0.55      1010\n",
      "         4.0       0.61      0.57      0.59       982\n",
      "         5.0       0.10      0.14      0.12       892\n",
      "         6.0       0.65      0.78      0.71       958\n",
      "         7.0       0.88      0.33      0.48      1028\n",
      "         8.0       0.37      0.77      0.50       974\n",
      "         9.0       0.39      0.61      0.48      1009\n",
      "\n",
      "    accuracy                           0.55     10000\n",
      "   macro avg       0.66      0.54      0.55     10000\n",
      "weighted avg       0.67      0.55      0.55     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.545389\n",
      "--------------------------------\n",
      "val predicted: (59950,) [8. 0. 5. ... 8. 6. 8.]\n",
      "probabilities: (59950, 10) \n",
      " [8 0 5 ... 8 6 8]\n",
      "trainset before (50, 784) (50,)\n",
      "trainset after (75, 784) (75,)\n",
      "updated train set: (75, 784) (75,) unique(labels): [ 4  7  5  8 12  3  9  7  6 14] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59925, 784) (59925,)\n",
      "\n",
      "Train set: (75, 784) y: (75,)\n",
      "Val   set: (59925, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 58.260000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.78      0.85       980\n",
      "         1.0       0.86      0.96      0.91      1135\n",
      "         2.0       0.96      0.26      0.41      1032\n",
      "         3.0       0.51      0.69      0.59      1010\n",
      "         4.0       0.55      0.63      0.59       982\n",
      "         5.0       0.11      0.17      0.13       892\n",
      "         6.0       0.78      0.75      0.77       958\n",
      "         7.0       0.88      0.52      0.65      1028\n",
      "         8.0       0.49      0.32      0.39       974\n",
      "         9.0       0.43      0.67      0.52      1009\n",
      "\n",
      "    accuracy                           0.58     10000\n",
      "   macro avg       0.65      0.57      0.58     10000\n",
      "weighted avg       0.66      0.58      0.59     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.579670\n",
      "--------------------------------\n",
      "val predicted: (59925,) [3. 0. 5. ... 8. 6. 8.]\n",
      "probabilities: (59925, 10) \n",
      " [3 0 5 ... 8 6 8]\n",
      "trainset before (75, 784) (75,)\n",
      "trainset after (100, 784) (100,)\n",
      "updated train set: (100, 784) (100,) unique(labels): [ 8  8  7  9 12  8  9 10 13 16] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 62.580000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.90      0.87       980\n",
      "         1.0       0.48      0.86      0.61      1135\n",
      "         2.0       0.93      0.32      0.48      1032\n",
      "         3.0       0.77      0.58      0.66      1010\n",
      "         4.0       0.77      0.58      0.66       982\n",
      "         5.0       0.43      0.15      0.23       892\n",
      "         6.0       0.89      0.73      0.80       958\n",
      "         7.0       0.91      0.60      0.72      1028\n",
      "         8.0       0.42      0.82      0.55       974\n",
      "         9.0       0.49      0.66      0.56      1009\n",
      "\n",
      "    accuracy                           0.63     10000\n",
      "   macro avg       0.69      0.62      0.61     10000\n",
      "weighted avg       0.69      0.63      0.62     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.614852\n",
      "--------------------------------\n",
      "val predicted: (59900,) [8. 0. 1. ... 8. 6. 8.]\n",
      "probabilities: (59900, 10) \n",
      " [8 0 1 ... 8 6 8]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (125, 784) (125,)\n",
      "updated train set: (125, 784) (125,) unique(labels): [ 9 13  9 10 17 13 11 10 15 18] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59875, 784) (59875,)\n",
      "\n",
      "Train set: (125, 784) y: (125,)\n",
      "Val   set: (59875, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 69.480000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.91      0.89       980\n",
      "         1.0       0.59      0.96      0.73      1135\n",
      "         2.0       0.93      0.45      0.61      1032\n",
      "         3.0       0.84      0.59      0.69      1010\n",
      "         4.0       0.65      0.71      0.68       982\n",
      "         5.0       0.78      0.33      0.47       892\n",
      "         6.0       0.91      0.81      0.85       958\n",
      "         7.0       0.86      0.68      0.76      1028\n",
      "         8.0       0.49      0.82      0.61       974\n",
      "         9.0       0.54      0.64      0.59      1009\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.75      0.69      0.69     10000\n",
      "weighted avg       0.75      0.69      0.69     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.688477\n",
      "--------------------------------\n",
      "val predicted: (59875,) [3. 0. 1. ... 5. 6. 8.]\n",
      "probabilities: (59875, 10) \n",
      " [3 0 1 ... 5 6 8]\n",
      "trainset before (125, 784) (125,)\n",
      "trainset after (150, 784) (150,)\n",
      "updated train set: (150, 784) (150,) unique(labels): [10 16 10 17 20 18 12 13 16 18] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59850, 784) (59850,)\n",
      "\n",
      "Train set: (150, 784) y: (150,)\n",
      "Val   set: (59850, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 6\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 74.180000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.86      0.89       980\n",
      "         1.0       0.73      0.98      0.84      1135\n",
      "         2.0       0.66      0.64      0.65      1032\n",
      "         3.0       0.77      0.71      0.74      1010\n",
      "         4.0       0.67      0.80      0.73       982\n",
      "         5.0       0.72      0.43      0.54       892\n",
      "         6.0       0.89      0.83      0.86       958\n",
      "         7.0       0.89      0.75      0.82      1028\n",
      "         8.0       0.61      0.76      0.68       974\n",
      "         9.0       0.65      0.59      0.62      1009\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.75      0.74      0.74     10000\n",
      "weighted avg       0.75      0.74      0.74     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.735097\n",
      "--------------------------------\n",
      "val predicted: (59850,) [3. 0. 2. ... 5. 6. 8.]\n",
      "probabilities: (59850, 10) \n",
      " [3 0 2 ... 5 6 8]\n",
      "trainset before (150, 784) (150,)\n",
      "trainset after (175, 784) (175,)\n",
      "updated train set: (175, 784) (175,) unique(labels): [11 16 11 18 22 28 13 13 22 21] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59825, 784) (59825,)\n",
      "\n",
      "Train set: (175, 784) y: (175,)\n",
      "Val   set: (59825, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 7\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 74.530000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.86      0.90       980\n",
      "         1.0       0.66      0.95      0.78      1135\n",
      "         2.0       0.88      0.49      0.63      1032\n",
      "         3.0       0.78      0.72      0.75      1010\n",
      "         4.0       0.71      0.76      0.74       982\n",
      "         5.0       0.72      0.59      0.65       892\n",
      "         6.0       0.92      0.85      0.88       958\n",
      "         7.0       0.89      0.74      0.81      1028\n",
      "         8.0       0.56      0.81      0.66       974\n",
      "         9.0       0.64      0.66      0.65      1009\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.77      0.74      0.74     10000\n",
      "weighted avg       0.77      0.75      0.74     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.744089\n",
      "--------------------------------\n",
      "val predicted: (59825,) [3. 0. 3. ... 8. 6. 8.]\n",
      "probabilities: (59825, 10) \n",
      " [3 0 3 ... 8 6 8]\n",
      "trainset before (175, 784) (175,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [13 16 11 19 27 31 14 16 25 28] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 8\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.280000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.86      0.89       980\n",
      "         1.0       0.62      0.95      0.75      1135\n",
      "         2.0       0.78      0.49      0.60      1032\n",
      "         3.0       0.81      0.71      0.76      1010\n",
      "         4.0       0.80      0.80      0.80       982\n",
      "         5.0       0.74      0.61      0.67       892\n",
      "         6.0       0.91      0.78      0.84       958\n",
      "         7.0       0.95      0.69      0.79      1028\n",
      "         8.0       0.57      0.83      0.68       974\n",
      "         9.0       0.70      0.78      0.74      1009\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.78      0.75      0.75     10000\n",
      "weighted avg       0.78      0.75      0.75     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.752562\n",
      "--------------------------------\n",
      "val predicted: (59800,) [5. 0. 2. ... 5. 6. 8.]\n",
      "probabilities: (59800, 10) \n",
      " [5 0 2 ... 5 6 8]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (225, 784) (225,)\n",
      "updated train set: (225, 784) (225,) unique(labels): [14 17 13 20 30 34 16 20 28 33] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59775, 784) (59775,)\n",
      "\n",
      "Train set: (225, 784) y: (225,)\n",
      "Val   set: (59775, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 9\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 77.160000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.90      0.92       980\n",
      "         1.0       0.63      0.95      0.76      1135\n",
      "         2.0       0.90      0.50      0.65      1032\n",
      "         3.0       0.84      0.72      0.77      1010\n",
      "         4.0       0.85      0.82      0.84       982\n",
      "         5.0       0.76      0.64      0.70       892\n",
      "         6.0       0.89      0.80      0.84       958\n",
      "         7.0       0.94      0.71      0.81      1028\n",
      "         8.0       0.57      0.81      0.67       974\n",
      "         9.0       0.71      0.84      0.77      1009\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.80      0.77      0.77     10000\n",
      "weighted avg       0.80      0.77      0.77     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.772135\n",
      "--------------------------------\n",
      "val predicted: (59775,) [5. 0. 2. ... 5. 6. 8.]\n",
      "probabilities: (59775, 10) \n",
      " [5 0 2 ... 5 6 8]\n",
      "trainset before (225, 784) (225,)\n",
      "trainset after (250, 784) (250,)\n",
      "updated train set: (250, 784) (250,) unique(labels): [15 17 14 23 33 40 19 23 31 35] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 10\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 76.380000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.90      0.91       980\n",
      "         1.0       0.61      0.93      0.74      1135\n",
      "         2.0       0.90      0.46      0.61      1032\n",
      "         3.0       0.85      0.70      0.77      1010\n",
      "         4.0       0.82      0.82      0.82       982\n",
      "         5.0       0.73      0.65      0.69       892\n",
      "         6.0       0.87      0.80      0.84       958\n",
      "         7.0       0.91      0.74      0.82      1028\n",
      "         8.0       0.58      0.82      0.68       974\n",
      "         9.0       0.74      0.79      0.76      1009\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.79      0.76      0.76     10000\n",
      "weighted avg       0.79      0.76      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.763273\n",
      "--------------------------------\n",
      "val predicted: (59750,) [5. 0. 2. ... 5. 6. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [5 0 2 ... 5 6 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (275, 784) (275,)\n",
      "updated train set: (275, 784) (275,) unique(labels): [18 18 16 25 37 42 21 25 36 37] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59725, 784) (59725,)\n",
      "\n",
      "Train set: (275, 784) y: (275,)\n",
      "Val   set: (59725, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 11\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.950000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.94      0.92       980\n",
      "         1.0       0.61      0.93      0.74      1135\n",
      "         2.0       0.94      0.48      0.64      1032\n",
      "         3.0       0.87      0.68      0.76      1010\n",
      "         4.0       0.77      0.84      0.80       982\n",
      "         5.0       0.77      0.65      0.71       892\n",
      "         6.0       0.88      0.80      0.84       958\n",
      "         7.0       0.89      0.75      0.81      1028\n",
      "         8.0       0.56      0.80      0.66       974\n",
      "         9.0       0.73      0.71      0.72      1009\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.79      0.76      0.76     10000\n",
      "weighted avg       0.79      0.76      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.759764\n",
      "--------------------------------\n",
      "val predicted: (59725,) [5. 0. 2. ... 5. 6. 8.]\n",
      "probabilities: (59725, 10) \n",
      " [5 0 2 ... 5 6 8]\n",
      "trainset before (275, 784) (275,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [18 19 19 29 37 46 22 27 40 43] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 12\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 77.930000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.94      0.92       980\n",
      "         1.0       0.61      0.93      0.73      1135\n",
      "         2.0       0.93      0.52      0.67      1032\n",
      "         3.0       0.87      0.73      0.79      1010\n",
      "         4.0       0.89      0.80      0.84       982\n",
      "         5.0       0.84      0.66      0.74       892\n",
      "         6.0       0.88      0.80      0.84       958\n",
      "         7.0       0.91      0.73      0.81      1028\n",
      "         8.0       0.59      0.81      0.68       974\n",
      "         9.0       0.72      0.85      0.78      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.81      0.78      0.78     10000\n",
      "weighted avg       0.81      0.78      0.78     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.781033\n",
      "--------------------------------\n",
      "val predicted: (59700,) [5. 0. 2. ... 5. 6. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [5 0 2 ... 5 6 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (325, 784) (325,)\n",
      "updated train set: (325, 784) (325,) unique(labels): [19 22 20 31 41 51 22 30 43 46] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59675, 784) (59675,)\n",
      "\n",
      "Train set: (325, 784) y: (325,)\n",
      "Val   set: (59675, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 13\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 78.460000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.95      0.93       980\n",
      "         1.0       0.62      0.96      0.76      1135\n",
      "         2.0       0.92      0.54      0.68      1032\n",
      "         3.0       0.86      0.73      0.79      1010\n",
      "         4.0       0.89      0.79      0.84       982\n",
      "         5.0       0.82      0.65      0.73       892\n",
      "         6.0       0.88      0.81      0.84       958\n",
      "         7.0       0.91      0.75      0.82      1028\n",
      "         8.0       0.60      0.79      0.68       974\n",
      "         9.0       0.73      0.85      0.79      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.81      0.78      0.79     10000\n",
      "weighted avg       0.81      0.78      0.78     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.785094\n",
      "--------------------------------\n",
      "val predicted: (59675,) [5. 0. 2. ... 5. 6. 8.]\n",
      "probabilities: (59675, 10) \n",
      " [5 0 2 ... 5 6 8]\n",
      "trainset before (325, 784) (325,)\n",
      "trainset after (350, 784) (350,)\n",
      "updated train set: (350, 784) (350,) unique(labels): [19 23 26 37 43 58 22 31 45 46] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59650, 784) (59650,)\n",
      "\n",
      "Train set: (350, 784) y: (350,)\n",
      "Val   set: (59650, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 14\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 80.320000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.95      0.93       980\n",
      "         1.0       0.67      0.96      0.79      1135\n",
      "         2.0       0.93      0.66      0.77      1032\n",
      "         3.0       0.88      0.75      0.81      1010\n",
      "         4.0       0.91      0.81      0.86       982\n",
      "         5.0       0.77      0.70      0.73       892\n",
      "         6.0       0.88      0.80      0.84       958\n",
      "         7.0       0.92      0.76      0.83      1028\n",
      "         8.0       0.62      0.78      0.69       974\n",
      "         9.0       0.76      0.85      0.80      1009\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.82      0.80      0.80     10000\n",
      "weighted avg       0.82      0.80      0.80     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.804345\n",
      "--------------------------------\n",
      "val predicted: (59650,) [5. 0. 0. ... 5. 6. 8.]\n",
      "probabilities: (59650, 10) \n",
      " [5 0 0 ... 5 6 8]\n",
      "trainset before (350, 784) (350,)\n",
      "trainset after (375, 784) (375,)\n",
      "updated train set: (375, 784) (375,) unique(labels): [20 26 27 43 44 60 23 34 51 47] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59625, 784) (59625,)\n",
      "\n",
      "Train set: (375, 784) y: (375,)\n",
      "Val   set: (59625, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 15\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 81.480000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.95      0.92       980\n",
      "         1.0       0.70      0.98      0.81      1135\n",
      "         2.0       0.93      0.67      0.78      1032\n",
      "         3.0       0.86      0.78      0.82      1010\n",
      "         4.0       0.90      0.82      0.86       982\n",
      "         5.0       0.83      0.68      0.75       892\n",
      "         6.0       0.86      0.85      0.86       958\n",
      "         7.0       0.89      0.76      0.82      1028\n",
      "         8.0       0.65      0.78      0.71       974\n",
      "         9.0       0.78      0.85      0.81      1009\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.83      0.81      0.81     10000\n",
      "weighted avg       0.83      0.81      0.81     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.814450\n",
      "--------------------------------\n",
      "val predicted: (59625,) [5. 0. 0. ... 5. 6. 8.]\n",
      "probabilities: (59625, 10) \n",
      " [5 0 0 ... 5 6 8]\n",
      "trainset before (375, 784) (375,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [21 26 29 47 50 64 24 35 56 48] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 16\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 81.920000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.95      0.93       980\n",
      "         1.0       0.68      0.98      0.80      1135\n",
      "         2.0       0.92      0.66      0.77      1032\n",
      "         3.0       0.84      0.80      0.82      1010\n",
      "         4.0       0.89      0.85      0.87       982\n",
      "         5.0       0.86      0.68      0.76       892\n",
      "         6.0       0.87      0.86      0.86       958\n",
      "         7.0       0.90      0.76      0.82      1028\n",
      "         8.0       0.67      0.79      0.73       974\n",
      "         9.0       0.80      0.83      0.82      1009\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.84      0.82      0.82     10000\n",
      "weighted avg       0.83      0.82      0.82     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.818980\n",
      "--------------------------------\n",
      "val predicted: (59600,) [5. 0. 6. ... 5. 6. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 6 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (425, 784) (425,)\n",
      "updated train set: (425, 784) (425,) unique(labels): [22 27 33 50 54 68 26 36 57 52] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59575, 784) (59575,)\n",
      "\n",
      "Train set: (425, 784) y: (425,)\n",
      "Val   set: (59575, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 17\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.720000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94       980\n",
      "         1.0       0.69      0.98      0.81      1135\n",
      "         2.0       0.92      0.69      0.79      1032\n",
      "         3.0       0.88      0.81      0.84      1010\n",
      "         4.0       0.90      0.85      0.87       982\n",
      "         5.0       0.84      0.70      0.77       892\n",
      "         6.0       0.89      0.86      0.88       958\n",
      "         7.0       0.91      0.76      0.83      1028\n",
      "         8.0       0.70      0.79      0.74       974\n",
      "         9.0       0.78      0.84      0.81      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.84      0.82      0.83     10000\n",
      "weighted avg       0.84      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.827369\n",
      "--------------------------------\n",
      "val predicted: (59575,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59575, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (425, 784) (425,)\n",
      "trainset after (450, 784) (450,)\n",
      "updated train set: (450, 784) (450,) unique(labels): [24 28 36 54 57 70 28 39 60 54] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59550, 784) (59550,)\n",
      "\n",
      "Train set: (450, 784) y: (450,)\n",
      "Val   set: (59550, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 18\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.260000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.96      0.93       980\n",
      "         1.0       0.68      0.98      0.80      1135\n",
      "         2.0       0.93      0.70      0.80      1032\n",
      "         3.0       0.89      0.79      0.84      1010\n",
      "         4.0       0.90      0.85      0.87       982\n",
      "         5.0       0.82      0.71      0.76       892\n",
      "         6.0       0.88      0.84      0.86       958\n",
      "         7.0       0.89      0.76      0.82      1028\n",
      "         8.0       0.71      0.79      0.75       974\n",
      "         9.0       0.78      0.83      0.80      1009\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.84      0.82      0.82     10000\n",
      "weighted avg       0.84      0.82      0.82     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.822891\n",
      "--------------------------------\n",
      "val predicted: (59550,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59550, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (450, 784) (450,)\n",
      "trainset after (475, 784) (475,)\n",
      "updated train set: (475, 784) (475,) unique(labels): [25 31 40 58 58 71 29 40 66 57] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59525, 784) (59525,)\n",
      "\n",
      "Train set: (475, 784) y: (475,)\n",
      "Val   set: (59525, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 19\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.360000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.95      0.92       980\n",
      "         1.0       0.69      0.98      0.81      1135\n",
      "         2.0       0.94      0.73      0.82      1032\n",
      "         3.0       0.89      0.79      0.83      1010\n",
      "         4.0       0.89      0.83      0.86       982\n",
      "         5.0       0.84      0.70      0.76       892\n",
      "         6.0       0.89      0.84      0.86       958\n",
      "         7.0       0.89      0.76      0.82      1028\n",
      "         8.0       0.70      0.80      0.75       974\n",
      "         9.0       0.77      0.84      0.80      1009\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.84      0.82      0.82     10000\n",
      "weighted avg       0.84      0.82      0.82     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.823657\n",
      "--------------------------------\n",
      "val predicted: (59525,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59525, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (475, 784) (475,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [25 31 42 62 61 77 33 43 67 59] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 20\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.340000 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91       980\n",
      "         1.0       0.69      0.98      0.81      1135\n",
      "         2.0       0.94      0.74      0.83      1032\n",
      "         3.0       0.89      0.79      0.84      1010\n",
      "         4.0       0.88      0.85      0.86       982\n",
      "         5.0       0.81      0.71      0.75       892\n",
      "         6.0       0.89      0.84      0.86       958\n",
      "         7.0       0.88      0.76      0.82      1028\n",
      "         8.0       0.72      0.78      0.75       974\n",
      "         9.0       0.78      0.83      0.80      1009\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.84      0.82      0.82     10000\n",
      "weighted avg       0.84      0.82      0.82     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.823428\n",
      "--------------------------------\n",
      "final active learning accuracies [39.28, 54.82, 58.26, 62.580000000000005, 69.48, 74.18, 74.53, 75.28, 77.16, 76.38000000000001, 75.94999999999999, 77.92999999999999, 78.46, 80.32000000000001, 81.47999999999999, 81.92, 82.72, 82.26, 82.36, 82.34]\n",
      "saved Active-learning-experiment-6.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 7, using model = SvmModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
      "\n",
      "initial random chosen samples (250,)\n",
      "initial train set: (250, 784) (250,) unique(labels): [32 27 26 28 21 18 28 30 21 19] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,) (250,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.770000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.95      0.93       980\n",
      "         1.0       0.86      0.98      0.92      1135\n",
      "         2.0       0.89      0.75      0.82      1032\n",
      "         3.0       0.72      0.81      0.76      1010\n",
      "         4.0       0.81      0.83      0.82       982\n",
      "         5.0       0.84      0.63      0.72       892\n",
      "         6.0       0.88      0.92      0.90       958\n",
      "         7.0       0.87      0.81      0.84      1028\n",
      "         8.0       0.75      0.82      0.78       974\n",
      "         9.0       0.77      0.73      0.75      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.82      0.82     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.824064\n",
      "--------------------------------\n",
      "val predicted: (59750,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [3 0 3 ... 5 6 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [50 55 47 49 48 39 58 61 48 45] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.120000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.96      0.94       980\n",
      "         1.0       0.89      0.99      0.94      1135\n",
      "         2.0       0.92      0.85      0.88      1032\n",
      "         3.0       0.79      0.84      0.82      1010\n",
      "         4.0       0.88      0.87      0.87       982\n",
      "         5.0       0.83      0.75      0.79       892\n",
      "         6.0       0.90      0.94      0.92       958\n",
      "         7.0       0.87      0.88      0.87      1028\n",
      "         8.0       0.87      0.80      0.83       974\n",
      "         9.0       0.83      0.83      0.83      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.868785\n",
      "--------------------------------\n",
      "final active learning accuracies [82.77, 87.12]\n",
      "saved Active-learning-experiment-7.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 8, using model = SvmModel, selection_function = RandomSelection, k = 100, iteration = 0.\n",
      "\n",
      "initial random chosen samples (100,)\n",
      "initial train set: (100, 784) (100,) unique(labels): [11 11 13  7 12  6 12  5 14  9] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,) (100,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.280000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.88       980\n",
      "         1.0       0.71      0.98      0.82      1135\n",
      "         2.0       0.88      0.66      0.76      1032\n",
      "         3.0       0.79      0.63      0.70      1010\n",
      "         4.0       0.73      0.72      0.73       982\n",
      "         5.0       0.82      0.54      0.65       892\n",
      "         6.0       0.78      0.92      0.85       958\n",
      "         7.0       0.91      0.57      0.70      1028\n",
      "         8.0       0.68      0.75      0.71       974\n",
      "         9.0       0.59      0.79      0.68      1009\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.77      0.75      0.75     10000\n",
      "weighted avg       0.77      0.75      0.75     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.746893\n",
      "--------------------------------\n",
      "val predicted: (59900,) [5. 0. 4. ... 5. 6. 5.]\n",
      "probabilities: (59900, 10) \n",
      " [8 0 4 ... 5 6 8]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [22 24 25 14 20 14 26 13 25 17] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.830000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.95      0.91       980\n",
      "         1.0       0.81      0.98      0.89      1135\n",
      "         2.0       0.88      0.76      0.81      1032\n",
      "         3.0       0.83      0.81      0.82      1010\n",
      "         4.0       0.82      0.77      0.79       982\n",
      "         5.0       0.87      0.67      0.75       892\n",
      "         6.0       0.84      0.94      0.89       958\n",
      "         7.0       0.92      0.79      0.85      1028\n",
      "         8.0       0.79      0.76      0.78       974\n",
      "         9.0       0.71      0.82      0.76      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.825188\n",
      "--------------------------------\n",
      "val predicted: (59800,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59800, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [33 34 33 26 28 23 34 24 36 29] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 85.690000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.93       980\n",
      "         1.0       0.84      0.99      0.91      1135\n",
      "         2.0       0.85      0.75      0.80      1032\n",
      "         3.0       0.83      0.84      0.84      1010\n",
      "         4.0       0.88      0.82      0.85       982\n",
      "         5.0       0.86      0.76      0.81       892\n",
      "         6.0       0.88      0.92      0.90       958\n",
      "         7.0       0.93      0.84      0.88      1028\n",
      "         8.0       0.80      0.80      0.80       974\n",
      "         9.0       0.80      0.85      0.83      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.85      0.85     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.854785\n",
      "--------------------------------\n",
      "val predicted: (59700,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [41 46 41 34 36 29 44 39 45 45] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.610000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.96      0.93       980\n",
      "         1.0       0.87      0.98      0.93      1135\n",
      "         2.0       0.87      0.76      0.81      1032\n",
      "         3.0       0.84      0.85      0.84      1010\n",
      "         4.0       0.87      0.85      0.86       982\n",
      "         5.0       0.85      0.77      0.80       892\n",
      "         6.0       0.87      0.94      0.90       958\n",
      "         7.0       0.92      0.88      0.90      1028\n",
      "         8.0       0.82      0.81      0.81       974\n",
      "         9.0       0.83      0.85      0.84      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.86      0.86     10000\n",
      "weighted avg       0.87      0.87      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.863611\n",
      "--------------------------------\n",
      "val predicted: (59600,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [48 58 47 45 48 37 56 53 52 56] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.770000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.96      0.94       980\n",
      "         1.0       0.89      0.98      0.93      1135\n",
      "         2.0       0.89      0.78      0.83      1032\n",
      "         3.0       0.87      0.86      0.86      1010\n",
      "         4.0       0.88      0.88      0.88       982\n",
      "         5.0       0.87      0.80      0.84       892\n",
      "         6.0       0.88      0.94      0.91       958\n",
      "         7.0       0.92      0.89      0.90      1028\n",
      "         8.0       0.81      0.81      0.81       974\n",
      "         9.0       0.86      0.85      0.85      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.875625\n",
      "--------------------------------\n",
      "final active learning accuracies [75.28, 82.83, 85.69, 86.61, 87.77000000000001]\n",
      "saved Active-learning-experiment-8.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 9, using model = SvmModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
      "\n",
      "initial random chosen samples (25,)\n",
      "initial train set: (25, 784) (25,) unique(labels): [1 1 4 4 3 1 1 3 3 4] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59975, 784) (59975,) (25,)\n",
      "\n",
      "Train set: (25, 784) y: (25,)\n",
      "Val   set: (59975, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 49.830000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.39      0.55       980\n",
      "         1.0       0.82      0.42      0.56      1135\n",
      "         2.0       0.61      0.61      0.61      1032\n",
      "         3.0       0.56      0.78      0.65      1010\n",
      "         4.0       0.34      0.46      0.39       982\n",
      "         5.0       0.40      0.31      0.35       892\n",
      "         6.0       0.45      0.25      0.32       958\n",
      "         7.0       0.88      0.65      0.75      1028\n",
      "         8.0       0.25      0.43      0.32       974\n",
      "         9.0       0.39      0.64      0.49      1009\n",
      "\n",
      "    accuracy                           0.50     10000\n",
      "   macro avg       0.57      0.49      0.50     10000\n",
      "weighted avg       0.57      0.50      0.50     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.499585\n",
      "--------------------------------\n",
      "val predicted: (59975,) [6. 0. 3. ... 6. 6. 9.]\n",
      "probabilities: (59975, 10) \n",
      " [3 3 3 ... 3 8 4]\n",
      "trainset before (25, 784) (25,)\n",
      "trainset after (50, 784) (50,)\n",
      "updated train set: (50, 784) (50,) unique(labels): [3 7 8 5 7 2 2 3 7 6] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59950, 784) (59950,)\n",
      "\n",
      "Train set: (50, 784) y: (50,)\n",
      "Val   set: (59950, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 66.040000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.85      0.87       980\n",
      "         1.0       0.65      0.99      0.79      1135\n",
      "         2.0       0.73      0.64      0.68      1032\n",
      "         3.0       0.64      0.71      0.67      1010\n",
      "         4.0       0.47      0.80      0.59       982\n",
      "         5.0       0.91      0.19      0.31       892\n",
      "         6.0       0.84      0.35      0.50       958\n",
      "         7.0       0.96      0.62      0.75      1028\n",
      "         8.0       0.53      0.70      0.61       974\n",
      "         9.0       0.60      0.65      0.62      1009\n",
      "\n",
      "    accuracy                           0.66     10000\n",
      "   macro avg       0.72      0.65      0.64     10000\n",
      "weighted avg       0.72      0.66      0.65     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.638954\n",
      "--------------------------------\n",
      "val predicted: (59950,) [3. 0. 3. ... 9. 6. 4.]\n",
      "probabilities: (59950, 10) \n",
      " [3 0 3 ... 8 4 4]\n",
      "trainset before (50, 784) (50,)\n",
      "trainset after (75, 784) (75,)\n",
      "updated train set: (75, 784) (75,) unique(labels): [ 6  8 12  8  8  4  4  7  9  9] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59925, 784) (59925,)\n",
      "\n",
      "Train set: (75, 784) y: (75,)\n",
      "Val   set: (59925, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.890000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88       980\n",
      "         1.0       0.74      0.99      0.85      1135\n",
      "         2.0       0.81      0.71      0.76      1032\n",
      "         3.0       0.65      0.75      0.70      1010\n",
      "         4.0       0.64      0.76      0.70       982\n",
      "         5.0       0.83      0.40      0.54       892\n",
      "         6.0       0.92      0.75      0.83       958\n",
      "         7.0       0.88      0.85      0.86      1028\n",
      "         8.0       0.65      0.67      0.66       974\n",
      "         9.0       0.71      0.75      0.73      1009\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.77      0.75      0.75     10000\n",
      "weighted avg       0.77      0.76      0.75     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.750840\n",
      "--------------------------------\n",
      "val predicted: (59925,) [3. 0. 3. ... 5. 4. 4.]\n",
      "probabilities: (59925, 10) \n",
      " [3 0 3 ... 9 4 4]\n",
      "trainset before (75, 784) (75,)\n",
      "trainset after (100, 784) (100,)\n",
      "updated train set: (100, 784) (100,) unique(labels): [ 6 14 13 11 10  6  7 11 10 12] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 77.850000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.87      0.89       980\n",
      "         1.0       0.71      0.99      0.82      1135\n",
      "         2.0       0.87      0.71      0.78      1032\n",
      "         3.0       0.70      0.77      0.74      1010\n",
      "         4.0       0.71      0.81      0.76       982\n",
      "         5.0       0.78      0.53      0.63       892\n",
      "         6.0       0.94      0.84      0.89       958\n",
      "         7.0       0.88      0.83      0.85      1028\n",
      "         8.0       0.66      0.64      0.65       974\n",
      "         9.0       0.73      0.75      0.74      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.79      0.77      0.77     10000\n",
      "weighted avg       0.79      0.78      0.78     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.774447\n",
      "--------------------------------\n",
      "val predicted: (59900,) [3. 0. 3. ... 5. 6. 4.]\n",
      "probabilities: (59900, 10) \n",
      " [3 0 3 ... 5 6 4]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (125, 784) (125,)\n",
      "updated train set: (125, 784) (125,) unique(labels): [ 7 18 16 14 10  8  9 13 14 16] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59875, 784) (59875,)\n",
      "\n",
      "Train set: (125, 784) y: (125,)\n",
      "Val   set: (59875, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 79.780000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.86      0.89       980\n",
      "         1.0       0.75      0.98      0.85      1135\n",
      "         2.0       0.80      0.71      0.75      1032\n",
      "         3.0       0.76      0.79      0.77      1010\n",
      "         4.0       0.77      0.77      0.77       982\n",
      "         5.0       0.76      0.62      0.68       892\n",
      "         6.0       0.91      0.85      0.88       958\n",
      "         7.0       0.90      0.83      0.87      1028\n",
      "         8.0       0.73      0.70      0.72       974\n",
      "         9.0       0.73      0.81      0.77      1009\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.80      0.79      0.79     10000\n",
      "weighted avg       0.80      0.80      0.80     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.794921\n",
      "--------------------------------\n",
      "val predicted: (59875,) [3. 0. 3. ... 5. 6. 8.]\n",
      "probabilities: (59875, 10) \n",
      " [3 0 3 ... 5 6 8]\n",
      "trainset before (125, 784) (125,)\n",
      "trainset after (150, 784) (150,)\n",
      "updated train set: (150, 784) (150,) unique(labels): [ 7 19 23 16 11 10 10 18 17 19] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59850, 784) (59850,)\n",
      "\n",
      "Train set: (150, 784) y: (150,)\n",
      "Val   set: (59850, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 6\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 81.670000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.85      0.89       980\n",
      "         1.0       0.78      0.98      0.87      1135\n",
      "         2.0       0.79      0.83      0.81      1032\n",
      "         3.0       0.79      0.76      0.77      1010\n",
      "         4.0       0.81      0.80      0.80       982\n",
      "         5.0       0.75      0.69      0.72       892\n",
      "         6.0       0.91      0.85      0.88       958\n",
      "         7.0       0.87      0.87      0.87      1028\n",
      "         8.0       0.79      0.68      0.73       974\n",
      "         9.0       0.76      0.82      0.79      1009\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.81      0.81     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.813952\n",
      "--------------------------------\n",
      "val predicted: (59850,) [5. 0. 9. ... 5. 6. 8.]\n",
      "probabilities: (59850, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (150, 784) (150,)\n",
      "trainset after (175, 784) (175,)\n",
      "updated train set: (175, 784) (175,) unique(labels): [ 8 19 25 21 17 12 14 21 18 20] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59825, 784) (59825,)\n",
      "\n",
      "Train set: (175, 784) y: (175,)\n",
      "Val   set: (59825, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 7\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.980000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.84      0.90       980\n",
      "         1.0       0.81      0.98      0.89      1135\n",
      "         2.0       0.82      0.83      0.83      1032\n",
      "         3.0       0.75      0.82      0.79      1010\n",
      "         4.0       0.80      0.88      0.84       982\n",
      "         5.0       0.73      0.70      0.71       892\n",
      "         6.0       0.90      0.84      0.87       958\n",
      "         7.0       0.88      0.87      0.88      1028\n",
      "         8.0       0.84      0.71      0.77       974\n",
      "         9.0       0.81      0.80      0.80      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.827281\n",
      "--------------------------------\n",
      "val predicted: (59825,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59825, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (175, 784) (175,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [14 20 25 22 19 18 17 23 20 22] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 8\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 83.860000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.90      0.92       980\n",
      "         1.0       0.82      0.98      0.89      1135\n",
      "         2.0       0.85      0.82      0.84      1032\n",
      "         3.0       0.78      0.82      0.80      1010\n",
      "         4.0       0.81      0.87      0.84       982\n",
      "         5.0       0.79      0.73      0.76       892\n",
      "         6.0       0.87      0.87      0.87       958\n",
      "         7.0       0.87      0.87      0.87      1028\n",
      "         8.0       0.85      0.71      0.77       974\n",
      "         9.0       0.82      0.79      0.80      1009\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.835841\n",
      "--------------------------------\n",
      "val predicted: (59800,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59800, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (225, 784) (225,)\n",
      "updated train set: (225, 784) (225,) unique(labels): [18 22 28 24 19 20 19 23 26 26] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59775, 784) (59775,)\n",
      "\n",
      "Train set: (225, 784) y: (225,)\n",
      "Val   set: (59775, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 9\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 84.650000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92       980\n",
      "         1.0       0.83      0.99      0.90      1135\n",
      "         2.0       0.86      0.84      0.85      1032\n",
      "         3.0       0.80      0.79      0.79      1010\n",
      "         4.0       0.83      0.86      0.84       982\n",
      "         5.0       0.80      0.69      0.74       892\n",
      "         6.0       0.88      0.87      0.88       958\n",
      "         7.0       0.89      0.86      0.87      1028\n",
      "         8.0       0.85      0.79      0.82       974\n",
      "         9.0       0.82      0.80      0.81      1009\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.84      0.84     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.843474\n",
      "--------------------------------\n",
      "val predicted: (59775,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59775, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (225, 784) (225,)\n",
      "trainset after (250, 784) (250,)\n",
      "updated train set: (250, 784) (250,) unique(labels): [21 27 30 27 20 22 19 27 29 28] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 10\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 84.860000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92       980\n",
      "         1.0       0.85      0.99      0.91      1135\n",
      "         2.0       0.88      0.83      0.85      1032\n",
      "         3.0       0.79      0.80      0.79      1010\n",
      "         4.0       0.84      0.86      0.85       982\n",
      "         5.0       0.82      0.68      0.74       892\n",
      "         6.0       0.90      0.87      0.88       958\n",
      "         7.0       0.87      0.88      0.88      1028\n",
      "         8.0       0.82      0.81      0.81       974\n",
      "         9.0       0.82      0.80      0.81      1009\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.845213\n",
      "--------------------------------\n",
      "val predicted: (59750,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (275, 784) (275,)\n",
      "updated train set: (275, 784) (275,) unique(labels): [23 31 32 32 22 24 23 29 29 30] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59725, 784) (59725,)\n",
      "\n",
      "Train set: (275, 784) y: (275,)\n",
      "Val   set: (59725, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 11\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 84.930000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92       980\n",
      "         1.0       0.86      0.99      0.92      1135\n",
      "         2.0       0.87      0.82      0.84      1032\n",
      "         3.0       0.76      0.81      0.79      1010\n",
      "         4.0       0.84      0.87      0.86       982\n",
      "         5.0       0.81      0.65      0.72       892\n",
      "         6.0       0.91      0.89      0.90       958\n",
      "         7.0       0.87      0.88      0.88      1028\n",
      "         8.0       0.83      0.80      0.82       974\n",
      "         9.0       0.82      0.80      0.81      1009\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.845566\n",
      "--------------------------------\n",
      "val predicted: (59725,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59725, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (275, 784) (275,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [27 33 32 33 27 26 25 29 34 34] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 12\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 85.250000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.93       980\n",
      "         1.0       0.87      0.98      0.92      1135\n",
      "         2.0       0.87      0.82      0.84      1032\n",
      "         3.0       0.79      0.81      0.80      1010\n",
      "         4.0       0.82      0.86      0.84       982\n",
      "         5.0       0.83      0.69      0.75       892\n",
      "         6.0       0.90      0.89      0.90       958\n",
      "         7.0       0.88      0.87      0.88      1028\n",
      "         8.0       0.84      0.82      0.83       974\n",
      "         9.0       0.81      0.80      0.80      1009\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.849460\n",
      "--------------------------------\n",
      "val predicted: (59700,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (325, 784) (325,)\n",
      "updated train set: (325, 784) (325,) unique(labels): [29 37 36 35 30 29 26 30 36 37] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59675, 784) (59675,)\n",
      "\n",
      "Train set: (325, 784) y: (325,)\n",
      "Val   set: (59675, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 13\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.160000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.95      0.94       980\n",
      "         1.0       0.86      0.98      0.92      1135\n",
      "         2.0       0.87      0.83      0.85      1032\n",
      "         3.0       0.82      0.81      0.82      1010\n",
      "         4.0       0.83      0.87      0.85       982\n",
      "         5.0       0.84      0.72      0.77       892\n",
      "         6.0       0.91      0.90      0.90       958\n",
      "         7.0       0.89      0.88      0.89      1028\n",
      "         8.0       0.85      0.83      0.84       974\n",
      "         9.0       0.82      0.81      0.82      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.859060\n",
      "--------------------------------\n",
      "val predicted: (59675,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59675, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (325, 784) (325,)\n",
      "trainset after (350, 784) (350,)\n",
      "updated train set: (350, 784) (350,) unique(labels): [31 38 39 36 33 30 29 32 39 43] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59650, 784) (59650,)\n",
      "\n",
      "Train set: (350, 784) y: (350,)\n",
      "Val   set: (59650, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 14\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.340000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.95      0.94       980\n",
      "         1.0       0.88      0.98      0.93      1135\n",
      "         2.0       0.87      0.84      0.85      1032\n",
      "         3.0       0.81      0.81      0.81      1010\n",
      "         4.0       0.83      0.88      0.85       982\n",
      "         5.0       0.84      0.72      0.78       892\n",
      "         6.0       0.91      0.89      0.90       958\n",
      "         7.0       0.90      0.88      0.89      1028\n",
      "         8.0       0.85      0.82      0.83       974\n",
      "         9.0       0.82      0.83      0.82      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.860842\n",
      "--------------------------------\n",
      "val predicted: (59650,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59650, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (350, 784) (350,)\n",
      "trainset after (375, 784) (375,)\n",
      "updated train set: (375, 784) (375,) unique(labels): [35 41 42 39 35 31 31 37 40 44] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59625, 784) (59625,)\n",
      "\n",
      "Train set: (375, 784) y: (375,)\n",
      "Val   set: (59625, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 15\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.160000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.96      0.94       980\n",
      "         1.0       0.88      0.98      0.93      1135\n",
      "         2.0       0.88      0.86      0.87      1032\n",
      "         3.0       0.82      0.85      0.83      1010\n",
      "         4.0       0.84      0.88      0.86       982\n",
      "         5.0       0.85      0.74      0.79       892\n",
      "         6.0       0.91      0.90      0.90       958\n",
      "         7.0       0.89      0.89      0.89      1028\n",
      "         8.0       0.87      0.82      0.84       974\n",
      "         9.0       0.83      0.83      0.83      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.869270\n",
      "--------------------------------\n",
      "val predicted: (59625,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59625, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (375, 784) (375,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [39 44 44 41 37 35 35 37 42 46] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 16\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.660000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.96      0.94       980\n",
      "         1.0       0.87      0.98      0.92      1135\n",
      "         2.0       0.88      0.86      0.87      1032\n",
      "         3.0       0.82      0.83      0.82      1010\n",
      "         4.0       0.84      0.86      0.85       982\n",
      "         5.0       0.83      0.74      0.78       892\n",
      "         6.0       0.91      0.89      0.90       958\n",
      "         7.0       0.89      0.89      0.89      1028\n",
      "         8.0       0.88      0.80      0.84       974\n",
      "         9.0       0.82      0.83      0.83      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.86      0.86     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.864159\n",
      "--------------------------------\n",
      "val predicted: (59600,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (425, 784) (425,)\n",
      "updated train set: (425, 784) (425,) unique(labels): [39 47 48 46 39 36 37 39 47 47] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59575, 784) (59575,)\n",
      "\n",
      "Train set: (425, 784) y: (425,)\n",
      "Val   set: (59575, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 17\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.640000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.95      0.94       980\n",
      "         1.0       0.89      0.99      0.93      1135\n",
      "         2.0       0.88      0.87      0.88      1032\n",
      "         3.0       0.82      0.86      0.84      1010\n",
      "         4.0       0.85      0.87      0.86       982\n",
      "         5.0       0.84      0.74      0.78       892\n",
      "         6.0       0.91      0.90      0.91       958\n",
      "         7.0       0.89      0.89      0.89      1028\n",
      "         8.0       0.91      0.83      0.87       974\n",
      "         9.0       0.84      0.83      0.84      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.873994\n",
      "--------------------------------\n",
      "val predicted: (59575,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59575, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (425, 784) (425,)\n",
      "trainset after (450, 784) (450,)\n",
      "updated train set: (450, 784) (450,) unique(labels): [39 48 50 49 42 37 40 42 54 49] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59550, 784) (59550,)\n",
      "\n",
      "Train set: (450, 784) y: (450,)\n",
      "Val   set: (59550, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 18\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.410000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.95      0.94       980\n",
      "         1.0       0.89      0.99      0.93      1135\n",
      "         2.0       0.88      0.87      0.88      1032\n",
      "         3.0       0.83      0.86      0.84      1010\n",
      "         4.0       0.85      0.87      0.86       982\n",
      "         5.0       0.83      0.73      0.78       892\n",
      "         6.0       0.91      0.91      0.91       958\n",
      "         7.0       0.88      0.89      0.89      1028\n",
      "         8.0       0.91      0.84      0.87       974\n",
      "         9.0       0.84      0.81      0.83      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.871565\n",
      "--------------------------------\n",
      "val predicted: (59550,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59550, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (450, 784) (450,)\n",
      "trainset after (475, 784) (475,)\n",
      "updated train set: (475, 784) (475,) unique(labels): [42 53 52 52 44 39 40 43 56 54] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59525, 784) (59525,)\n",
      "\n",
      "Train set: (475, 784) y: (475,)\n",
      "Val   set: (59525, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 19\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.480000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.96      0.95       980\n",
      "         1.0       0.89      0.98      0.93      1135\n",
      "         2.0       0.86      0.87      0.87      1032\n",
      "         3.0       0.83      0.86      0.85      1010\n",
      "         4.0       0.85      0.86      0.85       982\n",
      "         5.0       0.83      0.74      0.78       892\n",
      "         6.0       0.91      0.91      0.91       958\n",
      "         7.0       0.89      0.88      0.89      1028\n",
      "         8.0       0.91      0.85      0.88       974\n",
      "         9.0       0.83      0.82      0.83      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.872461\n",
      "--------------------------------\n",
      "val predicted: (59525,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59525, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (475, 784) (475,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [44 56 53 53 48 42 43 48 57 56] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 20\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.250000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.95      0.94       980\n",
      "         1.0       0.90      0.98      0.94      1135\n",
      "         2.0       0.87      0.87      0.87      1032\n",
      "         3.0       0.83      0.85      0.84      1010\n",
      "         4.0       0.84      0.86      0.85       982\n",
      "         5.0       0.82      0.73      0.77       892\n",
      "         6.0       0.90      0.89      0.90       958\n",
      "         7.0       0.89      0.89      0.89      1028\n",
      "         8.0       0.90      0.84      0.87       974\n",
      "         9.0       0.84      0.82      0.83      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.869879\n",
      "--------------------------------\n",
      "final active learning accuracies [49.830000000000005, 66.03999999999999, 75.89, 77.85, 79.78, 81.67, 82.98, 83.86, 84.65, 84.86, 84.93, 85.25, 86.16, 86.33999999999999, 87.16000000000001, 86.66, 87.64, 87.41, 87.48, 87.25]\n",
      "saved Active-learning-experiment-9.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 10, using model = SvmModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
      "\n",
      "initial random chosen samples (250,)\n",
      "initial train set: (250, 784) (250,) unique(labels): [28 26 31 16 24 20 24 31 24 26] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,) (250,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 83.300000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.93      0.92       980\n",
      "         1.0       0.84      0.99      0.90      1135\n",
      "         2.0       0.77      0.79      0.78      1032\n",
      "         3.0       0.88      0.75      0.81      1010\n",
      "         4.0       0.83      0.87      0.85       982\n",
      "         5.0       0.79      0.72      0.75       892\n",
      "         6.0       0.91      0.83      0.87       958\n",
      "         7.0       0.85      0.84      0.85      1028\n",
      "         8.0       0.78      0.76      0.77       974\n",
      "         9.0       0.79      0.82      0.80      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.830342\n",
      "--------------------------------\n",
      "val predicted: (59750,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [48 29 56 65 49 60 42 52 53 46] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.470000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.96      0.94       980\n",
      "         1.0       0.96      0.98      0.97      1135\n",
      "         2.0       0.84      0.84      0.84      1032\n",
      "         3.0       0.83      0.87      0.85      1010\n",
      "         4.0       0.83      0.91      0.87       982\n",
      "         5.0       0.87      0.75      0.81       892\n",
      "         6.0       0.94      0.89      0.91       958\n",
      "         7.0       0.88      0.90      0.89      1028\n",
      "         8.0       0.82      0.81      0.81       974\n",
      "         9.0       0.86      0.82      0.84      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.87      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.872457\n",
      "--------------------------------\n",
      "final active learning accuracies [83.3, 87.47]\n",
      "saved Active-learning-experiment-10.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-10.pkl', 'sample_data']\n",
      "{\n",
      "  \"NBModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          61.73,\n",
      "          70.22,\n",
      "          75.78,\n",
      "          78.9,\n",
      "          80.9\n",
      "        ]\n",
      "      ],\n",
      "      \"25\": [\n",
      "        [\n",
      "          39.28,\n",
      "          54.82,\n",
      "          58.26,\n",
      "          62.580000000000005,\n",
      "          69.48,\n",
      "          74.18,\n",
      "          74.53,\n",
      "          75.28,\n",
      "          77.16,\n",
      "          76.38000000000001,\n",
      "          75.94999999999999,\n",
      "          77.92999999999999,\n",
      "          78.46,\n",
      "          80.32000000000001,\n",
      "          81.47999999999999,\n",
      "          81.92,\n",
      "          82.72,\n",
      "          82.26,\n",
      "          82.36,\n",
      "          82.34\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          73.74000000000001,\n",
      "          80.16\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          60.099999999999994,\n",
      "          70.30999999999999,\n",
      "          76.4,\n",
      "          78.11,\n",
      "          79.95\n",
      "        ]\n",
      "      ],\n",
      "      \"25\": [\n",
      "        [\n",
      "          32.79,\n",
      "          38.79,\n",
      "          48.91,\n",
      "          64.21,\n",
      "          64.91,\n",
      "          68.72,\n",
      "          68.23,\n",
      "          71.07,\n",
      "          72.53,\n",
      "          73.81,\n",
      "          74.53999999999999,\n",
      "          75.14,\n",
      "          75.26,\n",
      "          75.92999999999999,\n",
      "          76.14999999999999,\n",
      "          77.09,\n",
      "          77.66999999999999,\n",
      "          78.06,\n",
      "          78.08,\n",
      "          78.35\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          75.47,\n",
      "          81.58\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"SvmModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"250\": [\n",
      "        [\n",
      "          83.3,\n",
      "          87.47\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          75.28,\n",
      "          82.83,\n",
      "          85.69,\n",
      "          86.61,\n",
      "          87.77000000000001\n",
      "        ]\n",
      "      ],\n",
      "      \"25\": [\n",
      "        [\n",
      "          49.830000000000005,\n",
      "          66.03999999999999,\n",
      "          75.89,\n",
      "          77.85,\n",
      "          79.78,\n",
      "          81.67,\n",
      "          82.98,\n",
      "          83.86,\n",
      "          84.65,\n",
      "          84.86,\n",
      "          84.93,\n",
      "          85.25,\n",
      "          86.16,\n",
      "          86.33999999999999,\n",
      "          87.16000000000001,\n",
      "          86.66,\n",
      "          87.64,\n",
      "          87.41,\n",
      "          87.48,\n",
      "          87.25\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          82.77,\n",
      "          87.12\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 11, using model = SvmModel, selection_function = MarginSamplingSelection, k = 100, iteration = 0.\n",
      "\n",
      "initial random chosen samples (100,)\n",
      "initial train set: (100, 784) (100,) unique(labels): [ 9 12 10 10 12 10 11 13  6  7] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,) (100,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 76.610000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.90      0.89       980\n",
      "         1.0       0.80      0.98      0.88      1135\n",
      "         2.0       0.86      0.68      0.76      1032\n",
      "         3.0       0.64      0.79      0.71      1010\n",
      "         4.0       0.61      0.92      0.73       982\n",
      "         5.0       0.70      0.72      0.71       892\n",
      "         6.0       0.85      0.87      0.86       958\n",
      "         7.0       0.82      0.84      0.83      1028\n",
      "         8.0       0.81      0.37      0.51       974\n",
      "         9.0       0.85      0.57      0.68      1009\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.78      0.76      0.76     10000\n",
      "weighted avg       0.78      0.77      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.755744\n",
      "--------------------------------\n",
      "val predicted: (59900,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59900, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [13 17 23 21 14 17 28 20 26 21] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 83.610000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94       980\n",
      "         1.0       0.88      0.97      0.92      1135\n",
      "         2.0       0.82      0.74      0.78      1032\n",
      "         3.0       0.78      0.80      0.79      1010\n",
      "         4.0       0.83      0.87      0.85       982\n",
      "         5.0       0.78      0.73      0.76       892\n",
      "         6.0       0.84      0.89      0.86       958\n",
      "         7.0       0.88      0.85      0.86      1028\n",
      "         8.0       0.78      0.74      0.76       974\n",
      "         9.0       0.81      0.80      0.81      1009\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.84      0.84      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.833171\n",
      "--------------------------------\n",
      "val predicted: (59800,) [3. 0. 9. ... 5. 6. 8.]\n",
      "probabilities: (59800, 10) \n",
      " [3 0 9 ... 5 6 8]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [16 17 30 35 32 34 32 31 39 34] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.000000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.91      0.93       980\n",
      "         1.0       0.92      0.96      0.94      1135\n",
      "         2.0       0.87      0.78      0.82      1032\n",
      "         3.0       0.80      0.89      0.84      1010\n",
      "         4.0       0.81      0.92      0.86       982\n",
      "         5.0       0.79      0.75      0.77       892\n",
      "         6.0       0.88      0.92      0.90       958\n",
      "         7.0       0.89      0.86      0.88      1028\n",
      "         8.0       0.86      0.78      0.82       974\n",
      "         9.0       0.83      0.80      0.81      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.857621\n",
      "--------------------------------\n",
      "val predicted: (59700,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [22 24 43 45 37 52 37 41 50 49] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 85.950000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.87      0.91       980\n",
      "         1.0       0.93      0.98      0.96      1135\n",
      "         2.0       0.81      0.82      0.82      1032\n",
      "         3.0       0.82      0.88      0.85      1010\n",
      "         4.0       0.81      0.91      0.85       982\n",
      "         5.0       0.79      0.80      0.79       892\n",
      "         6.0       0.88      0.90      0.89       958\n",
      "         7.0       0.88      0.86      0.87      1028\n",
      "         8.0       0.90      0.76      0.83       974\n",
      "         9.0       0.83      0.78      0.80      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.857349\n",
      "--------------------------------\n",
      "val predicted: (59600,) [3. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [33 25 53 54 56 67 41 47 63 61] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 88.220000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94       980\n",
      "         1.0       0.94      0.98      0.96      1135\n",
      "         2.0       0.85      0.84      0.85      1032\n",
      "         3.0       0.84      0.88      0.86      1010\n",
      "         4.0       0.84      0.94      0.89       982\n",
      "         5.0       0.80      0.84      0.82       892\n",
      "         6.0       0.91      0.90      0.91       958\n",
      "         7.0       0.92      0.88      0.90      1028\n",
      "         8.0       0.88      0.79      0.84       974\n",
      "         9.0       0.88      0.83      0.85      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.880369\n",
      "--------------------------------\n",
      "final active learning accuracies [76.61, 83.61, 86.0, 85.95, 88.22]\n",
      "saved Active-learning-experiment-11.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-11.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 12, using model = SvmModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
      "\n",
      "initial random chosen samples (25,)\n",
      "initial train set: (25, 784) (25,) unique(labels): [5 3 3 4 2 1 1 3 0 3] [0 1 2 3 4 5 6 7 9]\n",
      "val set: (59975, 784) (59975,) (25,)\n",
      "\n",
      "Train set: (25, 784) y: (25,)\n",
      "Val   set: (59975, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 46.530000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.97      0.55       980\n",
      "         1.0       0.74      0.88      0.81      1135\n",
      "         2.0       0.54      0.60      0.57      1032\n",
      "         3.0       0.27      0.61      0.38      1010\n",
      "         4.0       0.61      0.38      0.47       982\n",
      "         5.0       0.44      0.05      0.09       892\n",
      "         6.0       0.98      0.21      0.34       958\n",
      "         7.0       0.51      0.45      0.48      1028\n",
      "         8.0       0.00      0.00      0.00       974\n",
      "         9.0       0.39      0.38      0.39      1009\n",
      "\n",
      "    accuracy                           0.47     10000\n",
      "   macro avg       0.49      0.45      0.41     10000\n",
      "weighted avg       0.49      0.47      0.42     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.407602\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val predicted: (59975,) [0. 0. 0. ... 3. 4. 0.]\n",
      "probabilities: (59975, 9) \n",
      " [0 0 0 ... 0 4 0]\n",
      "trainset before (25, 784) (25,)\n",
      "trainset after (50, 784) (50,)\n",
      "updated train set: (50, 784) (50,) unique(labels): [9 6 6 7 3 3 3 6 2 5] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59950, 784) (59950,)\n",
      "\n",
      "Train set: (50, 784) y: (50,)\n",
      "Val   set: (59950, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 68.430000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.95      0.71       980\n",
      "         1.0       0.62      0.99      0.77      1135\n",
      "         2.0       0.81      0.63      0.71      1032\n",
      "         3.0       0.63      0.78      0.70      1010\n",
      "         4.0       0.74      0.71      0.72       982\n",
      "         5.0       0.59      0.37      0.45       892\n",
      "         6.0       0.91      0.72      0.81       958\n",
      "         7.0       0.74      0.82      0.78      1028\n",
      "         8.0       0.78      0.11      0.20       974\n",
      "         9.0       0.72      0.66      0.69      1009\n",
      "\n",
      "    accuracy                           0.68     10000\n",
      "   macro avg       0.71      0.68      0.65     10000\n",
      "weighted avg       0.71      0.68      0.66     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.653027\n",
      "--------------------------------\n",
      "val predicted: (59950,) [3. 0. 3. ... 5. 0. 0.]\n",
      "probabilities: (59950, 10) \n",
      " [3 0 3 ... 3 0 0]\n",
      "trainset before (50, 784) (50,)\n",
      "trainset after (75, 784) (75,)\n",
      "updated train set: (75, 784) (75,) unique(labels): [ 9  6  7  9  6  7 14  7  4  6] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59925, 784) (59925,)\n",
      "\n",
      "Train set: (75, 784) y: (75,)\n",
      "Val   set: (59925, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 73.850000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.93      0.79       980\n",
      "         1.0       0.74      0.99      0.85      1135\n",
      "         2.0       0.86      0.69      0.76      1032\n",
      "         3.0       0.63      0.76      0.69      1010\n",
      "         4.0       0.71      0.70      0.70       982\n",
      "         5.0       0.65      0.49      0.56       892\n",
      "         6.0       0.80      0.91      0.85       958\n",
      "         7.0       0.82      0.85      0.84      1028\n",
      "         8.0       0.91      0.36      0.52       974\n",
      "         9.0       0.71      0.64      0.68      1009\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.75      0.73      0.72     10000\n",
      "weighted avg       0.75      0.74      0.73     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.722992\n",
      "--------------------------------\n",
      "val predicted: (59925,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59925, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (75, 784) (75,)\n",
      "trainset after (100, 784) (100,)\n",
      "updated train set: (100, 784) (100,) unique(labels): [12  6  8 11 11 11 14  8  9 10] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 80.170000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.95      0.84       980\n",
      "         1.0       0.86      0.99      0.92      1135\n",
      "         2.0       0.90      0.67      0.77      1032\n",
      "         3.0       0.71      0.76      0.73      1010\n",
      "         4.0       0.80      0.80      0.80       982\n",
      "         5.0       0.66      0.61      0.63       892\n",
      "         6.0       0.90      0.88      0.89       958\n",
      "         7.0       0.88      0.86      0.87      1028\n",
      "         8.0       0.87      0.68      0.77       974\n",
      "         9.0       0.72      0.78      0.75      1009\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.80      0.80      0.80     10000\n",
      "weighted avg       0.81      0.80      0.80     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.796405\n",
      "--------------------------------\n",
      "val predicted: (59900,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59900, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (125, 784) (125,)\n",
      "updated train set: (125, 784) (125,) unique(labels): [12  7 12 17 12 15 17  9 13 11] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59875, 784) (59875,)\n",
      "\n",
      "Train set: (125, 784) y: (125,)\n",
      "Val   set: (59875, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.690000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.95      0.89       980\n",
      "         1.0       0.87      0.98      0.92      1135\n",
      "         2.0       0.92      0.78      0.84      1032\n",
      "         3.0       0.72      0.80      0.76      1010\n",
      "         4.0       0.86      0.79      0.82       982\n",
      "         5.0       0.69      0.67      0.68       892\n",
      "         6.0       0.90      0.89      0.89       958\n",
      "         7.0       0.92      0.82      0.87      1028\n",
      "         8.0       0.83      0.73      0.78       974\n",
      "         9.0       0.73      0.83      0.78      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.82      0.82     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.823728\n",
      "--------------------------------\n",
      "val predicted: (59875,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59875, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (125, 784) (125,)\n",
      "trainset after (150, 784) (150,)\n",
      "updated train set: (150, 784) (150,) unique(labels): [12  9 18 20 13 17 18 12 19 12] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59850, 784) (59850,)\n",
      "\n",
      "Train set: (150, 784) y: (150,)\n",
      "Val   set: (59850, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 6\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 83.410000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.94      0.89       980\n",
      "         1.0       0.88      0.98      0.93      1135\n",
      "         2.0       0.87      0.79      0.83      1032\n",
      "         3.0       0.76      0.81      0.78      1010\n",
      "         4.0       0.88      0.74      0.80       982\n",
      "         5.0       0.72      0.76      0.74       892\n",
      "         6.0       0.90      0.88      0.89       958\n",
      "         7.0       0.90      0.85      0.87      1028\n",
      "         8.0       0.86      0.73      0.79       974\n",
      "         9.0       0.75      0.84      0.79      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.84      0.83      0.83     10000\n",
      "weighted avg       0.84      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.831225\n",
      "--------------------------------\n",
      "val predicted: (59850,) [5. 0. 4. ... 5. 5. 8.]\n",
      "probabilities: (59850, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (150, 784) (150,)\n",
      "trainset after (175, 784) (175,)\n",
      "updated train set: (175, 784) (175,) unique(labels): [13 10 20 21 17 20 20 16 23 15] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59825, 784) (59825,)\n",
      "\n",
      "Train set: (175, 784) y: (175,)\n",
      "Val   set: (59825, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 7\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 83.470000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.91      0.89       980\n",
      "         1.0       0.90      0.98      0.94      1135\n",
      "         2.0       0.88      0.79      0.83      1032\n",
      "         3.0       0.79      0.76      0.77      1010\n",
      "         4.0       0.84      0.76      0.80       982\n",
      "         5.0       0.70      0.77      0.73       892\n",
      "         6.0       0.91      0.87      0.89       958\n",
      "         7.0       0.88      0.90      0.89      1028\n",
      "         8.0       0.83      0.75      0.79       974\n",
      "         9.0       0.76      0.84      0.80      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.84      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.831822\n",
      "--------------------------------\n",
      "val predicted: (59825,) [3. 0. 4. ... 5. 5. 8.]\n",
      "probabilities: (59825, 10) \n",
      " [3 0 4 ... 5 5 8]\n",
      "trainset before (175, 784) (175,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [13 12 22 25 22 26 21 18 26 15] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 8\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 84.430000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.89      0.89       980\n",
      "         1.0       0.91      0.97      0.94      1135\n",
      "         2.0       0.91      0.79      0.84      1032\n",
      "         3.0       0.81      0.83      0.82      1010\n",
      "         4.0       0.85      0.79      0.82       982\n",
      "         5.0       0.69      0.80      0.74       892\n",
      "         6.0       0.91      0.89      0.90       958\n",
      "         7.0       0.88      0.89      0.89      1028\n",
      "         8.0       0.85      0.74      0.79       974\n",
      "         9.0       0.78      0.82      0.80      1009\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.85      0.84      0.84     10000\n",
      "weighted avg       0.85      0.84      0.84     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.842216\n",
      "--------------------------------\n",
      "val predicted: (59800,) [3. 0. 4. ... 5. 5. 8.]\n",
      "probabilities: (59800, 10) \n",
      " [3 0 4 ... 5 5 8]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (225, 784) (225,)\n",
      "updated train set: (225, 784) (225,) unique(labels): [16 14 24 26 24 29 24 18 34 16] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59775, 784) (59775,)\n",
      "\n",
      "Train set: (225, 784) y: (225,)\n",
      "Val   set: (59775, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 9\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 84.860000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.94      0.91       980\n",
      "         1.0       0.91      0.98      0.94      1135\n",
      "         2.0       0.91      0.79      0.84      1032\n",
      "         3.0       0.84      0.83      0.84      1010\n",
      "         4.0       0.78      0.81      0.80       982\n",
      "         5.0       0.73      0.79      0.76       892\n",
      "         6.0       0.90      0.88      0.89       958\n",
      "         7.0       0.88      0.87      0.88      1028\n",
      "         8.0       0.85      0.81      0.83       974\n",
      "         9.0       0.79      0.75      0.77      1009\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.846087\n",
      "--------------------------------\n",
      "val predicted: (59775,) [3. 0. 4. ... 5. 5. 8.]\n",
      "probabilities: (59775, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (225, 784) (225,)\n",
      "trainset after (250, 784) (250,)\n",
      "updated train set: (250, 784) (250,) unique(labels): [17 17 26 29 29 30 27 18 36 21] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 10\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 85.690000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.94      0.91       980\n",
      "         1.0       0.88      0.99      0.93      1135\n",
      "         2.0       0.92      0.78      0.85      1032\n",
      "         3.0       0.84      0.81      0.83      1010\n",
      "         4.0       0.79      0.91      0.84       982\n",
      "         5.0       0.75      0.78      0.76       892\n",
      "         6.0       0.92      0.89      0.91       958\n",
      "         7.0       0.90      0.87      0.88      1028\n",
      "         8.0       0.85      0.80      0.82       974\n",
      "         9.0       0.84      0.78      0.81      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.85      0.85     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.854301\n",
      "--------------------------------\n",
      "val predicted: (59750,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (275, 784) (275,)\n",
      "updated train set: (275, 784) (275,) unique(labels): [18 18 33 30 31 32 31 21 36 25] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59725, 784) (59725,)\n",
      "\n",
      "Train set: (275, 784) y: (275,)\n",
      "Val   set: (59725, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 11\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 85.590000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.93      0.90       980\n",
      "         1.0       0.90      0.99      0.94      1135\n",
      "         2.0       0.89      0.83      0.86      1032\n",
      "         3.0       0.85      0.82      0.83      1010\n",
      "         4.0       0.78      0.91      0.84       982\n",
      "         5.0       0.74      0.78      0.76       892\n",
      "         6.0       0.93      0.91      0.92       958\n",
      "         7.0       0.90      0.86      0.88      1028\n",
      "         8.0       0.86      0.77      0.81       974\n",
      "         9.0       0.83      0.75      0.79      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.85      0.85     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.853337\n",
      "--------------------------------\n",
      "val predicted: (59725,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59725, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (275, 784) (275,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [19 20 35 33 34 36 32 24 40 27] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 12\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.240000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.96      0.92       980\n",
      "         1.0       0.91      0.98      0.94      1135\n",
      "         2.0       0.89      0.84      0.86      1032\n",
      "         3.0       0.85      0.83      0.84      1010\n",
      "         4.0       0.78      0.90      0.84       982\n",
      "         5.0       0.78      0.77      0.77       892\n",
      "         6.0       0.92      0.92      0.92       958\n",
      "         7.0       0.92      0.86      0.89      1028\n",
      "         8.0       0.86      0.76      0.80       974\n",
      "         9.0       0.83      0.78      0.80      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.859582\n",
      "--------------------------------\n",
      "val predicted: (59700,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (325, 784) (325,)\n",
      "updated train set: (325, 784) (325,) unique(labels): [19 21 37 35 36 40 34 25 45 33] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59675, 784) (59675,)\n",
      "\n",
      "Train set: (325, 784) y: (325,)\n",
      "Val   set: (59675, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 13\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.770000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.96      0.92       980\n",
      "         1.0       0.92      0.98      0.95      1135\n",
      "         2.0       0.89      0.83      0.86      1032\n",
      "         3.0       0.84      0.84      0.84      1010\n",
      "         4.0       0.78      0.92      0.85       982\n",
      "         5.0       0.81      0.78      0.79       892\n",
      "         6.0       0.91      0.93      0.92       958\n",
      "         7.0       0.91      0.87      0.89      1028\n",
      "         8.0       0.87      0.79      0.83       974\n",
      "         9.0       0.86      0.75      0.80      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.86     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.864950\n",
      "--------------------------------\n",
      "val predicted: (59675,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59675, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (325, 784) (325,)\n",
      "trainset after (350, 784) (350,)\n",
      "updated train set: (350, 784) (350,) unique(labels): [19 21 40 39 36 45 36 26 50 38] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59650, 784) (59650,)\n",
      "\n",
      "Train set: (350, 784) y: (350,)\n",
      "Val   set: (59650, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 14\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.540000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91       980\n",
      "         1.0       0.92      0.98      0.95      1135\n",
      "         2.0       0.89      0.84      0.87      1032\n",
      "         3.0       0.86      0.84      0.85      1010\n",
      "         4.0       0.83      0.91      0.87       982\n",
      "         5.0       0.79      0.80      0.79       892\n",
      "         6.0       0.90      0.92      0.91       958\n",
      "         7.0       0.93      0.87      0.90      1028\n",
      "         8.0       0.88      0.79      0.84       974\n",
      "         9.0       0.85      0.83      0.84      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.873112\n",
      "--------------------------------\n",
      "val predicted: (59650,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59650, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (350, 784) (350,)\n",
      "trainset after (375, 784) (375,)\n",
      "updated train set: (375, 784) (375,) unique(labels): [20 23 42 40 38 50 38 27 53 44] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59625, 784) (59625,)\n",
      "\n",
      "Train set: (375, 784) y: (375,)\n",
      "Val   set: (59625, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 15\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.760000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.93      0.91       980\n",
      "         1.0       0.93      0.98      0.95      1135\n",
      "         2.0       0.89      0.85      0.87      1032\n",
      "         3.0       0.87      0.84      0.85      1010\n",
      "         4.0       0.84      0.91      0.88       982\n",
      "         5.0       0.78      0.81      0.80       892\n",
      "         6.0       0.90      0.91      0.90       958\n",
      "         7.0       0.94      0.87      0.91      1028\n",
      "         8.0       0.89      0.80      0.84       974\n",
      "         9.0       0.84      0.85      0.85      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.875392\n",
      "--------------------------------\n",
      "val predicted: (59625,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59625, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (375, 784) (375,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [20 25 42 44 40 56 40 28 59 46] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 16\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.680000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.93      0.91       980\n",
      "         1.0       0.92      0.98      0.95      1135\n",
      "         2.0       0.90      0.84      0.87      1032\n",
      "         3.0       0.86      0.82      0.84      1010\n",
      "         4.0       0.86      0.91      0.88       982\n",
      "         5.0       0.77      0.81      0.79       892\n",
      "         6.0       0.90      0.91      0.91       958\n",
      "         7.0       0.94      0.88      0.91      1028\n",
      "         8.0       0.87      0.81      0.84       974\n",
      "         9.0       0.84      0.86      0.85      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.874657\n",
      "--------------------------------\n",
      "val predicted: (59600,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (425, 784) (425,)\n",
      "updated train set: (425, 784) (425,) unique(labels): [24 27 45 45 43 57 41 31 64 48] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59575, 784) (59575,)\n",
      "\n",
      "Train set: (425, 784) y: (425,)\n",
      "Val   set: (59575, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 17\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.980000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.96      0.92       980\n",
      "         1.0       0.93      0.99      0.96      1135\n",
      "         2.0       0.90      0.85      0.88      1032\n",
      "         3.0       0.86      0.83      0.84      1010\n",
      "         4.0       0.84      0.88      0.86       982\n",
      "         5.0       0.81      0.81      0.81       892\n",
      "         6.0       0.90      0.91      0.91       958\n",
      "         7.0       0.93      0.89      0.91      1028\n",
      "         8.0       0.89      0.82      0.85       974\n",
      "         9.0       0.84      0.85      0.84      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.877640\n",
      "--------------------------------\n",
      "val predicted: (59575,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59575, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (425, 784) (425,)\n",
      "trainset after (450, 784) (450,)\n",
      "updated train set: (450, 784) (450,) unique(labels): [24 28 50 49 46 61 43 32 66 51] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59550, 784) (59550,)\n",
      "\n",
      "Train set: (450, 784) y: (450,)\n",
      "Val   set: (59550, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 18\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 88.330000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.96      0.93       980\n",
      "         1.0       0.92      0.98      0.95      1135\n",
      "         2.0       0.91      0.85      0.88      1032\n",
      "         3.0       0.86      0.85      0.85      1010\n",
      "         4.0       0.84      0.90      0.87       982\n",
      "         5.0       0.81      0.81      0.81       892\n",
      "         6.0       0.91      0.91      0.91       958\n",
      "         7.0       0.93      0.89      0.91      1028\n",
      "         8.0       0.87      0.81      0.84       974\n",
      "         9.0       0.87      0.84      0.86      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.881134\n",
      "--------------------------------\n",
      "val predicted: (59550,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59550, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (450, 784) (450,)\n",
      "trainset after (475, 784) (475,)\n",
      "updated train set: (475, 784) (475,) unique(labels): [25 30 52 54 49 64 44 34 69 54] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59525, 784) (59525,)\n",
      "\n",
      "Train set: (475, 784) y: (475,)\n",
      "Val   set: (59525, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 19\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 88.560000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.95      0.93       980\n",
      "         1.0       0.93      0.98      0.95      1135\n",
      "         2.0       0.90      0.85      0.87      1032\n",
      "         3.0       0.85      0.88      0.86      1010\n",
      "         4.0       0.85      0.90      0.87       982\n",
      "         5.0       0.82      0.82      0.82       892\n",
      "         6.0       0.91      0.92      0.91       958\n",
      "         7.0       0.93      0.90      0.91      1028\n",
      "         8.0       0.89      0.81      0.85       974\n",
      "         9.0       0.86      0.85      0.85      1009\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.883620\n",
      "--------------------------------\n",
      "val predicted: (59525,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59525, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (475, 784) (475,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [27 33 56 54 51 68 44 35 74 58] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 20\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 88.290000 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.91       980\n",
      "         1.0       0.92      0.98      0.95      1135\n",
      "         2.0       0.91      0.84      0.87      1032\n",
      "         3.0       0.85      0.87      0.86      1010\n",
      "         4.0       0.86      0.90      0.88       982\n",
      "         5.0       0.81      0.81      0.81       892\n",
      "         6.0       0.91      0.91      0.91       958\n",
      "         7.0       0.92      0.89      0.91      1028\n",
      "         8.0       0.89      0.82      0.86       974\n",
      "         9.0       0.86      0.84      0.85      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.880873\n",
      "--------------------------------\n",
      "final active learning accuracies [46.53, 68.43, 73.85000000000001, 80.17, 82.69, 83.41, 83.47, 84.43, 84.86, 85.69, 85.59, 86.24000000000001, 86.77, 87.53999999999999, 87.76, 87.68, 87.98, 88.33, 88.56, 88.29]\n",
      "saved Active-learning-experiment-12.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-11.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 13, using model = RfModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
      "\n",
      "initial random chosen samples (250,)\n",
      "initial train set: (250, 784) (250,) unique(labels): [20 22 28 19 24 31 26 23 31 26] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,) (250,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 83.720000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.90      0.91       980\n",
      "         1.0       0.84      0.97      0.90      1135\n",
      "         2.0       0.80      0.72      0.76      1032\n",
      "         3.0       0.94      0.68      0.79      1010\n",
      "         4.0       0.90      0.80      0.84       982\n",
      "         5.0       0.71      0.85      0.77       892\n",
      "         6.0       0.84      0.87      0.86       958\n",
      "         7.0       0.90      0.85      0.87      1028\n",
      "         8.0       0.81      0.85      0.83       974\n",
      "         9.0       0.76      0.88      0.81      1009\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.835427\n",
      "--------------------------------\n",
      "val predicted: (59750,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [38 47 52 40 51 55 62 48 58 49] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.480000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.93      0.94       980\n",
      "         1.0       0.91      0.96      0.93      1135\n",
      "         2.0       0.88      0.83      0.86      1032\n",
      "         3.0       0.91      0.76      0.83      1010\n",
      "         4.0       0.85      0.87      0.86       982\n",
      "         5.0       0.83      0.87      0.85       892\n",
      "         6.0       0.84      0.94      0.88       958\n",
      "         7.0       0.93      0.86      0.89      1028\n",
      "         8.0       0.84      0.86      0.85       974\n",
      "         9.0       0.82      0.85      0.84      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.88      0.87      0.87     10000\n",
      "weighted avg       0.88      0.87      0.87     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.873448\n",
      "--------------------------------\n",
      "final active learning accuracies [83.72, 87.48]\n",
      "saved Active-learning-experiment-13.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-11.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 14, using model = RfModel, selection_function = RandomSelection, k = 100, iteration = 0.\n",
      "\n",
      "initial random chosen samples (100,)\n",
      "initial train set: (100, 784) (100,) unique(labels): [ 7 14 12 11 15  1 10  9 12  9] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,) (100,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 69.060000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.89      0.88       980\n",
      "         1.0       0.75      0.98      0.85      1135\n",
      "         2.0       0.71      0.77      0.74      1032\n",
      "         3.0       0.64      0.67      0.66      1010\n",
      "         4.0       0.43      0.86      0.58       982\n",
      "         5.0       0.86      0.02      0.04       892\n",
      "         6.0       0.84      0.86      0.85       958\n",
      "         7.0       0.86      0.80      0.83      1028\n",
      "         8.0       0.67      0.70      0.68       974\n",
      "         9.0       0.63      0.26      0.37      1009\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.73      0.68      0.65     10000\n",
      "weighted avg       0.72      0.69      0.66     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.646758\n",
      "--------------------------------\n",
      "val predicted: (59900,) [3. 0. 4. ... 3. 0. 8.]\n",
      "probabilities: (59900, 10) \n",
      " [3 0 4 ... 3 0 8]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [17 21 24 30 23  9 17 25 20 14] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 76.870000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.95      0.90       980\n",
      "         1.0       0.85      0.97      0.90      1135\n",
      "         2.0       0.86      0.82      0.84      1032\n",
      "         3.0       0.53      0.88      0.66      1010\n",
      "         4.0       0.64      0.91      0.75       982\n",
      "         5.0       0.97      0.32      0.48       892\n",
      "         6.0       0.92      0.86      0.89       958\n",
      "         7.0       0.75      0.93      0.83      1028\n",
      "         8.0       0.87      0.53      0.66       974\n",
      "         9.0       0.92      0.45      0.61      1009\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.82      0.76      0.75     10000\n",
      "weighted avg       0.81      0.77      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.751099\n",
      "--------------------------------\n",
      "val predicted: (59800,) [5. 0. 4. ... 5. 0. 2.]\n",
      "probabilities: (59800, 10) \n",
      " [5 0 4 ... 5 0 2]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [32 34 33 39 33 14 26 35 27 27] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.900000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.97      0.92       980\n",
      "         1.0       0.88      0.98      0.93      1135\n",
      "         2.0       0.89      0.81      0.85      1032\n",
      "         3.0       0.63      0.90      0.74      1010\n",
      "         4.0       0.76      0.93      0.84       982\n",
      "         5.0       0.95      0.41      0.57       892\n",
      "         6.0       0.92      0.88      0.90       958\n",
      "         7.0       0.85      0.90      0.87      1028\n",
      "         8.0       0.87      0.67      0.76       974\n",
      "         9.0       0.85      0.77      0.81      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.85      0.82      0.82     10000\n",
      "weighted avg       0.85      0.83      0.82     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.818408\n",
      "--------------------------------\n",
      "val predicted: (59700,) [3. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [3 0 4 ... 5 0 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [44 50 42 50 40 22 36 40 34 42] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 85.240000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.98      0.93       980\n",
      "         1.0       0.86      0.98      0.92      1135\n",
      "         2.0       0.90      0.81      0.85      1032\n",
      "         3.0       0.68      0.90      0.77      1010\n",
      "         4.0       0.87      0.86      0.86       982\n",
      "         5.0       0.97      0.52      0.68       892\n",
      "         6.0       0.93      0.91      0.92       958\n",
      "         7.0       0.91      0.87      0.89      1028\n",
      "         8.0       0.87      0.74      0.80       974\n",
      "         9.0       0.78      0.90      0.84      1009\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.87      0.85      0.85     10000\n",
      "weighted avg       0.87      0.85      0.85     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.846743\n",
      "--------------------------------\n",
      "val predicted: (59600,) [5. 0. 4. ... 3. 5. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 4 ... 3 5 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [51 65 47 62 50 27 44 55 44 55] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.460000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.93       980\n",
      "         1.0       0.88      0.98      0.93      1135\n",
      "         2.0       0.93      0.79      0.85      1032\n",
      "         3.0       0.69      0.92      0.79      1010\n",
      "         4.0       0.88      0.89      0.88       982\n",
      "         5.0       0.97      0.54      0.69       892\n",
      "         6.0       0.93      0.91      0.92       958\n",
      "         7.0       0.91      0.89      0.90      1028\n",
      "         8.0       0.88      0.80      0.83       974\n",
      "         9.0       0.82      0.91      0.86      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.88      0.86      0.86     10000\n",
      "weighted avg       0.88      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.859391\n",
      "--------------------------------\n",
      "final active learning accuracies [69.06, 76.87, 82.89999999999999, 85.24000000000001, 86.46000000000001]\n",
      "saved Active-learning-experiment-14.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 15, using model = RfModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
      "\n",
      "initial random chosen samples (25,)\n",
      "initial train set: (25, 784) (25,) unique(labels): [4 4 2 3 4 0 4 1 1 2] [0 1 2 3 4 6 7 8 9]\n",
      "val set: (59975, 784) (59975,) (25,)\n",
      "\n",
      "Train set: (25, 784) y: (25,)\n",
      "Val   set: (59975, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 45.280000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.82      0.70       980\n",
      "         1.0       0.37      1.00      0.54      1135\n",
      "         2.0       0.92      0.13      0.22      1032\n",
      "         3.0       0.60      0.65      0.62      1010\n",
      "         4.0       0.37      0.73      0.49       982\n",
      "         5.0       0.00      0.00      0.00       892\n",
      "         6.0       0.47      0.60      0.53       958\n",
      "         7.0       0.63      0.14      0.23      1028\n",
      "         8.0       0.59      0.03      0.06       974\n",
      "         9.0       0.36      0.34      0.35      1009\n",
      "\n",
      "    accuracy                           0.45     10000\n",
      "   macro avg       0.49      0.44      0.37     10000\n",
      "weighted avg       0.50      0.45      0.38     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.373719\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val predicted: (59975,) [1. 0. 4. ... 1. 4. 0.]\n",
      "probabilities: (59975, 9) \n",
      " [1 0 4 ... 1 4 0]\n",
      "trainset before (25, 784) (25,)\n",
      "trainset after (50, 784) (50,)\n",
      "updated train set: (50, 784) (50,) unique(labels): [8 7 4 9 5 2 6 4 3 2] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59950, 784) (59950,)\n",
      "\n",
      "Train set: (50, 784) y: (50,)\n",
      "Val   set: (59950, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 56.250000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.90      0.75       980\n",
      "         1.0       0.55      0.99      0.71      1135\n",
      "         2.0       0.93      0.38      0.54      1032\n",
      "         3.0       0.43      0.87      0.57      1010\n",
      "         4.0       0.47      0.64      0.54       982\n",
      "         5.0       0.92      0.03      0.05       892\n",
      "         6.0       0.63      0.56      0.59       958\n",
      "         7.0       0.62      0.67      0.64      1028\n",
      "         8.0       0.76      0.23      0.36       974\n",
      "         9.0       0.50      0.25      0.33      1009\n",
      "\n",
      "    accuracy                           0.56     10000\n",
      "   macro avg       0.64      0.55      0.51     10000\n",
      "weighted avg       0.64      0.56      0.52     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.508697\n",
      "--------------------------------\n",
      "val predicted: (59950,) [3. 0. 3. ... 3. 0. 4.]\n",
      "probabilities: (59950, 10) \n",
      " [3 0 3 ... 3 0 4]\n",
      "trainset before (50, 784) (50,)\n",
      "trainset after (75, 784) (75,)\n",
      "updated train set: (75, 784) (75,) unique(labels): [12 10  4 11  9  4 10  5  5  5] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59925, 784) (59925,)\n",
      "\n",
      "Train set: (75, 784) y: (75,)\n",
      "Val   set: (59925, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 63.970000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.89      0.78       980\n",
      "         1.0       0.62      0.98      0.76      1135\n",
      "         2.0       0.98      0.33      0.50      1032\n",
      "         3.0       0.53      0.86      0.66      1010\n",
      "         4.0       0.50      0.86      0.63       982\n",
      "         5.0       0.93      0.21      0.34       892\n",
      "         6.0       0.55      0.78      0.65       958\n",
      "         7.0       0.88      0.76      0.82      1028\n",
      "         8.0       0.73      0.25      0.38       974\n",
      "         9.0       0.77      0.40      0.52      1009\n",
      "\n",
      "    accuracy                           0.64     10000\n",
      "   macro avg       0.72      0.63      0.60     10000\n",
      "weighted avg       0.72      0.64      0.61     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.603115\n",
      "--------------------------------\n",
      "val predicted: (59925,) [3. 0. 4. ... 5. 0. 0.]\n",
      "probabilities: (59925, 10) \n",
      " [3 0 4 ... 5 0 0]\n",
      "trainset before (75, 784) (75,)\n",
      "trainset after (100, 784) (100,)\n",
      "updated train set: (100, 784) (100,) unique(labels): [13 14  8 13 11  6 13  6  8  8] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 68.540000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.92      0.84       980\n",
      "         1.0       0.64      0.99      0.78      1135\n",
      "         2.0       0.92      0.54      0.68      1032\n",
      "         3.0       0.57      0.86      0.69      1010\n",
      "         4.0       0.53      0.86      0.66       982\n",
      "         5.0       0.96      0.22      0.35       892\n",
      "         6.0       0.67      0.76      0.71       958\n",
      "         7.0       0.91      0.69      0.78      1028\n",
      "         8.0       0.75      0.41      0.53       974\n",
      "         9.0       0.70      0.52      0.60      1009\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.74      0.68      0.66     10000\n",
      "weighted avg       0.74      0.69      0.67     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.662025\n",
      "--------------------------------\n",
      "val predicted: (59900,) [3. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59900, 10) \n",
      " [3 0 4 ... 5 0 8]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (125, 784) (125,)\n",
      "updated train set: (125, 784) (125,) unique(labels): [18 16 10 16 12  8 16  7  9 13] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59875, 784) (59875,)\n",
      "\n",
      "Train set: (125, 784) y: (125,)\n",
      "Val   set: (59875, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 72.980000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.95      0.83       980\n",
      "         1.0       0.67      0.99      0.80      1135\n",
      "         2.0       0.90      0.52      0.66      1032\n",
      "         3.0       0.61      0.89      0.72      1010\n",
      "         4.0       0.73      0.77      0.75       982\n",
      "         5.0       0.95      0.33      0.49       892\n",
      "         6.0       0.72      0.84      0.78       958\n",
      "         7.0       0.95      0.70      0.80      1028\n",
      "         8.0       0.77      0.42      0.54       974\n",
      "         9.0       0.67      0.84      0.75      1009\n",
      "\n",
      "    accuracy                           0.73     10000\n",
      "   macro avg       0.77      0.72      0.71     10000\n",
      "weighted avg       0.77      0.73      0.71     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.710789\n",
      "--------------------------------\n",
      "val predicted: (59875,) [3. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59875, 10) \n",
      " [3 0 4 ... 5 0 8]\n",
      "trainset before (125, 784) (125,)\n",
      "trainset after (150, 784) (150,)\n",
      "updated train set: (150, 784) (150,) unique(labels): [19 20 13 17 16 10 19 10 11 15] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59850, 784) (59850,)\n",
      "\n",
      "Train set: (150, 784) y: (150,)\n",
      "Val   set: (59850, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 6\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 74.770000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.94      0.87       980\n",
      "         1.0       0.67      0.99      0.80      1135\n",
      "         2.0       0.87      0.60      0.71      1032\n",
      "         3.0       0.61      0.87      0.72      1010\n",
      "         4.0       0.75      0.81      0.78       982\n",
      "         5.0       0.95      0.36      0.53       892\n",
      "         6.0       0.73      0.82      0.77       958\n",
      "         7.0       0.93      0.73      0.82      1028\n",
      "         8.0       0.83      0.43      0.57       974\n",
      "         9.0       0.69      0.86      0.76      1009\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.78      0.74      0.73     10000\n",
      "weighted avg       0.78      0.75      0.74     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.732125\n",
      "--------------------------------\n",
      "val predicted: (59850,) [3. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59850, 10) \n",
      " [3 0 4 ... 5 0 8]\n",
      "trainset before (150, 784) (150,)\n",
      "trainset after (175, 784) (175,)\n",
      "updated train set: (175, 784) (175,) unique(labels): [22 20 16 20 20 11 23 10 13 20] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59825, 784) (59825,)\n",
      "\n",
      "Train set: (175, 784) y: (175,)\n",
      "Val   set: (59825, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 7\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 75.800000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.95      0.89       980\n",
      "         1.0       0.73      0.99      0.84      1135\n",
      "         2.0       0.85      0.66      0.75      1032\n",
      "         3.0       0.61      0.87      0.72      1010\n",
      "         4.0       0.75      0.82      0.78       982\n",
      "         5.0       0.98      0.33      0.49       892\n",
      "         6.0       0.74      0.85      0.79       958\n",
      "         7.0       0.97      0.71      0.82      1028\n",
      "         8.0       0.80      0.46      0.58       974\n",
      "         9.0       0.66      0.88      0.76      1009\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.79      0.75      0.74     10000\n",
      "weighted avg       0.79      0.76      0.75     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.741054\n",
      "--------------------------------\n",
      "val predicted: (59825,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59825, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (175, 784) (175,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [26 23 20 24 22 13 23 12 13 24] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 8\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 77.250000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.97      0.88       980\n",
      "         1.0       0.78      0.98      0.87      1135\n",
      "         2.0       0.83      0.73      0.78      1032\n",
      "         3.0       0.62      0.89      0.73      1010\n",
      "         4.0       0.76      0.82      0.79       982\n",
      "         5.0       0.94      0.37      0.53       892\n",
      "         6.0       0.81      0.82      0.82       958\n",
      "         7.0       0.96      0.73      0.83      1028\n",
      "         8.0       0.86      0.44      0.58       974\n",
      "         9.0       0.66      0.89      0.76      1009\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.80      0.77      0.76     10000\n",
      "weighted avg       0.80      0.77      0.76     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.756561\n",
      "--------------------------------\n",
      "val predicted: (59800,) [3. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59800, 10) \n",
      " [3 0 4 ... 5 0 8]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (225, 784) (225,)\n",
      "updated train set: (225, 784) (225,) unique(labels): [28 27 22 26 23 17 24 16 17 25] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59775, 784) (59775,)\n",
      "\n",
      "Train set: (225, 784) y: (225,)\n",
      "Val   set: (59775, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 9\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 78.820000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.97      0.89       980\n",
      "         1.0       0.80      0.98      0.88      1135\n",
      "         2.0       0.86      0.77      0.81      1032\n",
      "         3.0       0.62      0.90      0.73      1010\n",
      "         4.0       0.78      0.80      0.79       982\n",
      "         5.0       0.93      0.47      0.63       892\n",
      "         6.0       0.84      0.83      0.83       958\n",
      "         7.0       0.94      0.77      0.85      1028\n",
      "         8.0       0.85      0.45      0.59       974\n",
      "         9.0       0.68      0.88      0.76      1009\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.81      0.78      0.78     10000\n",
      "weighted avg       0.81      0.79      0.78     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.777031\n",
      "--------------------------------\n",
      "val predicted: (59775,) [3. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59775, 10) \n",
      " [3 0 4 ... 5 0 8]\n",
      "trainset before (225, 784) (225,)\n",
      "trainset after (250, 784) (250,)\n",
      "updated train set: (250, 784) (250,) unique(labels): [28 29 25 30 25 22 26 20 19 26] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 10\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 80.500000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.97      0.92       980\n",
      "         1.0       0.80      0.98      0.88      1135\n",
      "         2.0       0.85      0.78      0.81      1032\n",
      "         3.0       0.63      0.90      0.74      1010\n",
      "         4.0       0.78      0.83      0.80       982\n",
      "         5.0       0.92      0.55      0.69       892\n",
      "         6.0       0.86      0.85      0.86       958\n",
      "         7.0       0.94      0.81      0.87      1028\n",
      "         8.0       0.86      0.48      0.61       974\n",
      "         9.0       0.73      0.85      0.78      1009\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.82      0.80      0.80     10000\n",
      "weighted avg       0.82      0.81      0.80     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.796689\n",
      "--------------------------------\n",
      "val predicted: (59750,) [3. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59750, 10) \n",
      " [3 0 4 ... 5 0 8]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (275, 784) (275,)\n",
      "updated train set: (275, 784) (275,) unique(labels): [31 31 28 33 26 22 29 23 21 31] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59725, 784) (59725,)\n",
      "\n",
      "Train set: (275, 784) y: (275,)\n",
      "Val   set: (59725, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 11\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 80.800000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.97      0.92       980\n",
      "         1.0       0.82      0.98      0.89      1135\n",
      "         2.0       0.83      0.80      0.81      1032\n",
      "         3.0       0.63      0.87      0.73      1010\n",
      "         4.0       0.83      0.78      0.80       982\n",
      "         5.0       0.92      0.52      0.66       892\n",
      "         6.0       0.85      0.87      0.86       958\n",
      "         7.0       0.93      0.84      0.88      1028\n",
      "         8.0       0.89      0.49      0.63       974\n",
      "         9.0       0.71      0.90      0.79      1009\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.83      0.80      0.80     10000\n",
      "weighted avg       0.83      0.81      0.80     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.799094\n",
      "--------------------------------\n",
      "val predicted: (59725,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59725, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (275, 784) (275,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [33 35 32 34 28 24 33 26 22 33] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 12\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 81.540000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.97      0.93       980\n",
      "         1.0       0.81      0.98      0.89      1135\n",
      "         2.0       0.84      0.80      0.82      1032\n",
      "         3.0       0.64      0.86      0.73      1010\n",
      "         4.0       0.86      0.81      0.83       982\n",
      "         5.0       0.89      0.56      0.69       892\n",
      "         6.0       0.84      0.90      0.87       958\n",
      "         7.0       0.93      0.84      0.89      1028\n",
      "         8.0       0.88      0.48      0.62       974\n",
      "         9.0       0.74      0.90      0.81      1009\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.83      0.81      0.81     10000\n",
      "weighted avg       0.83      0.82      0.81     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.806744\n",
      "--------------------------------\n",
      "val predicted: (59700,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59700, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (325, 784) (325,)\n",
      "updated train set: (325, 784) (325,) unique(labels): [35 41 36 35 29 25 34 28 24 38] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59675, 784) (59675,)\n",
      "\n",
      "Train set: (325, 784) y: (325,)\n",
      "Val   set: (59675, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 13\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 81.420000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.97      0.92       980\n",
      "         1.0       0.82      0.98      0.89      1135\n",
      "         2.0       0.83      0.80      0.82      1032\n",
      "         3.0       0.64      0.87      0.73      1010\n",
      "         4.0       0.83      0.81      0.82       982\n",
      "         5.0       0.91      0.54      0.67       892\n",
      "         6.0       0.85      0.89      0.87       958\n",
      "         7.0       0.94      0.83      0.88      1028\n",
      "         8.0       0.90      0.50      0.65       974\n",
      "         9.0       0.73      0.89      0.81      1009\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.83      0.81      0.81     10000\n",
      "weighted avg       0.83      0.81      0.81     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.805668\n",
      "--------------------------------\n",
      "val predicted: (59675,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59675, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (325, 784) (325,)\n",
      "trainset after (350, 784) (350,)\n",
      "updated train set: (350, 784) (350,) unique(labels): [38 46 38 38 32 26 35 31 24 42] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59650, 784) (59650,)\n",
      "\n",
      "Train set: (350, 784) y: (350,)\n",
      "Val   set: (59650, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 14\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.210000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.93       980\n",
      "         1.0       0.84      0.98      0.90      1135\n",
      "         2.0       0.83      0.83      0.83      1032\n",
      "         3.0       0.65      0.87      0.75      1010\n",
      "         4.0       0.86      0.80      0.83       982\n",
      "         5.0       0.92      0.55      0.69       892\n",
      "         6.0       0.86      0.89      0.87       958\n",
      "         7.0       0.95      0.82      0.88      1028\n",
      "         8.0       0.93      0.51      0.66       974\n",
      "         9.0       0.70      0.93      0.79      1009\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.84      0.82      0.81     10000\n",
      "weighted avg       0.84      0.82      0.82     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.814382\n",
      "--------------------------------\n",
      "val predicted: (59650,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59650, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (350, 784) (350,)\n",
      "trainset after (375, 784) (375,)\n",
      "updated train set: (375, 784) (375,) unique(labels): [41 48 40 42 35 27 35 35 29 43] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59625, 784) (59625,)\n",
      "\n",
      "Train set: (375, 784) y: (375,)\n",
      "Val   set: (59625, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 15\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 83.490000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94       980\n",
      "         1.0       0.85      0.98      0.91      1135\n",
      "         2.0       0.85      0.83      0.84      1032\n",
      "         3.0       0.67      0.89      0.76      1010\n",
      "         4.0       0.84      0.83      0.84       982\n",
      "         5.0       0.95      0.56      0.70       892\n",
      "         6.0       0.90      0.89      0.89       958\n",
      "         7.0       0.92      0.86      0.89      1028\n",
      "         8.0       0.90      0.57      0.70       974\n",
      "         9.0       0.73      0.91      0.81      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.85      0.83      0.83     10000\n",
      "weighted avg       0.85      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.828284\n",
      "--------------------------------\n",
      "val predicted: (59625,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59625, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (375, 784) (375,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [45 50 44 45 35 33 37 37 30 44] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 16\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 84.370000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.98      0.93       980\n",
      "         1.0       0.87      0.98      0.92      1135\n",
      "         2.0       0.82      0.87      0.84      1032\n",
      "         3.0       0.74      0.88      0.80      1010\n",
      "         4.0       0.86      0.81      0.84       982\n",
      "         5.0       0.92      0.67      0.78       892\n",
      "         6.0       0.89      0.88      0.88       958\n",
      "         7.0       0.95      0.84      0.89      1028\n",
      "         8.0       0.91      0.56      0.69       974\n",
      "         9.0       0.72      0.92      0.81      1009\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.86      0.84      0.84     10000\n",
      "weighted avg       0.85      0.84      0.84     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.838283\n",
      "--------------------------------\n",
      "val predicted: (59600,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (425, 784) (425,)\n",
      "updated train set: (425, 784) (425,) unique(labels): [48 51 47 47 39 37 37 40 31 48] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59575, 784) (59575,)\n",
      "\n",
      "Train set: (425, 784) y: (425,)\n",
      "Val   set: (59575, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 17\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 84.540000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.98      0.93       980\n",
      "         1.0       0.85      0.97      0.91      1135\n",
      "         2.0       0.84      0.85      0.85      1032\n",
      "         3.0       0.73      0.89      0.80      1010\n",
      "         4.0       0.86      0.82      0.84       982\n",
      "         5.0       0.89      0.71      0.79       892\n",
      "         6.0       0.91      0.88      0.90       958\n",
      "         7.0       0.95      0.85      0.90      1028\n",
      "         8.0       0.91      0.55      0.68       974\n",
      "         9.0       0.73      0.92      0.81      1009\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.86      0.84      0.84     10000\n",
      "weighted avg       0.86      0.85      0.84     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.840399\n",
      "--------------------------------\n",
      "val predicted: (59575,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59575, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (425, 784) (425,)\n",
      "trainset after (450, 784) (450,)\n",
      "updated train set: (450, 784) (450,) unique(labels): [50 52 50 50 40 41 40 43 33 51] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59550, 784) (59550,)\n",
      "\n",
      "Train set: (450, 784) y: (450,)\n",
      "Val   set: (59550, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 18\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 85.110000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.94       980\n",
      "         1.0       0.87      0.98      0.92      1135\n",
      "         2.0       0.85      0.85      0.85      1032\n",
      "         3.0       0.77      0.87      0.82      1010\n",
      "         4.0       0.85      0.80      0.82       982\n",
      "         5.0       0.88      0.78      0.83       892\n",
      "         6.0       0.92      0.86      0.89       958\n",
      "         7.0       0.95      0.84      0.89      1028\n",
      "         8.0       0.91      0.59      0.71       974\n",
      "         9.0       0.71      0.93      0.81      1009\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.86      0.85      0.85     10000\n",
      "weighted avg       0.86      0.85      0.85     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.847498\n",
      "--------------------------------\n",
      "val predicted: (59550,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59550, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (450, 784) (450,)\n",
      "trainset after (475, 784) (475,)\n",
      "updated train set: (475, 784) (475,) unique(labels): [54 56 51 51 42 44 44 47 35 51] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59525, 784) (59525,)\n",
      "\n",
      "Train set: (475, 784) y: (475,)\n",
      "Val   set: (59525, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 19\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.590000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.93       980\n",
      "         1.0       0.88      0.98      0.93      1135\n",
      "         2.0       0.88      0.86      0.87      1032\n",
      "         3.0       0.80      0.89      0.84      1010\n",
      "         4.0       0.88      0.80      0.84       982\n",
      "         5.0       0.90      0.79      0.84       892\n",
      "         6.0       0.92      0.88      0.90       958\n",
      "         7.0       0.95      0.86      0.90      1028\n",
      "         8.0       0.91      0.66      0.77       974\n",
      "         9.0       0.74      0.92      0.82      1009\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.86      0.86     10000\n",
      "weighted avg       0.87      0.87      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.863219\n",
      "--------------------------------\n",
      "val predicted: (59525,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59525, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (475, 784) (475,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [58 60 56 52 44 44 45 48 38 55] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 20\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.490000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94       980\n",
      "         1.0       0.88      0.98      0.93      1135\n",
      "         2.0       0.86      0.85      0.86      1032\n",
      "         3.0       0.81      0.88      0.85      1010\n",
      "         4.0       0.87      0.80      0.83       982\n",
      "         5.0       0.89      0.76      0.82       892\n",
      "         6.0       0.92      0.88      0.90       958\n",
      "         7.0       0.94      0.86      0.90      1028\n",
      "         8.0       0.90      0.69      0.78       974\n",
      "         9.0       0.74      0.92      0.82      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.87      0.86      0.86     10000\n",
      "weighted avg       0.87      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.862271\n",
      "--------------------------------\n",
      "final active learning accuracies [45.28, 56.25, 63.970000000000006, 68.54, 72.98, 74.77000000000001, 75.8, 77.25, 78.82000000000001, 80.5, 80.80000000000001, 81.54, 81.42, 82.21000000000001, 83.49, 84.37, 84.54, 85.11, 86.59, 86.49]\n",
      "saved Active-learning-experiment-15.pkl /content ['.config', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "{\n",
      "  \"NBModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          61.73,\n",
      "          70.22,\n",
      "          75.78,\n",
      "          78.9,\n",
      "          80.9\n",
      "        ]\n",
      "      ],\n",
      "      \"25\": [\n",
      "        [\n",
      "          39.28,\n",
      "          54.82,\n",
      "          58.26,\n",
      "          62.580000000000005,\n",
      "          69.48,\n",
      "          74.18,\n",
      "          74.53,\n",
      "          75.28,\n",
      "          77.16,\n",
      "          76.38000000000001,\n",
      "          75.94999999999999,\n",
      "          77.92999999999999,\n",
      "          78.46,\n",
      "          80.32000000000001,\n",
      "          81.47999999999999,\n",
      "          81.92,\n",
      "          82.72,\n",
      "          82.26,\n",
      "          82.36,\n",
      "          82.34\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          73.74000000000001,\n",
      "          80.16\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          60.099999999999994,\n",
      "          70.30999999999999,\n",
      "          76.4,\n",
      "          78.11,\n",
      "          79.95\n",
      "        ]\n",
      "      ],\n",
      "      \"25\": [\n",
      "        [\n",
      "          32.79,\n",
      "          38.79,\n",
      "          48.91,\n",
      "          64.21,\n",
      "          64.91,\n",
      "          68.72,\n",
      "          68.23,\n",
      "          71.07,\n",
      "          72.53,\n",
      "          73.81,\n",
      "          74.53999999999999,\n",
      "          75.14,\n",
      "          75.26,\n",
      "          75.92999999999999,\n",
      "          76.14999999999999,\n",
      "          77.09,\n",
      "          77.66999999999999,\n",
      "          78.06,\n",
      "          78.08,\n",
      "          78.35\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          75.47,\n",
      "          81.58\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"RfModel\": {\n",
      "    \"RandomSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          69.06,\n",
      "          76.87,\n",
      "          82.89999999999999,\n",
      "          85.24000000000001,\n",
      "          86.46000000000001\n",
      "        ]\n",
      "      ],\n",
      "      \"25\": [\n",
      "        [\n",
      "          45.28,\n",
      "          56.25,\n",
      "          63.970000000000006,\n",
      "          68.54,\n",
      "          72.98,\n",
      "          74.77000000000001,\n",
      "          75.8,\n",
      "          77.25,\n",
      "          78.82000000000001,\n",
      "          80.5,\n",
      "          80.80000000000001,\n",
      "          81.54,\n",
      "          81.42,\n",
      "          82.21000000000001,\n",
      "          83.49,\n",
      "          84.37,\n",
      "          84.54,\n",
      "          85.11,\n",
      "          86.59,\n",
      "          86.49\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          83.72,\n",
      "          87.48\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"SvmModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          76.61,\n",
      "          83.61,\n",
      "          86.0,\n",
      "          85.95,\n",
      "          88.22\n",
      "        ]\n",
      "      ],\n",
      "      \"25\": [\n",
      "        [\n",
      "          46.53,\n",
      "          68.43,\n",
      "          73.85000000000001,\n",
      "          80.17,\n",
      "          82.69,\n",
      "          83.41,\n",
      "          83.47,\n",
      "          84.43,\n",
      "          84.86,\n",
      "          85.69,\n",
      "          85.59,\n",
      "          86.24000000000001,\n",
      "          86.77,\n",
      "          87.53999999999999,\n",
      "          87.76,\n",
      "          87.68,\n",
      "          87.98,\n",
      "          88.33,\n",
      "          88.56,\n",
      "          88.29\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          83.3,\n",
      "          87.47\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"100\": [\n",
      "        [\n",
      "          75.28,\n",
      "          82.83,\n",
      "          85.69,\n",
      "          86.61,\n",
      "          87.77000000000001\n",
      "        ]\n",
      "      ],\n",
      "      \"25\": [\n",
      "        [\n",
      "          49.830000000000005,\n",
      "          66.03999999999999,\n",
      "          75.89,\n",
      "          77.85,\n",
      "          79.78,\n",
      "          81.67,\n",
      "          82.98,\n",
      "          83.86,\n",
      "          84.65,\n",
      "          84.86,\n",
      "          84.93,\n",
      "          85.25,\n",
      "          86.16,\n",
      "          86.33999999999999,\n",
      "          87.16000000000001,\n",
      "          86.66,\n",
      "          87.64,\n",
      "          87.41,\n",
      "          87.48,\n",
      "          87.25\n",
      "        ]\n",
      "      ],\n",
      "      \"250\": [\n",
      "        [\n",
      "          82.77,\n",
      "          87.12\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 16, using model = RfModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
      "\n",
      "initial random chosen samples (250,)\n",
      "initial train set: (250, 784) (250,) unique(labels): [24 27 21 36 26 20 25 24 22 25] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,) (250,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.610000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.95      0.93       980\n",
      "         1.0       0.86      0.97      0.91      1135\n",
      "         2.0       0.81      0.74      0.78      1032\n",
      "         3.0       0.66      0.92      0.77      1010\n",
      "         4.0       0.80      0.88      0.84       982\n",
      "         5.0       0.87      0.57      0.69       892\n",
      "         6.0       0.87      0.89      0.88       958\n",
      "         7.0       0.88      0.84      0.86      1028\n",
      "         8.0       0.87      0.70      0.78       974\n",
      "         9.0       0.82      0.75      0.78      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.84      0.82      0.82     10000\n",
      "weighted avg       0.83      0.83      0.82     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.821414\n",
      "--------------------------------\n",
      "val predicted: (59750,) [5. 0. 4. ... 5. 0. 5.]\n",
      "probabilities: (59750, 10) \n",
      " [5 0 4 ... 5 0 5]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [38 32 56 61 51 60 37 44 57 64] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 90.160000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96       980\n",
      "         1.0       0.95      0.98      0.97      1135\n",
      "         2.0       0.92      0.89      0.90      1032\n",
      "         3.0       0.86      0.90      0.88      1010\n",
      "         4.0       0.91      0.89      0.90       982\n",
      "         5.0       0.82      0.86      0.84       892\n",
      "         6.0       0.96      0.89      0.92       958\n",
      "         7.0       0.96      0.86      0.91      1028\n",
      "         8.0       0.87      0.85      0.86       974\n",
      "         9.0       0.82      0.93      0.87      1009\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.900432\n",
      "--------------------------------\n",
      "final active learning accuracies [82.61, 90.16]\n",
      "saved Active-learning-experiment-16.pkl /content ['.config', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 17, using model = RfModel, selection_function = MarginSamplingSelection, k = 100, iteration = 0.\n",
      "\n",
      "initial random chosen samples (100,)\n",
      "initial train set: (100, 784) (100,) unique(labels): [ 6 10  9 12  8  9 10 11 10 15] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,) (100,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 67.720000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.71      0.81       980\n",
      "         1.0       0.79      0.94      0.86      1135\n",
      "         2.0       0.87      0.59      0.70      1032\n",
      "         3.0       0.56      0.90      0.69      1010\n",
      "         4.0       0.86      0.32      0.47       982\n",
      "         5.0       0.71      0.42      0.53       892\n",
      "         6.0       0.74      0.84      0.79       958\n",
      "         7.0       0.64      0.75      0.69      1028\n",
      "         8.0       0.72      0.46      0.56       974\n",
      "         9.0       0.44      0.77      0.56      1009\n",
      "\n",
      "    accuracy                           0.68     10000\n",
      "   macro avg       0.73      0.67      0.67     10000\n",
      "weighted avg       0.73      0.68      0.67     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.665367\n",
      "--------------------------------\n",
      "val predicted: (59900,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59900, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [17 12 24 17 27 29 15 18 22 19] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 83.870000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.92      0.93       980\n",
      "         1.0       0.91      0.93      0.92      1135\n",
      "         2.0       0.80      0.88      0.83      1032\n",
      "         3.0       0.92      0.76      0.83      1010\n",
      "         4.0       0.75      0.87      0.81       982\n",
      "         5.0       0.69      0.86      0.77       892\n",
      "         6.0       0.92      0.79      0.85       958\n",
      "         7.0       0.88      0.85      0.87      1028\n",
      "         8.0       0.82      0.84      0.83       974\n",
      "         9.0       0.80      0.68      0.74      1009\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.85      0.84      0.84     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.837357\n",
      "--------------------------------\n",
      "val predicted: (59800,) [5. 0. 4. ... 5. 5. 8.]\n",
      "probabilities: (59800, 10) \n",
      " [5 0 4 ... 5 5 8]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [25 16 32 29 38 38 27 24 34 37] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 88.450000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95       980\n",
      "         1.0       0.94      0.96      0.95      1135\n",
      "         2.0       0.89      0.86      0.87      1032\n",
      "         3.0       0.89      0.78      0.83      1010\n",
      "         4.0       0.85      0.90      0.87       982\n",
      "         5.0       0.81      0.85      0.83       892\n",
      "         6.0       0.92      0.91      0.91       958\n",
      "         7.0       0.95      0.86      0.90      1028\n",
      "         8.0       0.86      0.88      0.87       974\n",
      "         9.0       0.80      0.89      0.84      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.89      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.883231\n",
      "--------------------------------\n",
      "val predicted: (59700,) [3. 0. 4. ... 5. 6. 5.]\n",
      "probabilities: (59700, 10) \n",
      " [3 0 4 ... 5 6 5]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [29 18 41 47 52 46 36 34 51 46] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 90.500000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.97      0.96       980\n",
      "         1.0       0.97      0.96      0.97      1135\n",
      "         2.0       0.92      0.86      0.89      1032\n",
      "         3.0       0.86      0.89      0.87      1010\n",
      "         4.0       0.84      0.95      0.89       982\n",
      "         5.0       0.94      0.80      0.87       892\n",
      "         6.0       0.91      0.94      0.93       958\n",
      "         7.0       0.94      0.89      0.91      1028\n",
      "         8.0       0.86      0.92      0.89       974\n",
      "         9.0       0.88      0.86      0.87      1009\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.90      0.90     10000\n",
      "weighted avg       0.91      0.91      0.90     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.903780\n",
      "--------------------------------\n",
      "val predicted: (59600,) [3. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59600, 10) \n",
      " [3 0 4 ... 5 6 8]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [36 20 51 61 56 71 38 41 61 65] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 91.610000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97       980\n",
      "         1.0       0.98      0.97      0.97      1135\n",
      "         2.0       0.92      0.89      0.91      1032\n",
      "         3.0       0.87      0.89      0.88      1010\n",
      "         4.0       0.92      0.90      0.91       982\n",
      "         5.0       0.84      0.88      0.86       892\n",
      "         6.0       0.94      0.92      0.93       958\n",
      "         7.0       0.97      0.88      0.92      1028\n",
      "         8.0       0.92      0.89      0.91       974\n",
      "         9.0       0.84      0.94      0.89      1009\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.915171\n",
      "--------------------------------\n",
      "final active learning accuracies [67.72, 83.87, 88.44999999999999, 90.5, 91.61]\n",
      "saved Active-learning-experiment-17.pkl /content ['.config', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 18, using model = RfModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
      "\n",
      "initial random chosen samples (25,)\n",
      "initial train set: (25, 784) (25,) unique(labels): [1 1 2 4 2 2 1 4 7 1] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59975, 784) (59975,) (25,)\n",
      "\n",
      "Train set: (25, 784) y: (25,)\n",
      "Val   set: (59975, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 32.330000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.27      0.39       980\n",
      "         1.0       0.67      0.03      0.06      1135\n",
      "         2.0       0.69      0.27      0.39      1032\n",
      "         3.0       0.48      0.70      0.57      1010\n",
      "         4.0       0.78      0.05      0.10       982\n",
      "         5.0       0.64      0.10      0.17       892\n",
      "         6.0       0.92      0.12      0.21       958\n",
      "         7.0       0.33      0.81      0.47      1028\n",
      "         8.0       0.17      0.85      0.29       974\n",
      "         9.0       0.75      0.03      0.05      1009\n",
      "\n",
      "    accuracy                           0.32     10000\n",
      "   macro avg       0.62      0.32      0.27     10000\n",
      "weighted avg       0.62      0.32      0.27     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.269956\n",
      "--------------------------------\n",
      "val predicted: (59975,) [3. 7. 3. ... 7. 7. 8.]\n",
      "probabilities: (59975, 10) \n",
      " [3 7 3 ... 7 7 8]\n",
      "trainset before (25, 784) (25,)\n",
      "trainset after (50, 784) (50,)\n",
      "updated train set: (50, 784) (50,) unique(labels): [2 2 4 7 4 5 5 6 8 7] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59950, 784) (59950,)\n",
      "\n",
      "Train set: (50, 784) y: (50,)\n",
      "Val   set: (59950, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 59.510000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.51      0.65       980\n",
      "         1.0       0.96      0.40      0.57      1135\n",
      "         2.0       0.86      0.61      0.71      1032\n",
      "         3.0       0.52      0.81      0.63      1010\n",
      "         4.0       0.86      0.53      0.65       982\n",
      "         5.0       0.57      0.34      0.43       892\n",
      "         6.0       0.69      0.77      0.73       958\n",
      "         7.0       0.62      0.74      0.68      1028\n",
      "         8.0       0.33      0.68      0.44       974\n",
      "         9.0       0.47      0.55      0.50      1009\n",
      "\n",
      "    accuracy                           0.60     10000\n",
      "   macro avg       0.68      0.59      0.60     10000\n",
      "weighted avg       0.68      0.60      0.60     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.599398\n",
      "--------------------------------\n",
      "val predicted: (59950,) [3. 0. 9. ... 5. 9. 8.]\n",
      "probabilities: (59950, 10) \n",
      " [3 0 9 ... 5 9 8]\n",
      "trainset before (50, 784) (50,)\n",
      "trainset after (75, 784) (75,)\n",
      "updated train set: (75, 784) (75,) unique(labels): [ 6  3  7  9  6  9  5  9 12  9] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59925, 784) (59925,)\n",
      "\n",
      "Train set: (75, 784) y: (75,)\n",
      "Val   set: (59925, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 74.010000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.80      0.83       980\n",
      "         1.0       0.93      0.72      0.81      1135\n",
      "         2.0       0.67      0.67      0.67      1032\n",
      "         3.0       0.64      0.78      0.70      1010\n",
      "         4.0       0.85      0.73      0.79       982\n",
      "         5.0       0.84      0.54      0.66       892\n",
      "         6.0       0.81      0.69      0.75       958\n",
      "         7.0       0.79      0.86      0.82      1028\n",
      "         8.0       0.54      0.85      0.66       974\n",
      "         9.0       0.70      0.73      0.72      1009\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.76      0.74      0.74     10000\n",
      "weighted avg       0.76      0.74      0.74     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.740811\n",
      "--------------------------------\n",
      "val predicted: (59925,) [5. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59925, 10) \n",
      " [5 0 4 ... 5 0 8]\n",
      "trainset before (75, 784) (75,)\n",
      "trainset after (100, 784) (100,)\n",
      "updated train set: (100, 784) (100,) unique(labels): [ 8  8  9 11  9 14  8 10 12 11] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59900, 784) (59900,)\n",
      "\n",
      "Train set: (100, 784) y: (100,)\n",
      "Val   set: (59900, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 77.690000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.85      0.86       980\n",
      "         1.0       0.85      0.95      0.89      1135\n",
      "         2.0       0.85      0.64      0.73      1032\n",
      "         3.0       0.66      0.78      0.71      1010\n",
      "         4.0       0.79      0.78      0.79       982\n",
      "         5.0       0.65      0.70      0.67       892\n",
      "         6.0       0.82      0.72      0.77       958\n",
      "         7.0       0.82      0.84      0.83      1028\n",
      "         8.0       0.71      0.76      0.74       974\n",
      "         9.0       0.78      0.72      0.75      1009\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.77      0.77     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.773929\n",
      "--------------------------------\n",
      "val predicted: (59900,) [5. 0. 4. ... 5. 0. 8.]\n",
      "probabilities: (59900, 10) \n",
      " [5 0 4 ... 5 0 8]\n",
      "trainset before (100, 784) (100,)\n",
      "trainset after (125, 784) (125,)\n",
      "updated train set: (125, 784) (125,) unique(labels): [10 11  9 13 10 17 14 14 13 14] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59875, 784) (59875,)\n",
      "\n",
      "Train set: (125, 784) y: (125,)\n",
      "Val   set: (59875, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 78.960000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.87      0.88       980\n",
      "         1.0       0.80      0.97      0.87      1135\n",
      "         2.0       0.91      0.53      0.67      1032\n",
      "         3.0       0.79      0.76      0.77      1010\n",
      "         4.0       0.88      0.71      0.79       982\n",
      "         5.0       0.75      0.76      0.75       892\n",
      "         6.0       0.76      0.92      0.83       958\n",
      "         7.0       0.77      0.90      0.83      1028\n",
      "         8.0       0.70      0.74      0.72       974\n",
      "         9.0       0.73      0.72      0.73      1009\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.80      0.79      0.78     10000\n",
      "weighted avg       0.80      0.79      0.79     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.784526\n",
      "--------------------------------\n",
      "val predicted: (59875,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59875, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (125, 784) (125,)\n",
      "trainset after (150, 784) (150,)\n",
      "updated train set: (150, 784) (150,) unique(labels): [12 11 15 19 10 21 15 15 15 17] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59850, 784) (59850,)\n",
      "\n",
      "Train set: (150, 784) y: (150,)\n",
      "Val   set: (59850, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 6\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 82.930000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.87      0.90       980\n",
      "         1.0       0.85      0.97      0.91      1135\n",
      "         2.0       0.91      0.73      0.81      1032\n",
      "         3.0       0.81      0.85      0.83      1010\n",
      "         4.0       0.93      0.65      0.76       982\n",
      "         5.0       0.70      0.85      0.77       892\n",
      "         6.0       0.85      0.92      0.88       958\n",
      "         7.0       0.87      0.87      0.87      1028\n",
      "         8.0       0.80      0.73      0.76       974\n",
      "         9.0       0.72      0.85      0.78      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.84      0.83      0.83     10000\n",
      "weighted avg       0.84      0.83      0.83     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.826694\n",
      "--------------------------------\n",
      "val predicted: (59850,) [5. 0. 4. ... 5. 6. 8.]\n",
      "probabilities: (59850, 10) \n",
      " [5 0 4 ... 5 6 8]\n",
      "trainset before (150, 784) (150,)\n",
      "trainset after (175, 784) (175,)\n",
      "updated train set: (175, 784) (175,) unique(labels): [16 11 19 20 16 23 16 18 15 21] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59825, 784) (59825,)\n",
      "\n",
      "Train set: (175, 784) y: (175,)\n",
      "Val   set: (59825, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 7\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 84.580000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.93      0.93       980\n",
      "         1.0       0.89      0.96      0.92      1135\n",
      "         2.0       0.90      0.77      0.83      1032\n",
      "         3.0       0.81      0.84      0.82      1010\n",
      "         4.0       0.87      0.83      0.85       982\n",
      "         5.0       0.73      0.83      0.78       892\n",
      "         6.0       0.89      0.90      0.90       958\n",
      "         7.0       0.85      0.86      0.86      1028\n",
      "         8.0       0.86      0.69      0.76       974\n",
      "         9.0       0.76      0.83      0.79      1009\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.84      0.84     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.843637\n",
      "--------------------------------\n",
      "val predicted: (59825,) [5. 0. 4. ... 5. 6. 7.]\n",
      "probabilities: (59825, 10) \n",
      " [5 0 4 ... 5 6 7]\n",
      "trainset before (175, 784) (175,)\n",
      "trainset after (200, 784) (200,)\n",
      "updated train set: (200, 784) (200,) unique(labels): [18 13 23 23 20 24 16 20 20 23] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59800, 784) (59800,)\n",
      "\n",
      "Train set: (200, 784) y: (200,)\n",
      "Val   set: (59800, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 8\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 86.060000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93       980\n",
      "         1.0       0.90      0.96      0.93      1135\n",
      "         2.0       0.88      0.82      0.85      1032\n",
      "         3.0       0.82      0.83      0.82      1010\n",
      "         4.0       0.82      0.90      0.86       982\n",
      "         5.0       0.76      0.81      0.78       892\n",
      "         6.0       0.91      0.89      0.90       958\n",
      "         7.0       0.89      0.86      0.88      1028\n",
      "         8.0       0.90      0.75      0.81       974\n",
      "         9.0       0.80      0.83      0.81      1009\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.858711\n",
      "--------------------------------\n",
      "val predicted: (59800,) [5. 0. 4. ... 5. 6. 5.]\n",
      "probabilities: (59800, 10) \n",
      " [5 0 4 ... 5 6 5]\n",
      "trainset before (200, 784) (200,)\n",
      "trainset after (225, 784) (225,)\n",
      "updated train set: (225, 784) (225,) unique(labels): [19 13 23 27 21 26 20 24 25 27] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59775, 784) (59775,)\n",
      "\n",
      "Train set: (225, 784) y: (225,)\n",
      "Val   set: (59775, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 9\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.660000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.96      0.94       980\n",
      "         1.0       0.92      0.96      0.94      1135\n",
      "         2.0       0.93      0.79      0.85      1032\n",
      "         3.0       0.81      0.91      0.86      1010\n",
      "         4.0       0.89      0.86      0.88       982\n",
      "         5.0       0.88      0.80      0.84       892\n",
      "         6.0       0.89      0.93      0.91       958\n",
      "         7.0       0.86      0.88      0.87      1028\n",
      "         8.0       0.89      0.80      0.84       974\n",
      "         9.0       0.79      0.86      0.82      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.875297\n",
      "--------------------------------\n",
      "val predicted: (59775,) [5. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59775, 10) \n",
      " [5 0 4 ... 3 5 6]\n",
      "trainset before (225, 784) (225,)\n",
      "trainset after (250, 784) (250,)\n",
      "updated train set: (250, 784) (250,) unique(labels): [19 14 25 30 22 31 22 29 27 31] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59750, 784) (59750,)\n",
      "\n",
      "Train set: (250, 784) y: (250,)\n",
      "Val   set: (59750, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 10\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 87.620000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.93      0.94       980\n",
      "         1.0       0.91      0.96      0.94      1135\n",
      "         2.0       0.94      0.78      0.85      1032\n",
      "         3.0       0.80      0.90      0.85      1010\n",
      "         4.0       0.93      0.82      0.87       982\n",
      "         5.0       0.83      0.83      0.83       892\n",
      "         6.0       0.89      0.94      0.92       958\n",
      "         7.0       0.87      0.93      0.90      1028\n",
      "         8.0       0.90      0.77      0.83       974\n",
      "         9.0       0.78      0.89      0.83      1009\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.874826\n",
      "--------------------------------\n",
      "val predicted: (59750,) [5. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59750, 10) \n",
      " [5 0 4 ... 3 5 6]\n",
      "trainset before (250, 784) (250,)\n",
      "trainset after (275, 784) (275,)\n",
      "updated train set: (275, 784) (275,) unique(labels): [21 14 28 31 26 35 27 29 31 33] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59725, 784) (59725,)\n",
      "\n",
      "Train set: (275, 784) y: (275,)\n",
      "Val   set: (59725, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 11\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 89.700000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96       980\n",
      "         1.0       0.95      0.95      0.95      1135\n",
      "         2.0       0.94      0.87      0.90      1032\n",
      "         3.0       0.88      0.89      0.88      1010\n",
      "         4.0       0.94      0.83      0.88       982\n",
      "         5.0       0.88      0.82      0.85       892\n",
      "         6.0       0.90      0.97      0.93       958\n",
      "         7.0       0.90      0.92      0.91      1028\n",
      "         8.0       0.87      0.85      0.86       974\n",
      "         9.0       0.78      0.90      0.84      1009\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.895971\n",
      "--------------------------------\n",
      "val predicted: (59725,) [5. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59725, 10) \n",
      " [5 0 4 ... 3 5 6]\n",
      "trainset before (275, 784) (275,)\n",
      "trainset after (300, 784) (300,)\n",
      "updated train set: (300, 784) (300,) unique(labels): [24 15 29 33 32 37 28 31 34 37] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59700, 784) (59700,)\n",
      "\n",
      "Train set: (300, 784) y: (300,)\n",
      "Val   set: (59700, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 12\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 89.620000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.97      0.96       980\n",
      "         1.0       0.93      0.96      0.95      1135\n",
      "         2.0       0.96      0.84      0.89      1032\n",
      "         3.0       0.87      0.87      0.87      1010\n",
      "         4.0       0.89      0.86      0.88       982\n",
      "         5.0       0.85      0.87      0.86       892\n",
      "         6.0       0.91      0.95      0.93       958\n",
      "         7.0       0.91      0.90      0.90      1028\n",
      "         8.0       0.88      0.84      0.86       974\n",
      "         9.0       0.82      0.89      0.85      1009\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.895089\n",
      "--------------------------------\n",
      "val predicted: (59700,) [3. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59700, 10) \n",
      " [3 0 4 ... 3 5 6]\n",
      "trainset before (300, 784) (300,)\n",
      "trainset after (325, 784) (325,)\n",
      "updated train set: (325, 784) (325,) unique(labels): [25 16 31 39 34 38 31 31 38 42] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59675, 784) (59675,)\n",
      "\n",
      "Train set: (325, 784) y: (325,)\n",
      "Val   set: (59675, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 13\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 89.970000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.96       980\n",
      "         1.0       0.94      0.97      0.95      1135\n",
      "         2.0       0.94      0.84      0.89      1032\n",
      "         3.0       0.86      0.92      0.89      1010\n",
      "         4.0       0.88      0.88      0.88       982\n",
      "         5.0       0.91      0.84      0.88       892\n",
      "         6.0       0.88      0.97      0.92       958\n",
      "         7.0       0.91      0.88      0.89      1028\n",
      "         8.0       0.91      0.84      0.88       974\n",
      "         9.0       0.81      0.89      0.85      1009\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.898796\n",
      "--------------------------------\n",
      "val predicted: (59675,) [5. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59675, 10) \n",
      " [5 0 4 ... 3 5 6]\n",
      "trainset before (325, 784) (325,)\n",
      "trainset after (350, 784) (350,)\n",
      "updated train set: (350, 784) (350,) unique(labels): [27 17 37 42 37 39 32 33 41 45] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59650, 784) (59650,)\n",
      "\n",
      "Train set: (350, 784) y: (350,)\n",
      "Val   set: (59650, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 14\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 91.260000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.96       980\n",
      "         1.0       0.96      0.97      0.97      1135\n",
      "         2.0       0.94      0.90      0.92      1032\n",
      "         3.0       0.86      0.92      0.89      1010\n",
      "         4.0       0.90      0.91      0.91       982\n",
      "         5.0       0.94      0.84      0.89       892\n",
      "         6.0       0.90      0.96      0.93       958\n",
      "         7.0       0.92      0.89      0.91      1028\n",
      "         8.0       0.93      0.85      0.89       974\n",
      "         9.0       0.83      0.90      0.87      1009\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.911669\n",
      "--------------------------------\n",
      "val predicted: (59650,) [5. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59650, 10) \n",
      " [5 0 4 ... 3 5 6]\n",
      "trainset before (350, 784) (350,)\n",
      "trainset after (375, 784) (375,)\n",
      "updated train set: (375, 784) (375,) unique(labels): [27 17 42 45 38 44 33 35 46 48] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59625, 784) (59625,)\n",
      "\n",
      "Train set: (375, 784) y: (375,)\n",
      "Val   set: (59625, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 15\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 91.350000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96       980\n",
      "         1.0       0.97      0.96      0.97      1135\n",
      "         2.0       0.93      0.91      0.92      1032\n",
      "         3.0       0.87      0.92      0.89      1010\n",
      "         4.0       0.92      0.88      0.90       982\n",
      "         5.0       0.90      0.86      0.88       892\n",
      "         6.0       0.91      0.96      0.94       958\n",
      "         7.0       0.93      0.89      0.91      1028\n",
      "         8.0       0.93      0.85      0.89       974\n",
      "         9.0       0.81      0.92      0.86      1009\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.92      0.91      0.91     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.912767\n",
      "--------------------------------\n",
      "val predicted: (59625,) [5. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59625, 10) \n",
      " [5 0 4 ... 3 5 6]\n",
      "trainset before (375, 784) (375,)\n",
      "trainset after (400, 784) (400,)\n",
      "updated train set: (400, 784) (400,) unique(labels): [28 18 46 47 40 48 35 35 54 49] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59600, 784) (59600,)\n",
      "\n",
      "Train set: (400, 784) y: (400,)\n",
      "Val   set: (59600, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 16\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 91.660000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.96      0.96       980\n",
      "         1.0       0.97      0.97      0.97      1135\n",
      "         2.0       0.93      0.91      0.92      1032\n",
      "         3.0       0.90      0.88      0.89      1010\n",
      "         4.0       0.92      0.88      0.90       982\n",
      "         5.0       0.87      0.89      0.88       892\n",
      "         6.0       0.92      0.96      0.94       958\n",
      "         7.0       0.94      0.89      0.92      1028\n",
      "         8.0       0.91      0.89      0.90       974\n",
      "         9.0       0.84      0.92      0.88      1009\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.915790\n",
      "--------------------------------\n",
      "val predicted: (59600,) [5. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59600, 10) \n",
      " [5 0 4 ... 3 5 6]\n",
      "trainset before (400, 784) (400,)\n",
      "trainset after (425, 784) (425,)\n",
      "updated train set: (425, 784) (425,) unique(labels): [28 18 49 50 43 49 39 36 60 53] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59575, 784) (59575,)\n",
      "\n",
      "Train set: (425, 784) y: (425,)\n",
      "Val   set: (59575, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 17\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 91.860000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.96      0.97       980\n",
      "         1.0       0.98      0.96      0.97      1135\n",
      "         2.0       0.95      0.92      0.93      1032\n",
      "         3.0       0.90      0.89      0.90      1010\n",
      "         4.0       0.94      0.88      0.91       982\n",
      "         5.0       0.89      0.85      0.87       892\n",
      "         6.0       0.93      0.96      0.95       958\n",
      "         7.0       0.94      0.90      0.92      1028\n",
      "         8.0       0.85      0.92      0.88       974\n",
      "         9.0       0.84      0.92      0.88      1009\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.917800\n",
      "--------------------------------\n",
      "val predicted: (59575,) [5. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59575, 10) \n",
      " [5 0 4 ... 3 5 6]\n",
      "trainset before (425, 784) (425,)\n",
      "trainset after (450, 784) (450,)\n",
      "updated train set: (450, 784) (450,) unique(labels): [30 18 53 56 46 51 39 37 65 55] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59550, 784) (59550,)\n",
      "\n",
      "Train set: (450, 784) y: (450,)\n",
      "Val   set: (59550, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 18\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 92.000000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97       980\n",
      "         1.0       0.98      0.96      0.97      1135\n",
      "         2.0       0.94      0.93      0.93      1032\n",
      "         3.0       0.88      0.89      0.88      1010\n",
      "         4.0       0.94      0.89      0.91       982\n",
      "         5.0       0.88      0.86      0.87       892\n",
      "         6.0       0.94      0.96      0.95       958\n",
      "         7.0       0.95      0.88      0.91      1028\n",
      "         8.0       0.88      0.93      0.90       974\n",
      "         9.0       0.84      0.93      0.88      1009\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.919244\n",
      "--------------------------------\n",
      "val predicted: (59550,) [3. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59550, 10) \n",
      " [3 0 4 ... 3 5 6]\n",
      "trainset before (450, 784) (450,)\n",
      "trainset after (475, 784) (475,)\n",
      "updated train set: (475, 784) (475,) unique(labels): [32 19 56 62 48 53 39 42 67 57] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59525, 784) (59525,)\n",
      "\n",
      "Train set: (475, 784) y: (475,)\n",
      "Val   set: (59525, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 19\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 92.370000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.96       980\n",
      "         1.0       0.98      0.97      0.98      1135\n",
      "         2.0       0.93      0.93      0.93      1032\n",
      "         3.0       0.87      0.92      0.90      1010\n",
      "         4.0       0.92      0.91      0.92       982\n",
      "         5.0       0.91      0.86      0.88       892\n",
      "         6.0       0.95      0.95      0.95       958\n",
      "         7.0       0.94      0.91      0.92      1028\n",
      "         8.0       0.91      0.91      0.91       974\n",
      "         9.0       0.86      0.91      0.88      1009\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.922823\n",
      "--------------------------------\n",
      "val predicted: (59525,) [5. 0. 4. ... 3. 5. 6.]\n",
      "probabilities: (59525, 10) \n",
      " [5 0 4 ... 3 5 6]\n",
      "trainset before (475, 784) (475,)\n",
      "trainset after (500, 784) (500,)\n",
      "updated train set: (500, 784) (500,) unique(labels): [33 20 62 65 53 54 40 44 70 59] [0 1 2 3 4 5 6 7 8 9]\n",
      "val set: (59500, 784) (59500,)\n",
      "\n",
      "Train set: (500, 784) y: (500,)\n",
      "Val   set: (59500, 784)\n",
      "Test  set: (10000, 784)\n",
      "training random forest...\n",
      "--------------------------------\n",
      "Iteration: 20\n",
      "--------------------------------\n",
      "y-test set: (10000,)\n",
      "Accuracy rate for classification rate of: 92.670000 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.97      0.96       980\n",
      "         1.0       0.98      0.97      0.98      1135\n",
      "         2.0       0.93      0.94      0.93      1032\n",
      "         3.0       0.87      0.94      0.90      1010\n",
      "         4.0       0.93      0.94      0.94       982\n",
      "         5.0       0.94      0.82      0.87       892\n",
      "         6.0       0.95      0.93      0.94       958\n",
      "         7.0       0.93      0.91      0.92      1028\n",
      "         8.0       0.90      0.93      0.91       974\n",
      "         9.0       0.89      0.90      0.89      1009\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n",
      "\n",
      "F1 Score : 0.925432\n",
      "--------------------------------\n",
      "final active learning accuracies [32.33, 59.51, 74.00999999999999, 77.69, 78.96, 82.93, 84.58, 86.06, 87.66000000000001, 87.62, 89.7, 89.62, 89.97, 91.25999999999999, 91.35, 91.66, 91.86, 92.0, 92.36999999999999, 92.67]\n",
      "saved Active-learning-experiment-18.pkl /content ['.config', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "{'NBModel': {'RandomSelection': {'250': [[75.47, 81.58]], '100': [[60.099999999999994, 70.30999999999999, 76.4, 78.11, 79.95]], '25': [[32.79, 38.79, 48.91, 64.21, 64.91, 68.72, 68.23, 71.07, 72.53, 73.81, 74.53999999999999, 75.14, 75.26, 75.92999999999999, 76.14999999999999, 77.09, 77.66999999999999, 78.06, 78.08, 78.35]]}, 'MarginSamplingSelection': {'250': [[73.74000000000001, 80.16]], '100': [[61.73, 70.22, 75.78, 78.9, 80.9]], '25': [[39.28, 54.82, 58.26, 62.580000000000005, 69.48, 74.18, 74.53, 75.28, 77.16, 76.38000000000001, 75.94999999999999, 77.92999999999999, 78.46, 80.32000000000001, 81.47999999999999, 81.92, 82.72, 82.26, 82.36, 82.34]]}}, 'SvmModel': {'RandomSelection': {'250': [[82.77, 87.12]], '100': [[75.28, 82.83, 85.69, 86.61, 87.77000000000001]], '25': [[49.830000000000005, 66.03999999999999, 75.89, 77.85, 79.78, 81.67, 82.98, 83.86, 84.65, 84.86, 84.93, 85.25, 86.16, 86.33999999999999, 87.16000000000001, 86.66, 87.64, 87.41, 87.48, 87.25]]}, 'MarginSamplingSelection': {'250': [[83.3, 87.47]], '100': [[76.61, 83.61, 86.0, 85.95, 88.22]], '25': [[46.53, 68.43, 73.85000000000001, 80.17, 82.69, 83.41, 83.47, 84.43, 84.86, 85.69, 85.59, 86.24000000000001, 86.77, 87.53999999999999, 87.76, 87.68, 87.98, 88.33, 88.56, 88.29]]}}, 'RfModel': {'RandomSelection': {'250': [[83.72, 87.48]], '100': [[69.06, 76.87, 82.89999999999999, 85.24000000000001, 86.46000000000001]], '25': [[45.28, 56.25, 63.970000000000006, 68.54, 72.98, 74.77000000000001, 75.8, 77.25, 78.82000000000001, 80.5, 80.80000000000001, 81.54, 81.42, 82.21000000000001, 83.49, 84.37, 84.54, 85.11, 86.59, 86.49]]}, 'MarginSamplingSelection': {'250': [[82.61, 90.16]], '100': [[67.72, 83.87, 88.44999999999999, 90.5, 91.61]], '25': [[32.33, 59.51, 74.00999999999999, 77.69, 78.96, 82.93, 84.58, 86.06, 87.66000000000001, 87.62, 89.7, 89.62, 89.97, 91.25999999999999, 91.35, 91.66, 91.86, 92.0, 92.36999999999999, 92.67]]}}}\n",
      "{'NBModel': {'MarginSamplingSelection': {'100': [[61.73, 70.22, 75.78, 78.9, 80.9]], '25': [[39.28, 54.82, 58.26, 62.580000000000005, 69.48, 74.18, 74.53, 75.28, 77.16, 76.38000000000001, 75.94999999999999, 77.92999999999999, 78.46, 80.32000000000001, 81.47999999999999, 81.92, 82.72, 82.26, 82.36, 82.34]], '250': [[73.74000000000001, 80.16]]}, 'RandomSelection': {'100': [[60.099999999999994, 70.30999999999999, 76.4, 78.11, 79.95]], '25': [[32.79, 38.79, 48.91, 64.21, 64.91, 68.72, 68.23, 71.07, 72.53, 73.81, 74.53999999999999, 75.14, 75.26, 75.92999999999999, 76.14999999999999, 77.09, 77.66999999999999, 78.06, 78.08, 78.35]], '250': [[75.47, 81.58]]}}, 'RfModel': {'MarginSamplingSelection': {'100': [[67.72, 83.87, 88.44999999999999, 90.5, 91.61]], '25': [[32.33, 59.51, 74.00999999999999, 77.69, 78.96, 82.93, 84.58, 86.06, 87.66000000000001, 87.62, 89.7, 89.62, 89.97, 91.25999999999999, 91.35, 91.66, 91.86, 92.0, 92.36999999999999, 92.67]], '250': [[82.61, 90.16]]}, 'RandomSelection': {'100': [[69.06, 76.87, 82.89999999999999, 85.24000000000001, 86.46000000000001]], '25': [[45.28, 56.25, 63.970000000000006, 68.54, 72.98, 74.77000000000001, 75.8, 77.25, 78.82000000000001, 80.5, 80.80000000000001, 81.54, 81.42, 82.21000000000001, 83.49, 84.37, 84.54, 85.11, 86.59, 86.49]], '250': [[83.72, 87.48]]}}, 'SvmModel': {'MarginSamplingSelection': {'100': [[76.61, 83.61, 86.0, 85.95, 88.22]], '25': [[46.53, 68.43, 73.85000000000001, 80.17, 82.69, 83.41, 83.47, 84.43, 84.86, 85.69, 85.59, 86.24000000000001, 86.77, 87.53999999999999, 87.76, 87.68, 87.98, 88.33, 88.56, 88.29]], '250': [[83.3, 87.47]]}, 'RandomSelection': {'100': [[75.28, 82.83, 85.69, 86.61, 87.77000000000001]], '25': [[49.830000000000005, 66.03999999999999, 75.89, 77.85, 79.78, 81.67, 82.98, 83.86, 84.65, 84.86, 84.93, 85.25, 86.16, 86.33999999999999, 87.16000000000001, 86.66, 87.64, 87.41, 87.48, 87.25]], '250': [[82.77, 87.12]]}}}\n"
     ]
    }
   ],
   "source": [
    "(X, y) = download()\n",
    "(X_train_full, y_train_full, X_test, y_test) = split(trainset_size)\n",
    "print ('train:', X_train_full.shape, y_train_full.shape)\n",
    "print ('test :', X_test.shape, y_test.shape)\n",
    "classes = len(np.unique(y))\n",
    "print ('unique classes', classes)\n",
    "\n",
    "def pickle_save(fname, data):\n",
    "  filehandler = open(fname,\"wb\")\n",
    "  pickle.dump(data,filehandler)\n",
    "  filehandler.close() \n",
    "  print('saved', fname, os.getcwd(), os.listdir())\n",
    "\n",
    "def pickle_load(fname):\n",
    "  print(os.getcwd(), os.listdir())\n",
    "  file = open(fname,'rb')\n",
    "  data = pickle.load(file)\n",
    "  file.close()\n",
    "  print(data)\n",
    "  return data\n",
    "  \n",
    "def experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
    "    algos_temp = []\n",
    "    print ('stopping at:', max_queried)\n",
    "    count = 0\n",
    "    for model_object in models:\n",
    "      if model_object.__name__ not in d:\n",
    "          d[model_object.__name__] = {}\n",
    "      \n",
    "      for selection_function in selection_functions:\n",
    "        if selection_function.__name__ not in d[model_object.__name__]:\n",
    "            d[model_object.__name__][selection_function.__name__] = {}\n",
    "        \n",
    "        for k in Ks:\n",
    "            d[model_object.__name__][selection_function.__name__][str(k)] = []           \n",
    "            \n",
    "            for i in range(0, repeats):\n",
    "                count+=1\n",
    "                if count >= contfrom:\n",
    "                    print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
    "                    alg = TheAlgorithm(k, \n",
    "                                       model_object, \n",
    "                                       selection_function\n",
    "                                       )\n",
    "                    alg.run(X_train_full, y_train_full, X_test, y_test)\n",
    "                    d[model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
    "                    fname = 'Active-learning-experiment-' + str(count) + '.pkl'\n",
    "                    pickle_save(fname, d)\n",
    "                    if count % 5 == 0:\n",
    "                        print(json.dumps(d, indent=2, sort_keys=True))\n",
    "                    print ()\n",
    "                    print ('---------------------------- FINISHED ---------------------------')\n",
    "                    print ()\n",
    "    return d\n",
    "\n",
    "\n",
    "max_queried = 500 \n",
    "\n",
    "repeats = 1\n",
    "\n",
    "models = [NBModel, SvmModel, RfModel] \n",
    "\n",
    "selection_functions = [RandomSelection, MarginSamplingSelection] \n",
    "\n",
    "Ks = [250,100,25] \n",
    "\n",
    "d = {}\n",
    "stopped_at = -1 \n",
    "\n",
    "\n",
    "d = experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
    "print (d)\n",
    "results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZtaN-Dp3NgYH",
    "outputId": "599a7168-c195-448d-9f60-1a26e56e8b12"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz4AAAGfCAYAAACeFAe6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz4AAAGfCAYAAACeFAe6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQogGTzo+QgghhBBCiAbv/wMytFeS8rW1lAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz4AAAGfCAYAAACeFAe6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njxBCCCGEEOKeJx0fIYQQQgghxD3v/wMDX6jvjiwY6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots\n",
    "\n",
    "def performance_plot(fully_supervised_accuracy, dic, models, selection_functions, Ks, repeats):  \n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.plot([0,500],[fully_supervised_accuracy, fully_supervised_accuracy],label = 'algorithm-upper-bound')\n",
    "    for model_object in models:\n",
    "      for selection_function in selection_functions:\n",
    "        for idx, k in enumerate(Ks):\n",
    "            x = np.arange(float(Ks[idx]), 500 + float(Ks[idx]), float(Ks[idx]))            \n",
    "            Sum = np.array(dic[model_object][selection_function][k][0])\n",
    "            for i in range(1, repeats):\n",
    "                Sum = Sum + np.array(dic[model_object][selection_function][k][i])\n",
    "            mean = Sum / repeats\n",
    "            ax.plot(x, mean ,label = model_object + '-' + selection_function + '-' + str(k))\n",
    "    ax.legend()\n",
    "    ax.set_xlim([50,500])\n",
    "    ax.set_ylim([40,100])\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "models_str = ['NBModel','RfModel','SvmModel'] #['SvmModel', 'RfModel', 'LogModel']\n",
    "selection_functions_str = ['RandomSelection', 'MarginSamplingSelection']\n",
    "Ks_str = ['250','100','25'] \n",
    "repeats = 1\n",
    "random_forest_upper_bound = 97.\n",
    "naive_bayes_bound = 95\n",
    "svm_upper_bound = 94.\n",
    "log_upper_bound = 92.47\n",
    "total_experiments = len(models_str) * len(selection_functions_str) * len(Ks_str) * repeats\n",
    "\n",
    "\n",
    "performance_plot(random_forest_upper_bound, d, ['RfModel'] , selection_functions_str    , Ks_str, 1)\n",
    "performance_plot(naive_bayes_bound, d, ['NBModel'] , selection_functions_str    , Ks_str, 1)\n",
    "performance_plot(svm_upper_bound, d, ['SvmModel'] , selection_functions_str    , Ks_str, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZSUJ8-ony9m"
   },
   "source": [
    "## **Supervised Learning -  MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "83YWvctKnwFT",
    "outputId": "590f8413-9220-4bae-c792-591317460523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (60000, 784) (60000,)\n",
      "test : (10000, 784) (10000,)\n",
      "unique classes 10\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "trainset_size = 60000\n",
    "\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target.astype('float64') \n",
    "(X_train_full, y_train_full, X_test, y_test) = split(trainset_size)\n",
    "print ('train:', X_train_full.shape, y_train_full.shape)\n",
    "print ('test :', X_test.shape, y_test.shape)\n",
    "classes = len(np.unique(y))\n",
    "print ('unique classes', classes)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_full = scaler.fit_transform(X_train_full)\n",
    "X_test  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2h_1-ky0nxcn",
    "outputId": "c4d20ecc-925d-4d12-c87d-c8eff2c90661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naive Bayes:  0.8357\n",
      "F1 Score Naive Bayes:  0.8342123721010664\n"
     ]
    }
   ],
   "source": [
    "# naive bayes\n",
    "bayes_clf = MultinomialNB()\n",
    "bayes_clf.fit(X_train_full, y_train_full)\n",
    "y_pred = bayes_clf.predict(X_test)\n",
    "print(\"Accuracy Naive Bayes: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score Naive Bayes: \", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAiKNS3qq1lM"
   },
   "outputs": [],
   "source": [
    "# linear SVC - it runs FOREVER !\n",
    "linear_svc = SVC(C=1, kernel='linear', probability=True, class_weight='balanced')\n",
    "linear_svc.fit(X_train_full, y_train_full)\n",
    "y_pred = linear_svc.predict(X_test)\n",
    "print(\"Accuracy SVC: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score SVC: \", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DS_FVTKkqi6t",
    "outputId": "521e46fb-c752-441e-e762-fd295dd862f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Random Forest:  0.9671\n",
      "F1 Score Random Forest:  0.9667791494099767\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "forest_clf = RandomForestClassifier(n_estimators=50, class_weight='balanced')\n",
    "forest_clf.fit(X_train_full, y_train_full)\n",
    "y_pred = forest_clf.predict(X_test)\n",
    "print(\"Accuracy Random Forest: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score Random Forest: \", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woRbF2xL4yjy"
   },
   "source": [
    "## **Supervised Learning - Wine Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Scnma_T_twvH"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "features, target = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DZqGOrs8mPZ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hbiLvx9h-J9e"
   },
   "outputs": [],
   "source": [
    "msc = MinMaxScaler()\n",
    "msc.fit(X_train)\n",
    "X_train = msc.transform(X_train)\n",
    "X_test = msc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Hagk8ESF-aPq",
    "outputId": "77c3402e-f7b0-4059-bf5c-f28b24ff8f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naive Bayes:  0.9629629629629629\n",
      "F1 Score Naive Bayes:  0.9645191409897292\n",
      "Accuracy SVC:  1.0\n",
      "F1 Score SVC:  1.0\n",
      "Accuracy Random Forest:  1.0\n",
      "F1 Score Random Forest:  1.0\n"
     ]
    }
   ],
   "source": [
    "# naive bayes\n",
    "bayes_clf = MultinomialNB()\n",
    "bayes_clf.fit(X_train, y_train)\n",
    "y_pred = bayes_clf.predict(X_test)\n",
    "print(\"Accuracy Naive Bayes: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score Naive Bayes: \", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# linear svc\n",
    "linear_svc = SVC(C=1, kernel='linear', probability=True, class_weight='balanced')\n",
    "linear_svc.fit(X_train, y_train)\n",
    "y_pred = linear_svc.predict(X_test)\n",
    "print(\"Accuracy SVC: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score SVC: \", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "\n",
    "# random forest\n",
    "forest_clf = RandomForestClassifier(n_estimators=50, class_weight='balanced')\n",
    "forest_clf.fit(X_train, y_train)\n",
    "y_pred = forest_clf.predict(X_test)\n",
    "print(\"Accuracy Random Forest: \", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score Random Forest: \", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YDWLfdeBaR4"
   },
   "source": [
    "# **Active Learning - Wine Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AoEggRicDzS5"
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "trainset_size = 124\n",
    "max_queried = 500\n",
    "\n",
    "\n",
    "def download():\n",
    " wine = load_wine()\n",
    " X = wine.data\n",
    " y = wine.target  # fetch_openml() returns targets as strings\n",
    " print ('MNIST:', X.shape, y.shape)\n",
    " return (X, y)\n",
    "\n",
    "def split(train_size):\n",
    " X_train_full = X[:train_size]\n",
    " y_train_full = y[:train_size]\n",
    " X_test = X[train_size:]\n",
    " y_test = y[train_size:]\n",
    " return (X_train_full, y_train_full, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TtpoazHEBj4A",
    "outputId": "efa02426-c160-426d-bdd4-86e778bbe71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (124, 13) (124,)\n",
      "test : (54, 13) (54,)\n",
      "unique classes 3\n",
      "train: (124, 13) (124,)\n",
      "test : (54, 13) (54,)\n",
      "unique classes 3\n",
      "stopping at: 50\n",
      "Count = 1, using model = NBModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
      "\n",
      "initial random chosen samples (50,)\n",
      "initial train set: (50, 13) (50,) unique(labels): [23 27] [0 1]\n",
      "val set: (74, 13) (74,) (50,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.26      1.00      0.41         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.09      0.33      0.14        54\n",
      "weighted avg       0.03      0.11      0.05        54\n",
      "\n",
      "\n",
      "F1 Score : 0.137931\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111]\n",
      "saved Active-learning-experiment-1.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 2, using model = NBModel, selection_function = RandomSelection, k = 20, iteration = 0.\n",
      "\n",
      "initial random chosen samples (20,)\n",
      "initial train set: (20, 13) (20,) unique(labels): [ 8 12] [0 1]\n",
      "val set: (104, 13) (104,) (20,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.067797\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [19 21] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.14      1.00      0.24         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.05      0.33      0.08        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.081633\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (60, 13) (60,)\n",
      "updated train set: (60, 13) (60,) unique(labels): [30 30] [0 1]\n",
      "val set: (64, 13) (64,)\n",
      "\n",
      "Train set: (60, 13) y: (60,)\n",
      "Val   set: (64, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.18      1.00      0.30         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.33      0.10        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.100000\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-2.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 3, using model = NBModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
      "\n",
      "initial random chosen samples (10,)\n",
      "initial train set: (10, 13) (10,) unique(labels): [5 5] [0 1]\n",
      "val set: (114, 13) (114,) (10,)\n",
      "\n",
      "Train set: (10, 13) y: (10,)\n",
      "Val   set: (114, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.14      1.00      0.24         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.05      0.33      0.08        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.081633\n",
      "--------------------------------\n",
      "val predicted: (114,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
      " 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "probabilities: (114, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
      " 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "trainset before (10, 13) (10,)\n",
      "trainset after (20, 13) (20,)\n",
      "updated train set: (20, 13) (20,) unique(labels): [ 9 11] [0 1]\n",
      "val set: (104, 13) (104,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.17      1.00      0.29         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.33      0.10        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.097561\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (30, 13) (30,)\n",
      "updated train set: (30, 13) (30,) unique(labels): [12 18] [0 1]\n",
      "val set: (94, 13) (94,)\n",
      "\n",
      "Train set: (30, 13) y: (30,)\n",
      "Val   set: (94, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.067797\n",
      "--------------------------------\n",
      "val predicted: (94,) [0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (94, 2) \n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (30, 13) (30,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [17 23] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.12      1.00      0.21         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.070175\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (50, 13) (50,)\n",
      "updated train set: (50, 13) (50,) unique(labels): [20 30] [0 1]\n",
      "val set: (74, 13) (74,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.12      1.00      0.21         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.068966\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-3.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 4, using model = NBModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
      "\n",
      "initial random chosen samples (50,)\n",
      "initial train set: (50, 13) (50,) unique(labels): [28 22] [0 1]\n",
      "val set: (74, 13) (74,) (50,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.33      1.00      0.50         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.11      0.33      0.17        54\n",
      "weighted avg       0.04      0.11      0.06        54\n",
      "\n",
      "\n",
      "F1 Score : 0.166667\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111]\n",
      "saved Active-learning-experiment-4.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 5, using model = NBModel, selection_function = MarginSamplingSelection, k = 20, iteration = 0.\n",
      "\n",
      "initial random chosen samples (20,)\n",
      "initial train set: (20, 13) (20,) unique(labels): [10 10] [0 1]\n",
      "val set: (104, 13) (104,) (20,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.29      1.00      0.44         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.10      0.33      0.15        54\n",
      "weighted avg       0.03      0.11      0.05        54\n",
      "\n",
      "\n",
      "F1 Score : 0.148148\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (40, 13) (40,) unique(labels): [16 24] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.17      1.00      0.29         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.33      0.10        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.095238\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (60, 13) (60,)\n",
      "updated train set: (60, 13) (60,) unique(labels): [36 24] [0 1]\n",
      "val set: (64, 13) (64,)\n",
      "\n",
      "Train set: (60, 13) y: (60,)\n",
      "Val   set: (64, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 7.407407 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.80      0.67      0.73         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.07        54\n",
      "   macro avg       0.27      0.22      0.24        54\n",
      "weighted avg       0.09      0.07      0.08        54\n",
      "\n",
      "\n",
      "F1 Score : 0.242424\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 7.4074074074074066]\n",
      "saved Active-learning-experiment-5.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "{\n",
      "  \"NBModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          7.4074074074074066\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"10\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 6, using model = NBModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
      "\n",
      "initial random chosen samples (10,)\n",
      "initial train set: (10, 13) (10,) unique(labels): [4 6] [0 1]\n",
      "val set: (114, 13) (114,) (10,)\n",
      "\n",
      "Train set: (10, 13) y: (10,)\n",
      "Val   set: (114, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.21      1.00      0.35         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.07      0.33      0.12        54\n",
      "weighted avg       0.02      0.11      0.04        54\n",
      "\n",
      "\n",
      "F1 Score : 0.117647\n",
      "--------------------------------\n",
      "val predicted: (114,) [0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "probabilities: (114, 2) \n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "trainset before (10, 13) (10,)\n",
      "trainset after (20, 13) (20,)\n",
      "updated train set: (20, 13) (20,) unique(labels): [14  6] [0 1]\n",
      "val set: (104, 13) (104,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 3.703704 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.33      0.50         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.04        54\n",
      "   macro avg       0.33      0.11      0.17        54\n",
      "weighted avg       0.11      0.04      0.06        54\n",
      "\n",
      "\n",
      "F1 Score : 0.166667\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (30, 13) (30,)\n",
      "updated train set: (30, 13) (30,) unique(labels): [15 15] [0 1]\n",
      "val set: (94, 13) (94,)\n",
      "\n",
      "Train set: (30, 13) y: (30,)\n",
      "Val   set: (94, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.43      1.00      0.60         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.14      0.33      0.20        54\n",
      "weighted avg       0.05      0.11      0.07        54\n",
      "\n",
      "\n",
      "F1 Score : 0.200000\n",
      "--------------------------------\n",
      "val predicted: (94,) [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1]\n",
      "probabilities: (94, 2) \n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1]\n",
      "trainset before (30, 13) (30,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [18 22] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.13      1.00      0.23         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.08        54\n",
      "weighted avg       0.01      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.075472\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (50, 13) (50,)\n",
      "updated train set: (50, 13) (50,) unique(labels): [25 25] [0 1]\n",
      "val set: (74, 13) (74,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training Naive Bayes  ...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.30      1.00      0.46         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.10      0.33      0.15        54\n",
      "weighted avg       0.03      0.11      0.05        54\n",
      "\n",
      "\n",
      "F1 Score : 0.153846\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 3.7037037037037033, 11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-6.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 7, using model = SvmModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
      "\n",
      "initial random chosen samples (50,)\n",
      "initial train set: (50, 13) (50,) unique(labels): [23 27] [0 1]\n",
      "val set: (74, 13) (74,) (50,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.12      1.00      0.22         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.074074\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111]\n",
      "saved Active-learning-experiment-7.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 8, using model = SvmModel, selection_function = RandomSelection, k = 20, iteration = 0.\n",
      "\n",
      "initial random chosen samples (20,)\n",
      "initial train set: (20, 13) (20,) unique(labels): [ 7 13] [0 1]\n",
      "val set: (104, 13) (104,) (20,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.067797\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [18 22] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.067797\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0]\n",
      "probabilities: (84, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (60, 13) (60,)\n",
      "updated train set: (60, 13) (60,) unique(labels): [26 34] [0 1]\n",
      "val set: (64, 13) (64,)\n",
      "\n",
      "Train set: (60, 13) y: (60,)\n",
      "Val   set: (64, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.067797\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-8.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 9, using model = SvmModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
      "\n",
      "initial random chosen samples (10,)\n",
      "initial train set: (10, 13) (10,) unique(labels): [6 4] [0 1]\n",
      "val set: (114, 13) (114,) (10,)\n",
      "\n",
      "Train set: (10, 13) y: (10,)\n",
      "Val   set: (114, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.26      1.00      0.41         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.09      0.33      0.14        54\n",
      "weighted avg       0.03      0.11      0.05        54\n",
      "\n",
      "\n",
      "F1 Score : 0.137931\n",
      "--------------------------------\n",
      "val predicted: (114,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 1]\n",
      "probabilities: (114, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 1]\n",
      "trainset before (10, 13) (10,)\n",
      "trainset after (20, 13) (20,)\n",
      "updated train set: (20, 13) (20,) unique(labels): [ 9 11] [0 1]\n",
      "val set: (104, 13) (104,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.17      1.00      0.29         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.33      0.10        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.097561\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (30, 13) (30,)\n",
      "updated train set: (30, 13) (30,) unique(labels): [16 14] [0 1]\n",
      "val set: (94, 13) (94,)\n",
      "\n",
      "Train set: (30, 13) y: (30,)\n",
      "Val   set: (94, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.15      1.00      0.26         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.05      0.33      0.09        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.085106\n",
      "--------------------------------\n",
      "val predicted: (94,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "probabilities: (94, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "trainset before (30, 13) (30,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [22 18] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.17      1.00      0.29         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.33      0.10        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.097561\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (50, 13) (50,)\n",
      "updated train set: (50, 13) (50,) unique(labels): [26 24] [0 1]\n",
      "val set: (74, 13) (74,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.14      1.00      0.25         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.05      0.33      0.08        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.083333\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-9.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 10, using model = SvmModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
      "\n",
      "initial random chosen samples (50,)\n",
      "initial train set: (50, 13) (50,) unique(labels): [26 24] [0 1]\n",
      "val set: (74, 13) (74,) (50,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.12      1.00      0.22         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.074074\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111]\n",
      "saved Active-learning-experiment-10.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "{\n",
      "  \"NBModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"10\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          3.7037037037037033,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          7.4074074074074066\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"10\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"SvmModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"10\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 11, using model = SvmModel, selection_function = MarginSamplingSelection, k = 20, iteration = 0.\n",
      "\n",
      "initial random chosen samples (20,)\n",
      "initial train set: (20, 13) (20,) unique(labels): [11  9] [0 1]\n",
      "val set: (104, 13) (104,) (20,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.17      1.00      0.29         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.33      0.10        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.095238\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [17 23] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.40      1.00      0.57         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.13      0.33      0.19        54\n",
      "weighted avg       0.04      0.11      0.06        54\n",
      "\n",
      "\n",
      "F1 Score : 0.190476\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0]\n",
      "probabilities: (84, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (60, 13) (60,)\n",
      "updated train set: (60, 13) (60,) unique(labels): [27 33] [0 1]\n",
      "val set: (64, 13) (64,)\n",
      "\n",
      "Train set: (60, 13) y: (60,)\n",
      "Val   set: (64, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.19      1.00      0.32         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.33      0.11        54\n",
      "weighted avg       0.02      0.11      0.04        54\n",
      "\n",
      "\n",
      "F1 Score : 0.105263\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-11.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 12, using model = SvmModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
      "\n",
      "initial random chosen samples (10,)\n",
      "initial train set: (10, 13) (10,) unique(labels): [7 3] [0 1]\n",
      "val set: (114, 13) (114,) (10,)\n",
      "\n",
      "Train set: (10, 13) y: (10,)\n",
      "Val   set: (114, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 9.259259 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.11      0.83      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.09        54\n",
      "   macro avg       0.04      0.28      0.07        54\n",
      "weighted avg       0.01      0.09      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.065359\n",
      "--------------------------------\n",
      "val predicted: (114,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
      " 0 1 0]\n",
      "probabilities: (114, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
      " 0 1 0]\n",
      "trainset before (10, 13) (10,)\n",
      "trainset after (20, 13) (20,)\n",
      "updated train set: (20, 13) (20,) unique(labels): [ 8 12] [0 1]\n",
      "val set: (104, 13) (104,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.35      1.00      0.52         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.12      0.33      0.17        54\n",
      "weighted avg       0.04      0.11      0.06        54\n",
      "\n",
      "\n",
      "F1 Score : 0.173913\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (30, 13) (30,)\n",
      "updated train set: (30, 13) (30,) unique(labels): [17 13] [0 1]\n",
      "val set: (94, 13) (94,)\n",
      "\n",
      "Train set: (30, 13) y: (30,)\n",
      "Val   set: (94, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.21      1.00      0.35         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.07      0.33      0.12        54\n",
      "weighted avg       0.02      0.11      0.04        54\n",
      "\n",
      "\n",
      "F1 Score : 0.117647\n",
      "--------------------------------\n",
      "val predicted: (94,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "probabilities: (94, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "trainset before (30, 13) (30,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [19 21] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.40      1.00      0.57         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.13      0.33      0.19        54\n",
      "weighted avg       0.04      0.11      0.06        54\n",
      "\n",
      "\n",
      "F1 Score : 0.190476\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (50, 13) (50,)\n",
      "updated train set: (50, 13) (50,) unique(labels): [21 29] [0 1]\n",
      "val set: (74, 13) (74,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training svm...\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.19      1.00      0.32         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.33      0.11        54\n",
      "weighted avg       0.02      0.11      0.04        54\n",
      "\n",
      "\n",
      "F1 Score : 0.105263\n",
      "--------------------------------\n",
      "final active learning accuracies [9.25925925925926, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-12.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 13, using model = RfModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
      "\n",
      "initial random chosen samples (50,)\n",
      "initial train set: (50, 13) (50,) unique(labels): [17 33] [0 1]\n",
      "val set: (74, 13) (74,) (50,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.12      1.00      0.21         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.068966\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111]\n",
      "saved Active-learning-experiment-13.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 14, using model = RfModel, selection_function = RandomSelection, k = 20, iteration = 0.\n",
      "\n",
      "initial random chosen samples (20,)\n",
      "initial train set: (20, 13) (20,) unique(labels): [11  9] [0 1]\n",
      "val set: (104, 13) (104,) (20,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.12      1.00      0.21         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.070175\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [24 16] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.14      1.00      0.24         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.05      0.33      0.08        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.080000\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (60, 13) (60,)\n",
      "updated train set: (60, 13) (60,) unique(labels): [33 27] [0 1]\n",
      "val set: (64, 13) (64,)\n",
      "\n",
      "Train set: (60, 13) y: (60,)\n",
      "Val   set: (64, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.13      1.00      0.23         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.08        54\n",
      "weighted avg       0.01      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.076923\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-14.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 15, using model = RfModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
      "\n",
      "initial random chosen samples (10,)\n",
      "initial train set: (10, 13) (10,) unique(labels): [5 5] [0 1]\n",
      "val set: (114, 13) (114,) (10,)\n",
      "\n",
      "Train set: (10, 13) y: (10,)\n",
      "Val   set: (114, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.13      1.00      0.23         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.08        54\n",
      "weighted avg       0.01      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.076923\n",
      "--------------------------------\n",
      "val predicted: (114,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1]\n",
      "probabilities: (114, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1]\n",
      "trainset before (10, 13) (10,)\n",
      "trainset after (20, 13) (20,)\n",
      "updated train set: (20, 13) (20,) unique(labels): [13  7] [0 1]\n",
      "val set: (104, 13) (104,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.15      1.00      0.27         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.05      0.33      0.09        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.088889\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (30, 13) (30,)\n",
      "updated train set: (30, 13) (30,) unique(labels): [16 14] [0 1]\n",
      "val set: (94, 13) (94,)\n",
      "\n",
      "Train set: (30, 13) y: (30,)\n",
      "Val   set: (94, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.13      1.00      0.23         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.08        54\n",
      "weighted avg       0.01      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.075472\n",
      "--------------------------------\n",
      "val predicted: (94,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (94, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (30, 13) (30,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [18 22] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.12      1.00      0.21         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.070175\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (50, 13) (50,)\n",
      "updated train set: (50, 13) (50,) unique(labels): [23 27] [0 1]\n",
      "val set: (74, 13) (74,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.067797\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-15.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "{\n",
      "  \"NBModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"10\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          3.7037037037037033,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          7.4074074074074066\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"10\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"RfModel\": {\n",
      "    \"RandomSelection\": {\n",
      "      \"10\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"SvmModel\": {\n",
      "    \"MarginSamplingSelection\": {\n",
      "      \"10\": [\n",
      "        [\n",
      "          9.25925925925926,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"RandomSelection\": {\n",
      "      \"10\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"20\": [\n",
      "        [\n",
      "          11.11111111111111,\n",
      "          11.11111111111111,\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ],\n",
      "      \"50\": [\n",
      "        [\n",
      "          11.11111111111111\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 16, using model = RfModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
      "\n",
      "initial random chosen samples (50,)\n",
      "initial train set: (50, 13) (50,) unique(labels): [29 21] [0 1]\n",
      "val set: (74, 13) (74,) (50,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.15      1.00      0.26         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.05      0.33      0.09        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.086957\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111]\n",
      "saved Active-learning-experiment-16.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 17, using model = RfModel, selection_function = MarginSamplingSelection, k = 20, iteration = 0.\n",
      "\n",
      "initial random chosen samples (20,)\n",
      "initial train set: (20, 13) (20,) unique(labels): [ 8 12] [0 1]\n",
      "val set: (104, 13) (104,) (20,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.15      1.00      0.26         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.05      0.33      0.09        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.085106\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [15 25] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.12      1.00      0.21         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.070175\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (60, 13) (60,)\n",
      "updated train set: (60, 13) (60,) unique(labels): [31 29] [0 1]\n",
      "val set: (64, 13) (64,)\n",
      "\n",
      "Train set: (60, 13) y: (60,)\n",
      "Val   set: (64, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.50      0.10        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.100000\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-17.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 18, using model = RfModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
      "\n",
      "initial random chosen samples (10,)\n",
      "initial train set: (10, 13) (10,) unique(labels): [5 5] [0 1]\n",
      "val set: (114, 13) (114,) (10,)\n",
      "\n",
      "Train set: (10, 13) y: (10,)\n",
      "Val   set: (114, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.50      0.10        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.100000\n",
      "--------------------------------\n",
      "val predicted: (114,) [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "probabilities: (114, 2) \n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "trainset before (10, 13) (10,)\n",
      "trainset after (20, 13) (20,)\n",
      "updated train set: (20, 13) (20,) unique(labels): [12  8] [0 1]\n",
      "val set: (104, 13) (104,)\n",
      "\n",
      "Train set: (20, 13) y: (20,)\n",
      "Val   set: (104, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.17      1.00      0.29         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.33      0.10        54\n",
      "weighted avg       0.02      0.11      0.03        54\n",
      "\n",
      "\n",
      "F1 Score : 0.095238\n",
      "--------------------------------\n",
      "val predicted: (104,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (104, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (20, 13) (20,)\n",
      "trainset after (30, 13) (30,)\n",
      "updated train set: (30, 13) (30,) unique(labels): [17 13] [0 1]\n",
      "val set: (94, 13) (94,)\n",
      "\n",
      "Train set: (30, 13) y: (30,)\n",
      "Val   set: (94, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.12      1.00      0.22         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.04      0.33      0.07        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.074074\n",
      "--------------------------------\n",
      "val predicted: (94,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (94, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (30, 13) (30,)\n",
      "trainset after (40, 13) (40,)\n",
      "updated train set: (40, 13) (40,) unique(labels): [22 18] [0 1]\n",
      "val set: (84, 13) (84,)\n",
      "\n",
      "Train set: (40, 13) y: (40,)\n",
      "Val   set: (84, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.50      0.10        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.100000\n",
      "--------------------------------\n",
      "val predicted: (84,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (84, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "trainset before (40, 13) (40,)\n",
      "trainset after (50, 13) (50,)\n",
      "updated train set: (50, 13) (50,) unique(labels): [22 28] [0 1]\n",
      "val set: (74, 13) (74,)\n",
      "\n",
      "Train set: (50, 13) y: (50,)\n",
      "Val   set: (74, 13)\n",
      "Test  set: (54, 13)\n",
      "training random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (54,)\n",
      "Accuracy rate for classification rate of: 11.111111 \n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.11      1.00      0.20         6\n",
      "           2       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.11        54\n",
      "   macro avg       0.06      0.50      0.10        54\n",
      "weighted avg       0.01      0.11      0.02        54\n",
      "\n",
      "\n",
      "F1 Score : 0.100000\n",
      "--------------------------------\n",
      "final active learning accuracies [11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]\n",
      "saved Active-learning-experiment-18.pkl /content ['.config', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-26.pkl', 'Active-learning-experiment-42.pkl', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-36.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-31.pkl', 'Active-learning-experiment-18.pkl', 'Active-learning-experiment-23.pkl', 'Active-learning-experiment-1.pkl', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-10.pkl', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-41.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-9.pkl', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-13.pkl', 'Active-learning-experiment-40.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-17.pkl', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-45.pkl', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-12.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-14.pkl', 'sample_data']\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "{'NBModel': {'RandomSelection': {'50': [[11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '10': [[11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]]}, 'MarginSamplingSelection': {'50': [[11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 7.4074074074074066]], '10': [[11.11111111111111, 3.7037037037037033, 11.11111111111111, 11.11111111111111, 11.11111111111111]]}}, 'SvmModel': {'RandomSelection': {'50': [[11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '10': [[11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]]}, 'MarginSamplingSelection': {'50': [[11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '10': [[9.25925925925926, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]]}}, 'RfModel': {'RandomSelection': {'50': [[11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '10': [[11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]]}, 'MarginSamplingSelection': {'50': [[11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '10': [[11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]]}}}\n",
      "{'NBModel': {'MarginSamplingSelection': {'10': [[11.11111111111111, 3.7037037037037033, 11.11111111111111, 11.11111111111111, 11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 7.4074074074074066]], '50': [[11.11111111111111]]}, 'RandomSelection': {'10': [[11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '50': [[11.11111111111111]]}}, 'RfModel': {'MarginSamplingSelection': {'10': [[11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '50': [[11.11111111111111]]}, 'RandomSelection': {'10': [[11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '50': [[11.11111111111111]]}}, 'SvmModel': {'MarginSamplingSelection': {'10': [[9.25925925925926, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '50': [[11.11111111111111]]}, 'RandomSelection': {'10': [[11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111, 11.11111111111111]], '20': [[11.11111111111111, 11.11111111111111, 11.11111111111111]], '50': [[11.11111111111111]]}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "trainset_size = 124\n",
    "\n",
    "(X_train_full, y_train_full, X_test, y_test) = split(trainset_size)\n",
    "print ('train:', X_train_full.shape, y_train_full.shape)\n",
    "print ('test :', X_test.shape, y_test.shape)\n",
    "classes = len(np.unique(y))\n",
    "print ('unique classes', classes)\n",
    "print ('train:', X_train_full.shape, y_train_full.shape)\n",
    "print ('test :', X_test.shape, y_test.shape)\n",
    "classes = len(np.unique(y))\n",
    "print ('unique classes', classes)\n",
    "\n",
    "def pickle_save(fname, data):\n",
    "  filehandler = open(fname,\"wb\")\n",
    "  pickle.dump(data,filehandler)\n",
    "  filehandler.close() \n",
    "  print('saved', fname, os.getcwd(), os.listdir())\n",
    "\n",
    "def pickle_load(fname):\n",
    "  print(os.getcwd(), os.listdir())\n",
    "  file = open(fname,'rb')\n",
    "  data = pickle.load(file)\n",
    "  file.close()\n",
    "  print(data)\n",
    "  return data\n",
    "  \n",
    "def experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
    "    algos_temp = []\n",
    "    print ('stopping at:', max_queried)\n",
    "    count = 0\n",
    "    for model_object in models:\n",
    "      if model_object.__name__ not in d:\n",
    "          d[model_object.__name__] = {}\n",
    "      \n",
    "      for selection_function in selection_functions:\n",
    "        if selection_function.__name__ not in d[model_object.__name__]:\n",
    "            d[model_object.__name__][selection_function.__name__] = {}\n",
    "        \n",
    "        for k in Ks:\n",
    "            d[model_object.__name__][selection_function.__name__][str(k)] = []           \n",
    "            \n",
    "            for i in range(0, repeats):\n",
    "                count+=1\n",
    "                if count >= contfrom:\n",
    "                    print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
    "                    alg = TheAlgorithm(k, \n",
    "                                       model_object, \n",
    "                                       selection_function\n",
    "                                       )\n",
    "                    alg.run(X_train_full, y_train_full, X_test, y_test)\n",
    "                    d[model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
    "                    fname = 'Active-learning-experiment-' + str(count) + '.pkl'\n",
    "                    pickle_save(fname, d)\n",
    "                    if count % 5 == 0:\n",
    "                        print(json.dumps(d, indent=2, sort_keys=True))\n",
    "                    print ()\n",
    "                    print ('---------------------------- FINISHED ---------------------------')\n",
    "                    print ()\n",
    "    return d\n",
    "\n",
    "\n",
    "max_queried = 50 \n",
    "\n",
    "repeats = 1\n",
    "\n",
    "models = [NBModel, SvmModel, RfModel] \n",
    "\n",
    "selection_functions = [RandomSelection, MarginSamplingSelection] \n",
    "\n",
    "Ks = [50,20,10] \n",
    "\n",
    "d = {}\n",
    "stopped_at = -1 \n",
    "\n",
    "\n",
    "d = experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
    "print (d)\n",
    "results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_8_week_13.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
